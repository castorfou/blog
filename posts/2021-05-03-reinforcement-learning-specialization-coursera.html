<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.159">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-05-03">
<meta name="description" content="From University of Alberta. My notes on course 1.">

<title>Guillaume’s blog - Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon_small.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-12YC1FPHWN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-12YC1FPHWN', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Guillaume’s blog - Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning">
<meta name="twitter:description" content="From University of Alberta. My notes on course 1.">
<meta name="twitter:image" content="https://castorfou.github.io/posts/images/RL.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Guillaume’s blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/castorfou" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/GuillaumeRamel1" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning</h1>
                  <div>
        <div class="description">
          From University of Alberta. My notes on course 1.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">reinforcement learning</div>
                <div class="quarto-category">deepmind</div>
                <div class="quarto-category">coursera</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 3, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#course-1---week-1---an-introduction-to-sequential-decision-making" id="toc-course-1---week-1---an-introduction-to-sequential-decision-making" class="nav-link active" data-scroll-target="#course-1---week-1---an-introduction-to-sequential-decision-making">5/3/21 - Course 1 - Week 1 - An introduction to Sequential Decision-Making</a></li>
  <li><a href="#course-1---week-2---markov-decision-process" id="toc-course-1---week-2---markov-decision-process" class="nav-link" data-scroll-target="#course-1---week-2---markov-decision-process">5/7/21 - Course 1 - Week 2 - Markov Decision Process</a></li>
  <li><a href="#course-1---week-3---value-functions-bellman-equations" id="toc-course-1---week-3---value-functions-bellman-equations" class="nav-link" data-scroll-target="#course-1---week-3---value-functions-bellman-equations">5/10/21 - Course 1 - Week 3 - Value Functions &amp; Bellman Equations</a></li>
  <li><a href="#course-1---week-4---dynamic-programming" id="toc-course-1---week-4---dynamic-programming" class="nav-link" data-scroll-target="#course-1---week-4---dynamic-programming">5/18/21 - Course 1 - Week 4 - Dynamic Programming</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Coursera website: <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome">course 1 - Fundamentals of Reinforcement Learning</a> of <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning Specialization</a></p>
<p>my notes on <a href="../guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html">course 2 - Sample-based Learning Methods</a>, <a href="../guillaume_blog/blog/reinforcement-learning-specialization-coursera-course3.html">course 3 - Prediction and Control with Function Approximation</a>, <a href="../guillaume_blog/blog/reinforcement-learning-specialization-coursera-course4.html">course 4 - A Complete Reinforcement Learning System (Capstone)</a></p>
<p><a href="https://www.coursera.org/specializations/reinforcement-learning">Syllabus</a></p>
<p>4 courses on 16 weeks by Martha White and Adam White.</p>
<ul>
<li><p><a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning?specialization=reinforcement-learning">Fundamentals of Reinforcement Learning</a></p></li>
<li><p><a href="https://www.coursera.org/learn/sample-based-learning-methods?specialization=reinforcement-learning">Sample-based Learning Methods</a></p></li>
<li><p><a href="https://www.coursera.org/learn/prediction-control-function-approximation?specialization=reinforcement-learning">Prediction and Control with Function Approximation</a></p></li>
<li><p><a href="https://www.coursera.org/learn/complete-reinforcement-learning-system?specialization=reinforcement-learning">A Complete Reinforcement Learning System (Capstone)</a></p></li>
</ul>
<p><strong>specialization roadmap</strong></p>
<p><strong>course 1</strong> - we begin our study with multi-arm bandit problems. Here, we get our first taste of the complexities of <strong>incremental learning</strong>, <strong>exploration</strong>, and <strong>exploitation</strong>. After that, we move onto <strong>Markov decision processes</strong> to broaden the class of problems we can solve with reinforcement learning methods. Here we will learn about <strong>balancing short-term and long-term reward</strong>. We will introduce key ideas like <strong>policies</strong> and <strong>value functions</strong> using almost all RL systems. We conclude Course 1 with classic planning methods called <strong>dynamic programming</strong>. These methods have been used in large industrial control problems and can compute optimal policies given a complete model of the world.</p>
<p><strong>course 2</strong> - In Course 2, we built on these ideas and design algorithms for learning <strong>without a model</strong> of the world. We study three classes of methods designed for learning from trial and error interaction. We start with <strong>Monte Carlo</strong> methods and then move on to <strong>temporal difference</strong> learning, including Q learning. We conclude Course 2 with an investigation of methods for <strong>planning</strong> with learned models.</p>
<p><strong>course 3</strong> - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with <strong>function approximation</strong>. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger <strong>infinite state spaces</strong>. We will cover <strong>feature construction</strong>, <strong>neural network learning</strong>, <strong>policy gradient methods</strong>, and other particularities of the function approximation setting.</p>
<p><strong>course 4</strong> - The final course in this specialization brings everything together in a Capstone project. Throughout this specialization, as in Rich and Andy’s book, we stress a rigorous and scientific approach to RL. We conduct numerous experiments designed to carefully compare algorithms. It takes careful planning and a lot of hard work to produce a meaningful empirical results. In the Capstone, we will walk you through each step of this process so that you can conduct your own scientific experiment. We will explore all the stages from problem specification, all the way to publication quality plots. This is not just academic. In real problems, it’s important to verify and understand your system. After that, you should be ready to test your own new ideas or tackle a new exciting application of RL in your job. We hope you enjoyed the show half as much as we enjoyed making it for you.</p>
<p>Alberta is in Canada.</p>
<section id="course-1---week-1---an-introduction-to-sequential-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="course-1---week-1---an-introduction-to-sequential-decision-making">5/3/21 - Course 1 - Week 1 - An introduction to Sequential Decision-Making</h2>
<p>I have set recommended goals 3 times a week.</p>
<p><strong>about supervised learning, unsupervised learning and RL</strong></p>
<p>You might wonder what’s the difference between supervised learning, unsupervised learning, and reinforcement learning? The differences are quite simple. In supervised learning we assume the learner has access to labeled examples giving the correct answer. In RL, the reward gives the agent some idea of how good or bad its recent actions were. You can think of supervised learning as requiring a teacher that helps you by telling you the correct answer. A reward on the other hand, is like having someone who can identify what good behavior looks like but can’t tell you exactly how to do it. Unsupervised learning sounds like it could be related but really has a very different goal. Unsupervised learning is about extracting underlying structure in data. It’s about the data representation. It can be used to construct representations that make a supervised or RL system better. In fact, as you’ll see later in this course, techniques from both supervised learning and unsupervised learning can be used within RL to aid generalization</p>
<p><strong>industrial control</strong></p>
<p>So I think the place we’re really going to see it take off is an industrial control. In industrial control, we have experts that are really looking for ways to improve the optimal- how well their systems work. So we’re going to see it do things like reduce energy costs or save on other types of costs that we have in these industrial control systems. In the hands of experts, we can really make these algorithms work well in the near future. So I really see it as a tool that’s going to facilitate experts in their work rather than say, doing something like replacing people or automating them away.</p>
<p><strong>Reinforcement Learning Textbook</strong></p>
<p>as always, <strong>Reinforcement Learning: An introduction (Second Edition) by Richard S. Sutton and Andrew G. Barto</strong> is THE reference. I didn’t know that Adam White was student from Sutton. Lucky guy ;)</p>
<p><strong>K-armed Bandit problem</strong></p>
<p><img src="https://miro.medium.com/max/640/1*Ahv2hWGCZiwTDQX5TIiUjw.jpeg" class="img-fluid"></p>
<p>Starts with reading of RLbook p25-36 (Chapter 2 Multi-armed Bandits)</p>
<p><a href="http://incompleteideas.net/book/first/ebook/node14.html">Evaluative</a> vs instructive feedback. Nonassociative refers to cases where you take one action per state. At the end there is a generalization where bandit problem becomes associative, that is, when actions are taken in more than one situation.</p>
<p>It is a stationary case meaning that value of actions are fixed during experiences. If the bandit task were nonstationary, that is, the true values of the actions changed over time. In this case exploration is needed even in the deterministic case to make sure one of the nongreedy actions has not changed to become better than the greedy one.</p>
<p>sample-average action-value estimates</p>
<p><span class="math display">\[
Q_t(a) = \frac{\text{sum of rewards when } \mathit{a} \text{ taken prior to }\mathit{t}}{\text{number of times } \mathit{a} \text{ taken prior to }\mathit{t}} \\
Q_t(a)  = \frac{\displaystyle\sum_{i=1}^{t-1} R_i.\mathcal{1}_{A_i=a}}{\displaystyle\sum_{i=1}^{t-1} \mathcal{1}_{A_i=a}}
\]</span></p>
<p><span class="math inline">\(\epsilon\)</span>-greedy action selection</p>
<p><span class="math display">\[
A_t=\underset{a}{\mathrm{argmax}}{\text{ }Q_t(a)}
\]</span></p>
<p>With nonstationary problem, we want to give more weights to recent rewards. It can be done with <span class="math display">\[
Q_{n+1}=Q_n+\alpha[R_n-Q_n]
\]</span> Where <span class="math display">\[\alpha\]</span> is a constant step-size parameter, <span class="math display">\[\alpha \in [0,1]\]</span>. So it can be written that way <span class="math display">\[
Q_{n+1}=(1-\alpha)^nQ_1+\displaystyle\sum_{i=1}^{n} \alpha(1-\alpha)^{n-i}R_i
\]</span> . Weighted average because the sum of the weights is 1.</p>
<p>2 other topics are discussed: <strong>optimistic initial values</strong> (that can push exploration in 1st steps) and <strong>upper-confidence-bound (UCB)</strong> action selection. With optimistic initial values the idea is too have high initial value for reward so that the 1st actions are disappointing pushing for explorations. With UCB</p>
<p><span class="math display">\[
A_t= \underset{a} {\mathrm{argmax}} {\text{ }\bigg[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}\bigg]}
\]</span></p>
<blockquote class="blockquote">
<p>The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the confidence level. Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.</p>
</blockquote>
<p><strong>Exploration vs Exploitation trade-off</strong></p>
<p>How do we choose when to explore, and when to exploit? Randomly</p>
<p><img src="../images/alberta_rl_epsilon_greedy.png" class="img-fluid"></p>
<p><strong>Assignement</strong></p>
<p>implementation of greedy agent, <span class="math inline">\(\epsilon\)</span>-greedy agent. Comparisons. Various <span class="math inline">\(\epsilon\)</span> values, various step-sizes (1/N(a), …)</p>
<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%201%20week%201">github</a></p>
<p>end of C1W1 (course 1 week 1)</p>
</section>
<section id="course-1---week-2---markov-decision-process" class="level2">
<h2 class="anchored" data-anchor-id="course-1---week-2---markov-decision-process">5/7/21 - Course 1 - Week 2 - Markov Decision Process</h2>
<section id="module-2-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-2-learning-objectives">Module 2 Learning Objectives</h6>
<p><strong>Lesson 1: Introduction to Markov Decision Processes</strong></p>
<ul>
<li><p>Understand Markov Decision Processes, or MDPs</p></li>
<li><p>Describe how the dynamics of an MDP are defined</p></li>
<li><p>Understand the graphical representation of a Markov Decision Process</p></li>
<li><p>Explain how many diverse processes can be written in terms of the MDP framework</p></li>
</ul>
<p><strong>Lesson 2: Goal of Reinforcement Learning</strong></p>
<ul>
<li><p>Describe how rewards relate to the goal of an agent</p></li>
<li><p>Understand episodes and identify episodic tasks</p></li>
</ul>
<p><strong>Lesson 3: Continuing Tasks</strong></p>
<ul>
<li><p>Formulate returns for continuing tasks using discounting</p></li>
<li><p>Describe how returns at successive time steps are related to each other</p></li>
<li><p>Understand when to formalize a task as episodic or continuing</p></li>
</ul>
</section>
<section id="lesson-1-introduction-to-markov-decision-processes" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-introduction-to-markov-decision-processes">Lesson 1: Introduction to Markov Decision Processes</h6>
<p><strong>Reading</strong> chapter 3.1 to 3.3 (p47-56) in Sutton’s book</p>
<p><strong>Finite Markov Decision Processes</strong></p>
<ul>
<li>3.1 - the Agent-Environment Interface</li>
<li>3.2 - Goals and Rewards</li>
<li>3.3 - Returns and Episodes</li>
</ul>
<p>In a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for <span class="math inline">\(S_t\)</span> and <span class="math inline">\(R_t\)</span> depends only on the immediately preceding state and action, <span class="math inline">\(S_{t-1}\)</span> and <span class="math inline">\(A_{t-1}\)</span> , and, given them, not at all on earlier states and actions.</p>
<p><span class="math display">\[
p(s',r|s,a) \doteq Pr\{S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a\}
\]</span></p>
<p>The <strong>state</strong> must include information about all aspects of the past agent–environment interaction that make a difference for the future. In general, <strong>actions</strong> can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them.</p>
<p>The <strong>agent–environment</strong> boundary represents the limit of the agent’s absolute control, not of its knowledge.</p>
<p><strong>Goal</strong> can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called <strong>reward</strong>). The reward signal is your way of communicating to the agent what you want it to achieve, not how you want it achieved.</p>
<p><strong>Expected return</strong> <span class="math inline">\(G_t\)</span> is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards: <span class="math display">\[
G_t \doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}
\]</span> where <span class="math inline">\(T\)</span> is the final time step.</p>
<p>With <em>continuing</em> tasks, we can have <span class="math inline">\(T=\infty\)</span>, we can then introduce <em>discounting</em>. Agent chooses <span class="math inline">\(A_t\)</span> to maximize the expected discounted return: <span class="math display">\[
G_t \doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...=\displaystyle\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]</span> where <span class="math inline">\(\gamma\)</span> is called the <em>discount rate</em>. <span class="math display">\[
G_t = R_{t+1}+\gamma G_{t+1}
\]</span> <strong>Video MDP</strong> by Martha. By the end of this video: <em>Understand</em> <strong>Markov Decision Process (MDP)</strong>, <em>Describe</em> how the <strong>dynamics of an MDP</strong> are defined.</p>
<p>Martha highlights differences between k-armed bandit and MDP. The k-armed bandit agent is presented with the same situation at each time and the same action is always optimal. In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future. In particular if state changes, k-armed bandit don’t adapt. It is why we need MDP.</p>
<p><strong>Video examples of MDPs</strong> by Adam . By the end of this video: Gain experience <strong>formalizing decision-making problems as MDPs</strong>, Appreciate the <em>flexibility</em> of the MDP formalism.</p>
<p>Adam uses 2 examples: robot recycling cans and robot arm.</p>
</section>
<section id="lesson-2-goal-of-reinforcement-learning" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-goal-of-reinforcement-learning">Lesson 2: Goal of Reinforcement Learning</h6>
<p><strong>Video the Goal of Reinforcement Learning</strong> by Adam. By the end of this video: <em>Describe</em> how rewards relate to the <strong>goal</strong> of an agent, <em>Identify</em> <strong>episodic tasks</strong>.</p>
<p>With MDP, agents can have long-term goals.</p>
<p><strong>Video the Reward Hypothesis</strong> by Michael Littman.</p>
<p>He gives a nice idea when defining reward hypothesis: a contrast between the simplicity of the idea of rewards with the complexity of the real world.</p>
</section>
<section id="lesson-3-continuing-tasks" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-continuing-tasks">Lesson 3: Continuing Tasks</h6>
<p><strong>Video Continuing Tasks</strong> by Martha. By the end of this video: <em>Differentiate</em> between <strong>episodic</strong> and <strong>continuing tasks</strong>. <em>Formulate</em> returns for continuing tasks using <strong>discounting</strong>. <em>Describe</em> how <strong>returns at successive time steps</strong> are related to each other.</p>
<p>Adam uses a <a href="http://www.incompleteideas.net/book/the-book.html">link</a> to Sutton’s book., This is a 2020 version of this book.</p>
<p><strong>Video Examples of Episodic and Continuing Tasks</strong> by Martha. By the end of this video: <em>Understand</em> when to formalize a task as <strong>episodic or continuing</strong>.</p>
<p>Martha gives 2 examples: one of an episodic tasks where episode ends when player is touched by an enemy, one of continuous tasks where an agent accepts or rejects tasks depending on priority and servers available (never ending episode).</p>
<p>Weekly assessment.</p>
<p>This is a quizz and a peer-graded assignment. I had to describe 3 MDPs with all its detail (states actions, rewards).</p>
</section>
</section>
<section id="course-1---week-3---value-functions-bellman-equations" class="level2">
<h2 class="anchored" data-anchor-id="course-1---week-3---value-functions-bellman-equations">5/10/21 - Course 1 - Week 3 - Value Functions &amp; Bellman Equations</h2>
<section id="module-3-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-3-learning-objectives">Module 3 Learning Objectives</h6>
<p><strong>Lesson 1: Policies and Value Functions</strong></p>
<ul>
<li>Recognize that a policy is a distribution over actions for each possible state</li>
<li>Describe the similarities and differences between stochastic and deterministic policies</li>
<li>Identify the characteristics of a well-defined policy</li>
<li>Generate examples of valid policies for a given MDP</li>
<li>Describe the roles of state-value and action-value functions in reinforcement learning</li>
<li>Describe the relationship between value functions and policies</li>
<li>Create examples of valid value functions for a given MDP</li>
</ul>
<p><strong>Lesson 2: Bellman Equations</strong></p>
<ul>
<li>Derive the Bellman equation for state-value functions</li>
<li>Derive the Bellman equation for action-value functions</li>
<li>Understand how Bellman equations relate current and future values</li>
<li>Use the Bellman equations to compute value functions</li>
</ul>
<p><strong>Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</strong></p>
<ul>
<li>Define an optimal policy</li>
<li>Understand how a policy can be at least as good as every other policy in every state</li>
<li>Identify an optimal policy for given MDPs</li>
<li>Derive the Bellman optimality equation for state-value functions</li>
<li>Derive the Bellman optimality equation for action-value functions</li>
<li>Understand how the Bellman optimality equations relate to the previously introduced Bellman equations</li>
<li>Understand the connection between the optimal value function and optimal policies</li>
<li>Verify the optimal value function for given MDPs</li>
</ul>
</section>
<section id="lesson-1-policies-and-value-functions" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-policies-and-value-functions">Lesson 1: Policies and Value Functions</h6>
<p><strong>Reading</strong> chapter 3.5 to 3.8 (p58-67) in Sutton’s book</p>
<p>Almost all reinforcement learning algorithms involve estimating <strong>value functions</strong>—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).</p>
<p>Searching for additional informations, I have fallen into <a href="https://shangtongzhang.github.io/">ShangtongZhang</a> page and <a href="https://github.com/ShangtongZhang">repos</a>. Only 2 of them but seem to be great: <strong><a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction">reinforcement-learning-an-introduction</a></strong> contains implementations in Python of all concepts from Sutton’s book. <strong><a href="https://github.com/ShangtongZhang/DeepRL">DeepRL</a></strong> seems to be a pytorch implementations (DQN, A2C, PPO, …)</p>
<p>Here we see <strong>Bellman equation</strong> for state-value function <span class="math inline">\(v_\pi(s)\)</span></p>
<p><span class="math display">\[
v_\pi(s) \doteq \mathbb{E}[G_t|S_t=s]
\\
v_\pi(s) = \displaystyle\sum_{a} \pi(a|s) \displaystyle\sum_{s',r} p(s', r|s, a)\big[r+\gamma.v_\pi(s')\big]
\]</span></p>
<p><strong>Bellman equation</strong> for action-value function <span class="math inline">\(q_\pi(s,a)\)</span></p>
<p><span class="math display">\[
q_\pi(s,a) \doteq \mathbb{E}[R_{t+1}+\gamma.G_{t+1}|S_t=s, A_t=a]
\\
q_\pi(s, a) = \displaystyle\sum_{s',r} p(s', r|s, a) \big[ r + \gamma\displaystyle\sum_{a'} \pi(s', a')q_\pi(s',a') \big]
\]</span></p>
<p><strong>Optimal state-value function <span class="math inline">\(v_*\)</span></strong>:</p>
<p><span class="math display">\[
v_*(s)\doteq \max\limits_{\pi} v_\pi(s), \forall s \in S
\]</span></p>
<p><strong>Optimal action-value function <span class="math inline">\(q_*\)</span></strong>: <span class="math display">\[
q_*(s,a) \doteq \max\limits_{\pi} q_\pi(s,a) = \mathbb{E}[R_{t+1}+\gamma.v_*(S_{t+1})|S_t=s, A_t=a]
\]</span></p>
<p>We denote all optimal policies by <span class="math inline">\(\pi_*\)</span></p>
<p><strong>Bellman optimality equation</strong> for <span class="math inline">\(v_*\)</span></p>
<p><span class="math display">\[
v_*(s) = \max\limits_{a} \displaystyle\sum_{s', r} p(s',r|s, a)\big[ r + \gamma .v_*(s') \big]
\]</span></p>
<p><strong>Bellman optimality equation</strong> for <span class="math inline">\(q_*\)</span></p>
<p><span class="math display">\[
q_*(s,a) = \displaystyle\sum_{s',r} p(s', r|s, a) \big[ r + \gamma.\max\limits_{a'} q_*(s',a') \big]
\]</span></p>
<p><img src="../home/explore/git/guillaume/blog/images/alberta_rl-backup-diagrams.png" class="img-fluid"></p>
<p><strong>Video Specifying Policies</strong> by Adam.</p>
<p>By the end of this video, you’ll be able to</p>
<p><em>Recognize</em> that a <strong>policy</strong> is a <strong>distribution over actions</strong> for each possible <strong>state</strong>, <em>describe</em> the similarities and differences between <strong>stochastic and deterministic policies</strong>, and <em>generate examples</em> of <strong>valid policies</strong> for a given MDP or Markup Decision Process.</p>
<p><strong>Video Value Functions</strong> by Adam.</p>
<p>By the end of this video, you’ll be able to</p>
<p><em>describe the roles</em> of the <strong>state-value and</strong> <strong>action-value functions</strong> in reinforcement learning, <em>describe the relationship</em> between <strong>value-functions</strong> and <strong>policies</strong>, and <em>create examples</em> of <strong>value-functions</strong> for a given MDP.</p>
<p><strong>Video Rich Sutton and Andy Barto: A brief History of RL</strong></p>
</section>
<section id="lesson-2-bellman-equations" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-bellman-equations">Lesson 2: Bellman Equations</h6>
<p><strong>Video Bellman Equation Derivation</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>derive</em> the <strong>Bellman equation</strong> for <strong>state-value functions</strong>, <em>derive</em> the <strong>Bellman equation</strong> for <strong>action-value functions</strong>, and <em>understand</em> how <strong>Bellman equations</strong> relate <strong>current</strong> and <strong>future values</strong>.</p>
<p><strong>Video Why Bellman Equations?</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>use</em> the <strong>Bellman equations</strong> to compute <strong>value functions</strong></p>
</section>
<section id="lesson-3-optimality-optimal-policies-value-functions" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-optimality-optimal-policies-value-functions">Lesson 3: Optimality (Optimal Policies &amp; Value Functions)</h6>
<p><strong>Video Optimal Policies</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>define</em> an <strong>optimal policy</strong>, <em>understand</em> how <strong>policy</strong> can be at least as good as every other policy in every state, and <em>identify</em> an <strong>optimal policy</strong> for a given MDP.</p>
<p><strong>Video Optimal Value Functions</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>derive</em> the <strong>Bellman optimality equation</strong> for the <strong>state-value function</strong>, <em>derive</em> the <strong>Bellman optimality equation</strong> for the <strong>action-value function</strong>, and <em>understand</em> how the <strong>Bellman optimality equations</strong> relate to the previously introduced <strong>Bellman equations</strong>.</p>
<p><strong>Video Using Optimal Value Functions to Get Optimal Policies</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>understand</em> the connection between the <strong>optimal value function</strong> and <strong>optimal policies</strong> and <em>verify</em> the <strong>optimal value function</strong> for a given MDP</p>
<p><strong>Video week 3 summary</strong> by Adam</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../images/C1W3_1_policies.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Policies</figcaption><p></p>
</figure>
</div>
<p><img src="../images/C1W3_2_value_functions.png" class="img-fluid"></p>
<p><img src="../images/C1W3_3_bellman_equations.png" class="img-fluid"></p>
<p><img src="../images/C1W3_4_optimal_policies.png" class="img-fluid"></p>
<p><img src="../images/C1W3_5_bellman_optimality_equations.png" class="img-fluid"></p>
</section>
</section>
<section id="course-1---week-4---dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="course-1---week-4---dynamic-programming">5/18/21 - Course 1 - Week 4 - Dynamic Programming</h2>
<section id="module-4-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-4-learning-objectives">Module 4 Learning Objectives</h6>
<p><strong>Lesson 1: Policy Evaluation (Prediction)</strong></p>
<ul>
<li>Understand the distinction between policy evaluation and control</li>
<li>Explain the setting in which dynamic programming can be applied, as well as its limitations</li>
<li>Outline the iterative policy evaluation algorithm for estimating state values under a given policy<br>
</li>
<li>Apply iterative policy evaluation to compute value functions</li>
</ul>
<p><strong>Lesson 2: Policy Iteration (Control)</strong></p>
<ul>
<li>Understand the policy improvement theorem</li>
<li>Use a value function for a policy to produce a better policy for a given MDP</li>
<li>Outline the policy iteration algorithm for finding the optimal policy</li>
<li>Understand “the dance of policy and value”</li>
<li>Apply policy iteration to compute optimal policies and optimal value functions</li>
</ul>
<p><strong>Lesson 3: Generalized Policy Iteration</strong></p>
<ul>
<li>Understand the framework of generalized policy iteration</li>
<li>Outline value iteration, an important example of generalized policy iteration</li>
<li>Understand the distinction between synchronous and asynchronous dynamic programming methods</li>
<li>Describe brute force search as an alternative method for searching for an optimal policy</li>
<li>Describe Monte Carlo as an alternative method for learning a value function</li>
<li>Understand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy</li>
</ul>
</section>
<section id="lesson-1-policy-evaluation-prediction" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-policy-evaluation-prediction">Lesson 1: Policy Evaluation (Prediction)</h6>
<p><strong>Reading</strong> chapter <strong>4.1, 4.2, 4.3, 4.4, 4.6, 4.7</strong> <strong>(pages 73-88)</strong> in Sutton’s book (with the help of <a href="https://github.com/LyWangPX/Reinforcement-Learning-2nd-Edition-by-Sutton-Exercise-Solutions/blob/master/Chapter%204/Solutions_to_Reinforcement_Learning_by_Sutton_Chapter_4_r5.pdf">Solutions_to_Reinforcement_Learning_by_Sutton_Chapter_4_r5.pdf</a>)</p>
<p>A common way of obtaining approximate solutions for tasks with <strong>continuous states</strong> <strong>and actions</strong> is to <strong>quantize</strong> the state and action spaces and then apply finite-state DP methods.</p>
<p><strong>Video Policy Evaluation vs.&nbsp;Control</strong> by Martha</p>
<p>By the end of this video you will be able to <em>understand</em> the distinction between <strong>policy evaluation</strong> and <strong>control</strong>, and <em>explain</em> the setting in which <strong>dynamic programming</strong> can be applied as well as its limitations.</p>
<p><strong>Video Iterative Policy Evaluation</strong> by Martha</p>
<p>By the end of this video you will be able to <em>outline</em> the <strong>iterative policy evaluation</strong> algorithm for estimating state values for a given policy, and <em>apply</em> iterative policy evaluation to compute value functions.</p>
<p>The magic here is to turn the bellman equation into an iterative evaluation which converges to <span class="math inline">\(v_\pi\)</span>.</p>
<p><img src="../images/C1W4_1_iterative_policy_evaluation.png" class="img-fluid"></p>
</section>
<section id="lesson-2-policy-iteration-control" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-policy-iteration-control">Lesson 2: Policy Iteration (Control)</h6>
<p><strong>Video Policy Improvement</strong> by Marta</p>
<p>By the end of this video, you will be able to <em>understand</em> the <strong>policy improvement theorem</strong>, and how it can be used to construct improved policies, and <em>use</em> the value function for a policy to produce a better policy for a given MDP.</p>
<p>Greedified policy is a strict improvement.</p>
<p><img src="../images/C1W4_1_policy_improvement theorem.png" class="img-fluid"></p>
<p><strong>Video Policy Iteration</strong> by Marta</p>
<p>By the end of this video, you will be able to <em>outline</em> the <strong>policy iteration</strong> algorithm for finding the optimal policy, <em>understand</em> the <strong>dance of policy and value</strong>, how policy iteration reaches the optimal policy by alternating between evaluating policy and improving it, and <em>apply</em> policy iteration to compute optimal policies and optimal value functions.</p>
<p><img src="../images/C1W4_1_policy_iteration.png" class="img-fluid"></p>
<p><img src="../images/C1W4_1_policy_iteration_graph.png" class="img-fluid"></p>
</section>
<section id="lesson-3-generalized-policy-iteration" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-generalized-policy-iteration">Lesson 3: Generalized Policy Iteration</h6>
<p><strong>Video Flexibility of the Policy Iteration Framework</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>understand</em> the framework of <strong>generalized policy iteration</strong>, <em>outline</em> <strong>value iteration</strong> and important special case of generalized policy iteration, and <em>differentiate</em> <strong>synchronous</strong> and <strong>asynchronous</strong> dynamic programming methods.</p>
<p><strong>Video Efficiency of Dynamic Programming</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>describe</em> <strong>Monte Carlo sampling</strong> as an alternative method for learning a value function. <em>Describe</em> <strong>brute force-search</strong> as an alternative method for finding an optimal policy. And <em>understand</em> the advantages of <strong>dynamic programming</strong> and <strong>bootstrapping</strong> over these alternatives.</p>
<p>The most important takeaway is that bootstrapping can save us from performing a huge amount of unnecessary work by exploiting the connection between the value of a state and its possible successors.</p>
<p><strong>Video Warren Powell: Approximate Dynamic Programming for Fleet Management (Short)</strong></p>
<p><strong>Video Week 4 Summary</strong> by Adam</p>
<p>Reading chapter summary <strong>Chapter 4.8, (pages 88-89)</strong></p>
</section>
<section id="assignment" class="level6">
<h6 class="anchored" data-anchor-id="assignment">Assignment</h6>
<p>Optimal Policies with Dynamic Programming</p>
<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%201%20week%204">github</a></p>
<p>end of C1W4 (course 1 week 4)</p>
<p>end of course 1 (and with a <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/blob/main/certificates/course%201.pdf">certificate</a> ;) )</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/^(?:http:|https:)\/\/castorfou\.github\.io\//);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>