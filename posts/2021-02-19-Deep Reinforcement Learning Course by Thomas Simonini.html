<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.222">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-02-19">
<meta name="description" content="A Free course in Deep Reinforcement Learning from beginner to expert. My notes">

<title>Guillaume’s blog - Practicing: Deep Reinforcement Learning Course by Thomas Simonini</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon_small.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-12YC1FPHWN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-12YC1FPHWN', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Guillaume’s blog - Practicing: Deep Reinforcement Learning Course by Thomas Simonini">
<meta name="twitter:description" content="A Free course in Deep Reinforcement Learning from beginner to expert. My notes">
<meta name="twitter:image" content="https://castorfou.github.io/posts/images/RL.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Guillaume’s blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/castorfou" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/GuillaumeRamel1" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Practicing: Deep Reinforcement Learning Course by Thomas Simonini</h1>
                  <div>
        <div class="description">
          A Free course in Deep Reinforcement Learning from beginner to expert. My notes
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">reinforcement learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 19, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#chapter-1---an-introduction-to-deep-reinforcement-learning" id="toc-chapter-1---an-introduction-to-deep-reinforcement-learning" class="nav-link active" data-scroll-target="#chapter-1---an-introduction-to-deep-reinforcement-learning">(2/19/21) - Chapter 1 - An Introduction to Deep Reinforcement Learning?</a>
  <ul class="collapse">
  <li><a href="#three-approaches-to-reinforcement-learning" id="toc-three-approaches-to-reinforcement-learning" class="nav-link" data-scroll-target="#three-approaches-to-reinforcement-learning">Three approaches to Reinforcement Learning</a></li>
  <li><a href="#deep-reinforcement-learning" id="toc-deep-reinforcement-learning" class="nav-link" data-scroll-target="#deep-reinforcement-learning">Deep Reinforcement Learning</a></li>
  </ul></li>
  <li><a href="#chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi" id="toc-chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi" class="nav-link" data-scroll-target="#chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi">(2/19/21) - Chapter 2 - part 1 - Q-Learning, let’s create an autonomous Taxi</a></li>
  <li><a href="#chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi" id="toc-chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi" class="nav-link" data-scroll-target="#chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi">(2/24/21) - Chapter 2 - part 2 - Q-Learning, let’s create an autonomous Taxi</a></li>
  <li><a href="#back-to-2018---chapter-3---deep-q-learning-with-doom" id="toc-back-to-2018---chapter-3---deep-q-learning-with-doom" class="nav-link" data-scroll-target="#back-to-2018---chapter-3---deep-q-learning-with-doom">(3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom</a></li>
  <li><a href="#chapter-4-improvements-in-deep-q-learning-v1" id="toc-chapter-4-improvements-in-deep-q-learning-v1" class="nav-link" data-scroll-target="#chapter-4-improvements-in-deep-q-learning-v1">(3/10/21) - Chapter 4: Improvements in Deep Q Learning V1</a></li>
  <li><a href="#chapter-5-policy-gradients-v1" id="toc-chapter-5-policy-gradients-v1" class="nav-link" data-scroll-target="#chapter-5-policy-gradients-v1">(3/17/21) - Chapter 5: Policy Gradients V1</a></li>
  <li><a href="#chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1" id="toc-chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1" class="nav-link" data-scroll-target="#chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1">(3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1</a></li>
  <li><a href="#chapter-7-proximal-policy-optimization-ppo-v1" id="toc-chapter-7-proximal-policy-optimization-ppo-v1" class="nav-link" data-scroll-target="#chapter-7-proximal-policy-optimization-ppo-v1">(3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>A course by <a href="https://www.simoninithomas.com/">Thomas Simonini</a></p>
<p><a href="https://simoninithomas.github.io/deep-rl-course/">Syllabus (from 2018)</a></p>
<p><a href="https://medium.com/deep-reinforcement-learning-course/launching-deep-reinforcement-learning-course-v2-0-38fa3c24bcbc">Course introduction (from 2020)</a></p>
<p>Everything available in <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course">github</a></p>
<p>I appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine..</p>
<section id="chapter-1---an-introduction-to-deep-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="chapter-1---an-introduction-to-deep-reinforcement-learning">(2/19/21) - <a href="https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c">Chapter 1</a> - An Introduction to Deep Reinforcement Learning?</h2>
<p>Previous version from <a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419">2018: What is Deep Reinforcement Learning?</a> is quite interesting. With 3 parts:</p>
<ul>
<li>What Reinforcement Learning is, and how rewards are the central idea</li>
<li>The three approaches of Reinforcement Learning</li>
<li>What the “Deep” in Deep Reinforcement Learning means</li>
</ul>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*aKYFRoEmmKkybqJOvLt2JQ.png" class="img-fluid"></p>
<p>Rewards, long-term future reward, discount rate.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*zrzRTXt8rtWF5fX__kZ-yQ.png" class="img-fluid"></p>
<p>Episodic (starting and ending point) vs Continuous (e.g.&nbsp;stock trading) tasks.</p>
<p>Way of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step)</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*LLfj11fivpkKZkwQ8uPi3A.png" class="img-fluid"></p>
<p>Exploration/Exploitation trade off. Will see later different ways to handle that trade-off.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*APLmZ8CVgu0oY3sQBVYIuw.png" class="img-fluid"></p>
<section id="three-approaches-to-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="three-approaches-to-reinforcement-learning">Three approaches to Reinforcement Learning</h3>
<p>These are value-based, policy-based, and model-based.</p>
<section id="value-based" class="level4">
<h4 class="anchored" data-anchor-id="value-based">Value Based</h4>
<p>In value-based RL, the goal is to optimize the value function <em>V(s)</em>.</p>
<p>The value function is a function that tells us the maximum expected future reward the agent will get at each state.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*2_JRk-4O523bcOcSy1u31g.png" class="img-fluid"></p>
</section>
<section id="policy-based" class="level4">
<h4 class="anchored" data-anchor-id="policy-based">Policy Based</h4>
<p>In policy-based RL, we want to directly optimize the policy function <em>π(s)</em> without using a value function.</p>
<p>The policy is what defines the agent behavior at a given time.</p>
<p>We have two types of policy:</p>
<ul>
<li>Deterministic: a policy at a given state will always return the same action.</li>
<li>Stochastic: output a distribution probability over actions.</li>
</ul>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*fii7Z01laRGateAJDvloAQ.png" class="img-fluid"></p>
</section>
<section id="model-based" class="level4">
<h4 class="anchored" data-anchor-id="model-based">Model Based</h4>
<p>In model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course.</p>
</section>
</section>
<section id="deep-reinforcement-learning" class="level3">
<h3 class="anchored" data-anchor-id="deep-reinforcement-learning">Deep Reinforcement Learning</h3>
<p>In Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge.</p>
<p>Deep Learning allows to approximate this Q function.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png" class="img-fluid"></p>
<p><a href="https://thomassimonini.medium.com/an-introduction-to-deep-reinforcement-learning-17a565999c0c">Updated version</a> from 2020 (and <a href="https://www.youtube.com/watch?v=q0BiUn5LiBc">video</a> version)</p>
<p>This is a good starting point, well explained.</p>
<p>Reinforcement Learning is just a <strong>computational approach of learning from action.</strong></p>
<p><strong>A formal definition</strong></p>
<blockquote class="blockquote">
<p>Reinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that <strong>learn from the environment</strong> by <strong>interacting with it</strong> through trial and error and <strong>receiving rewards</strong> (positive or negative) <strong>as unique feedback.</strong></p>
</blockquote>
<p>Some explanations about <strong>observations</strong> (partial description) vs <strong>states</strong> (fully observed envt). Only differs in implementation, all theoretical background stays the same.</p>
<p>Action space where we can distinguish <strong>discrete</strong> (e.g.&nbsp;fire, up) actions from <strong>continuous</strong> (e.g.&nbsp;turn 23deg) ones.</p>
<p><strong>Reward</strong> part is the same as the one from 2018. With cheese, mouse, maze example.</p>
<p><strong>Episodic</strong> and <strong>continuous</strong> tasks part is the same as the one from 2018.</p>
<p><strong>Exploration/Exploitation trade-off</strong> is explained the same way + an additional example taken from <a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa18/">berkley - CS 294-112</a> - Deep Reinforcement Learning course. I want to learn more about this course!</p>
<p>About <strong>solving RL problems</strong>, it is now presented as 2 main approaches:</p>
<ul>
<li><strong>policy-based</strong> methods</li>
<li><strong>value-based</strong> methods</li>
</ul>
<p><img src="https://miro.medium.com/max/700/1*Vujmmyswrg2wIjmpvSUBfg.png" class="img-fluid"></p>
<p>And bedore to explain that, nice presentation of what is a <strong>policy <span class="math inline">\(\pi\)</span></strong>. Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method.</p>
<p>There is an explanation about different types of policy: <strong>deterministic</strong> and <strong>stochastic</strong>.</p>
<p>And that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the <a href="https://castorfou.github.io/guillaume_blog/deep%20learning/mit/tensorflow/2021/02/05/learning-MIT-6.S191-2021.html">one</a> (version 2021) I follow these days.</p>
</section>
</section>
<section id="chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi">(2/19/21) - <a href="https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-1-2-3e8f5e764358">Chapter 2 - part 1</a> - Q-Learning, let’s create an autonomous Taxi</h2>
<p>And in <a href="https://www.youtube.com/watch?v=230bR2DrbdE&amp;feature=emb_logo">video</a> (I like to read + watch the video at the same time)</p>
<p>Here in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2:</p>
<p><img src="https://miro.medium.com/max/700/1*2yYWVAXJh4FI2lpsL0ajwQ.png" class="img-fluid"></p>
<section id="value-based-method" class="level4">
<h4 class="anchored" data-anchor-id="value-based-method">Value-based method</h4>
<p>Remember what we mean in value-based method</p>
<p><img src="https://miro.medium.com/max/700/1*jfUUaZuHUa1h61oD6O18KA.png" class="img-fluid"></p>
<p>you don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action.</p>
</section>
<section id="bellman-equation" class="level4">
<h4 class="anchored" data-anchor-id="bellman-equation"><strong>Bellman equation</strong></h4>
<p>each value as the sum of the expected return, <strong>which is a long process.</strong> This is equivalent <strong>to the sum of immediate reward + the discounted value of the state that follows.</strong></p>
<p><img src="https://miro.medium.com/max/700/1*FMjoVEELvz0oKcIfmcvGPQ.png" class="img-fluid"></p>
</section>
<section id="monte-carlo-vs-temporal-difference" class="level4">
<h4 class="anchored" data-anchor-id="monte-carlo-vs-temporal-difference">Monte Carlo vs Temporal Difference</h4>
<p>And then an explanation about 2 types of method to learn a policy or a value-function:</p>
<ul>
<li><em>Monte Carlo</em>: learning at the end of the episode. With <em>Monte Carlo</em>, we update the value function from a complete episode and so we <strong>use the actual accurate discounted return of this episode.</strong></li>
<li><em>TD learning</em>: learning at each step. With <em>TD learning</em>, we update the value function from a step, so we replace Gt that we don’t have with <strong>an estimated return called TD target.</strong> (chich is the immediate reward + the discounted value of the next state)</li>
</ul>
<p><img src="https://miro.medium.com/max/700/1*c8nfnXRu8n1h78bWPEK5vg.png" class="img-fluid"></p>
<p>It was not clear to me that these methods could be used for policy-based approach. It is now!</p>
</section>
</section>
<section id="chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi" class="level2">
<h2 class="anchored" data-anchor-id="chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi">(2/24/21) - <a href="https://thomassimonini.medium.com/q-learning-lets-create-an-autonomous-taxi-part-2-2-8cbafa19d7f5">Chapter 2 - part 2</a> - Q-Learning, let’s create an autonomous Taxi</h2>
<p>But the video is not yet available.</p>
<p><strong>What is Q-Learning?</strong></p>
<p>Q-Learning is an <strong>off-policy value-based method that uses a TD approach to train its action-value function:</strong></p>
<ul>
<li><em>“Off-policy”</em>: we’ll talk about that at the end of this chapter.</li>
<li><em>“Value-based method”</em>: it means that it finds its optimal policy indirectly by training a value-function or action-value function that will tell us what’s <strong>the value of each state or each state-action pair.</strong></li>
<li><em>“Uses a TD approach”</em>: <strong>updates its action-value function at each step.</strong></li>
</ul>
<p>Q stands for quality (quality of action). After training we’ll get the optimal Q-function.</p>
<p>When choosing an action, we have to balance between exploration and exploitation with <span class="math display">\[\epsilon\]</span> - greedy:</p>
<p><img src="https://miro.medium.com/max/700/1*AYz65tJDERsWTg2DGEJ35g.png" class="img-fluid"></p>
<p>But at beginning Q table is not trained yet so we have to increase exploitation. It is done with some decreasing <span class="math display">\[\epsilon\]</span>.</p>
<p><img src="https://miro.medium.com/max/700/1*1J2lJN48gdjeuoRBqsO_CA.png" class="img-fluid"></p>
<p>The Q-learning algorithm is a 4-step process:</p>
<ul>
<li>step1: Q-Table init</li>
<li>step2: Choose action (<span class="math display">\[\epsilon\]</span> - greedy strategy)</li>
<li>step3: Perform action A<sub>t</sub> and get R<sub>t+1</sub> and S<sub>t+1</sub></li>
<li>step4: Update Q(S<sub>t</sub>, A<sub>t</sub>)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://miro.medium.com/max/700/1*teZ5KRfvYjMKZnmhaWTUXg.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Update Q(S<sub>t</sub>, A<sub>t</sub>)</figcaption><p></p>
</figure>
</div>
<p>Why it is called <strong>off-policy</strong>? Because we don’t have the same logic to select action (<span class="math display">\[\epsilon\]</span> - greedy) and update Q (greedy).</p>
<p>With <em>On-policy:</em> we use the <strong>same policy for acting and updating.</strong> Sarsa is such an algorithm.</p>
<p><img src="https://miro.medium.com/max/700/1*gVl6V-wbX_hOoNQATx081Q.png" class="img-fluid"></p>
<p>Nice and simple manual example with mouse, cheese in a maze. We run Q-learning and make all calculation by hands.</p>
<p><img src="https://miro.medium.com/max/500/1*GMuThIF7aNj-V_d6hTRN8A.png" class="img-fluid"></p>
<p><todo> </todo></p>
<div class="alert alert-info">
implement with numpy+gym this algorithm should be a nice exercise.
</div>
<p>There is an exercise to implement a taxi, within this <a href="https://colab.research.google.com/gist/simoninithomas/466c81aa1c2a07dd14793240c6d033c5/q-learning-with-taxi-v3.ipynb#scrollTo=20tSdDbxxK_H">notebook</a> at colab google. Taxi V3 is an env from opengym.</p>
</section>
<section id="back-to-2018---chapter-3---deep-q-learning-with-doom" class="level2">
<h2 class="anchored" data-anchor-id="back-to-2018---chapter-3---deep-q-learning-with-doom">(3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom</h2>
<p><a href="https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8">Article</a>, <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb">Notebook</a>, <a href="https://youtu.be/gCJyVX98KJ4">Video</a></p>
<p>We’ll create an agent that learns to play Doom. Doom is a big environment with a gigantic state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient at all.</p>
<p>The best idea in this case is to create a <a href="http://neuralnetworksanddeeplearning.com/">neural network</a> that will approximate, given a state, the different Q-values for each action.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*w5GuxedZ9ivRYqM_MLUxOQ.png" class="img-fluid"></p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*LglEewHrVsuEGpBun8_KTg.png" class="img-fluid"></p>
<p>Addresses pb of temporal limitation: get multiple frames to have sense of motion.</p>
<p>Video is nice because it goes from start and follows closely all steps.</p>
<p>I wil try to implement in my own by creating an environment and running under a clone of Deep_reinforcement_learning_Course <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course">Thomas’s repo</a></p>
<p>Here at <a href="https://github.com/castorfou/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb">Deep Q learning with Doom.ipynb</a></p>
<p>I had to switch to tensorflow-gpu 1.13. Manage some cuda memory issue. But then was able to run it.</p>
<p>However as Thomas says, I should do it step by step on my own.</p>
</section>
<section id="chapter-4-improvements-in-deep-q-learning-v1" class="level2">
<h2 class="anchored" data-anchor-id="chapter-4-improvements-in-deep-q-learning-v1">(3/10/21) - Chapter 4: Improvements in Deep Q Learning V1</h2>
<p><a href="https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682">Article</a>, <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb">Notebook</a>, <a href="https://www.youtube.com/watch?v=-Ynjw0Vl3i4&amp;feature=emb_title">Video</a></p>
<p>four strategies that improve — dramatically — the training and the results of our DQN agents:</p>
<ul>
<li>fixed Q-targets</li>
<li>double DQNs</li>
<li>dueling DQN (aka DDQN)</li>
<li>Prioritized Experience Replay (aka PER)</li>
</ul>
<p><strong>fixed Q-targets</strong> to avoid chasing a moving target</p>
<ul>
<li>Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target.</li>
<li>At every <span class="math display">\[\Tau\]</span> step, we copy the parameters from our DQN network to update the target network.</li>
</ul>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*D9i0I2EO7LKL2aAb2HLfTg.png" class="img-fluid"></p>
<p><a href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/">Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed…</a></p>
<p><em>Implementation</em></p>
<p>Implementing fixed q-targets is pretty straightforward:</p>
<ul>
<li><p>First, we create two networks (<code>DQNetwork</code>, <code>TargetNetwork</code>)</p></li>
<li><p>Then, we create a function that will take our <code>DQNetwork</code> parameters and copy them to our <code>TargetNetwork</code></p></li>
<li><p>Finally, during the training, we calculate the TD target using our target network. We update the target network with the <code>DQNetwork</code> every <span class="math display">\[\Tau\]</span> step (<span class="math display">\[\Tau\]</span> is an hyper-parameter that we define).</p></li>
</ul>
<p><strong>double DQNs</strong> to handle overestimating of Q-values (at the beginning of training, taking the maximum q value (which is noisy) as the best action to take can lead to false positives)</p>
<p>we move from this TD target logic</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*KsQ46R8zyTQlKGv91xi6ww.png" class="img-fluid"></p>
<p>to the use of 2 networks</p>
<ul>
<li>use our DQN network to select what is the best action to take for the next state (the action with the highest Q value).</li>
<li>use our target network to calculate the target Q value of taking that action at the next state.</li>
</ul>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*g5l4q162gDRZAAsFWtX7Nw.png" class="img-fluid"></p>
<p><em>Implementation</em></p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*oyGR6gJ4WyqeKOfq0Cd8iQ.png" class="img-fluid"></p>
<p><strong>Dueling DQN (aka DDQN)</strong></p>
<p>based on this paper <a href="https://arxiv.org/pdf/1511.06581.pdf">Dueling Network Architectures for Deep Reinforcement Learning</a>.</p>
<p>With DDQN, we want to separate the estimator of these two elements, using two new streams:</p>
<ul>
<li>one that estimates the <strong>state value V(s)</strong></li>
<li>one that estimates the <strong>advantage for each action A(s,a)</strong></li>
</ul>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*FkHqwA2eSGixdS-3dvVoMA.png" class="img-fluid"></p>
<p>and this can be combined with <strong>Prioritized experience replay</strong>.</p>
<p>This is nicely explained in this <a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/">article</a>. DDQN explanation is clearer than Thomas’.</p>
<p>The key here is to deal efficiently with experiences. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay (PER) is one strategy that tries to leverage this fact by changing the sampling distribution.</p>
<p>I guess there are several options to manage this prioritization (we would prefer transitions that do not fit well to our current estimate of Q function). And a key aspect is the performance of this selection. One implementation is SumTree.</p>
<p>I have to see full implementation in the notebook to fully understand the logic.</p>
<p>About the <strong>video</strong></p>
<p>Thomas has insisted about the importance to master these architecture (DQN then DDQN, etc) before going further with state of the art architectures (Policy Gradient, PPO…)</p>
<p>Approach in videos is now different. In previous videos it was about explaining articles. Now it is more turned to implementation details based on notebooks.</p>
<p>Thomas has given a reference to Arthur Juliani who is a senior ML engineer at <a href="https://unity.com/fr/products/machine-learning-agents">Unity</a>. I would like to browse though this reference and see what can be done.</p>
<p>Should follow video and run/update notebook in //.</p>
</section>
<section id="chapter-5-policy-gradients-v1" class="level2">
<h2 class="anchored" data-anchor-id="chapter-5-policy-gradients-v1">(3/17/21) - Chapter 5: Policy Gradients V1</h2>
<p><a href="https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/">Article</a>, <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/Policy%20Gradients">Notebook</a>, <a href="https://www.youtube.com/watch?v=wLTQRuizVyE">Video</a></p>
<p>In policy-based methods, instead of learning a value function that tells us what is the expected sum of rewards given a state and an action, we learn directly the policy function that maps state to action (select actions without using a value function).</p>
<p>3 main advantages to use Policy Gradients vs Q learning:</p>
<ul>
<li>convergence - have better convergence properties</li>
<li>effective in high dimension, or with continuous actions</li>
<li>stochastic policy - no need for exploration,/exploitation tradeoff</li>
</ul>
<p>But can be longer to train.</p>
<p><strong>Policy search</strong></p>
<p>We can dfine our policy as the probability distribution of actions (for a given state)</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/0*354cfoILK19WFTWa." class="img-fluid"></p>
<p>And how good is this policy? Measured with J(<span class="math display">\[\theta\]</span>)</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/0*PfUAJaIGoEsvfbCG." class="img-fluid"></p>
<p>We must find <span class="math display">\[\theta\]</span> to maximize J(<span class="math display">\[\theta\]</span>). How?</p>
<p>2 steps:</p>
<ul>
<li>Measure the quality of a π (policy) with a policy score function J(θ)</li>
<li>Use policy gradient ascent to find the best parameter θ that improves our π.</li>
</ul>
<p><strong>the Policy Score function J(θ)</strong></p>
<p>3 ways (maybe more)</p>
<p>Calculate the mean of the return from the first time step (G1). This is the cumulative discounted reward for the entire episode.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*tP4l4IrIG3aMLTrMt-1-HA.png" class="img-fluid"></p>
<p>In a continuous environment, we can use the average value, because we can’t rely on a specific start state. Each state value is now weighted (because some happen more than others) by the probability of the occurrence of the respected state.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*S-XLkrvPuVUqLrFW1hmIMg.png" class="img-fluid"></p>
<p>Third, we can use the average reward per time step. The idea here is that we want to get the most reward per time step.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*3SejRRby6vAnThZ8c2UaQg.png" class="img-fluid"></p>
<p><strong>Policy gradient ascent</strong></p>
<p>because we want to maximize our Policy score function</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/0*oh-lF13hYWt2Bd6V." class="img-fluid"></p>
<p>The solution will be to use the Policy Gradient Theorem. This provides an analytic expression for the gradient ∇ of J(θ) (performance) with respect to policy θ that does not involve the differentiation of the state distribution. (using <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">likelihood ratio trick</a>)</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*iKhO5anOAfc3oqJOM2i_8A.png" class="img-fluid"></p>
<p>It gives</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*zjEh737KfmDUzNECjW4e4w.png" class="img-fluid"></p>
<p>R(<span class="math display">\[\tau\]</span>) is like a scalar value score.</p>
<p><strong>Implementation</strong></p>
<p>As with the previous section, this is good to watch the video at the same time.</p>
<p>And now this is the implementation in</p>
<p><a href="https://github.com/castorfou/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Doom%20Deathmatch/Doom-deathmatch%20REINFORCE%20Monte%20Carlo%20Policy%20gradients.ipynb">doom deathmatch notebook</a></p>
<p><img src="https://github.com/castorfou/Deep_reinforcement_learning_Course/raw/master/Policy%20Gradients/Doom%20Deathmatch/assets/doomPG1.png" class="img-fluid"></p>
<p>as with Pong, we <a href="https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/">stack</a> frames to understand dynamic with deque.</p>
<p>Even with GPU growth setup, I run an error after the 1st epoch.</p>
<pre><code>==========================================
Epoch:  1 / 5000

Number of training episodes: 15
Total reward: 7.0
Mean Reward of that batch 0.4666666666666667
Average Reward of all training: 0.4666666666666667
Max reward for a batch so far: 7.0</code></pre>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">ResourceExhaustedError:</span> OOM when allocating tensor with shape<span class="pp">[</span><span class="ss">5030,32,24,39</span><span class="pp">]</span> and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>     <span class="ex">[[{{node</span> PGNetwork/train/gradients/PGNetwork/conv2/conv2/Conv2D_grad/Conv2DBackpropInput}}]]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Hint:</span> If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I have to reduce batch size (to 1000) to make it work.</p>
<p>And I can monitor gpu memory consumption with <code>watch nvidia-smi</code></p>
<p><img src="../images/watch_nvidia_smi.png" class="img-fluid"></p>
<p>or we can use <code>gpustat -i 2</code></p>
<p>[0] Quadro RTX 4000 | 59’C, <strong>34 %</strong>, 39 W | 7819 / 7982 MB | explore(6729M) gdm(162M) explore(388M) explore(282M) explore(86M) explore(89M) explore(3M)</p>
</section>
<section id="chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1" class="level2">
<h2 class="anchored" data-anchor-id="chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1">(3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1</h2>
<p><a href="https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/">Article</a>, <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/A2C%20with%20Sonic%20the%20Hedgehog">Notebook</a>, <a href="https://www.youtube.com/watch?v=GCfUdkCL7FQ">Video</a></p>
<p>“hybrid method”: <strong>Actor Critic</strong>. We’ll using two neural networks:</p>
<ul>
<li>an <strong>Actor</strong> that controls how our agent behaves (policy-based)</li>
<li>a <strong>Critic</strong> that measures how good the action taken is (value-based)</li>
</ul>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*e1N-YzQmJt-5KwUkdUvAHg.png" class="img-fluid"></p>
<p>Actor is using a <strong>policy</strong> function <span class="math display">\[
\pi(s, a, \theta)
\]</span> Critic is using a <strong>value</strong> function</p>
<p><span class="math display">\[
\widehat{q}(s,a,w)
\]</span> Which means 2 sets of weights to be optimized separately <span class="math display">\[\theta\]</span> and w.</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*KlX2-kNXRYLAYpdnI8VPiA.png" class="img-fluid"></p>
<p>We can use advantage function to stabilize learning:</p>
<p><img src="https://cdn-media-1.freecodecamp.org/images/1*SvSFYWx5-u5zf38baqBgyQ.png" class="img-fluid"></p>
<section id="two-different-strategies-asynchronous-or-synchronous" class="level4">
<h4 class="anchored" data-anchor-id="two-different-strategies-asynchronous-or-synchronous">Two different strategies: Asynchronous or Synchronous</h4>
<p>We have two different strategies to implement an Actor Critic agent:</p>
<ul>
<li>A2C (aka Advantage Actor Critic)</li>
<li>A3C (aka Asynchronous Advantage Actor Critic)</li>
</ul>
<p>Here we focus on A2C.</p>
<p><strong>(3/22/21) - Implementation and video</strong></p>
<p>It is a little bit confusing. I won’t run it. I would have liked a more pregressive approach and to understand all steps Thomas did to get to that final implementation.</p>
</section>
</section>
<section id="chapter-7-proximal-policy-optimization-ppo-v1" class="level2">
<h2 class="anchored" data-anchor-id="chapter-7-proximal-policy-optimization-ppo-v1">(3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1</h2>
<p><a href="https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e">Article</a>, <a href="https://github.com/simoninithomas/Deep_reinforcement_learning_Course/tree/master/PPO%20with%20Sonic%20the%20Hedgehog">Notebook</a></p>
<p>The central idea of Proximal Policy Optimization is to avoid having too large policy update. (we use a ratio that will tells us the difference between our new and old policy and clip this ratio from 0.8 to 1.2)</p>
<p><strong>Clipped Surrogate Objective Function</strong></p>
<p><img src="https://miro.medium.com/max/700/0*Ab-_UqHukYN6syxS.png" class="img-fluid"></p>
<p>We will penalize changes that lead to a ratio that will away from 1 (in the paper ratio can only vary from 0.8 to 1.2). <strong>By doing that we’ll ensure that not having too large policy update because the new policy can’t be too different from the older one.</strong></p>
<p>2 implementations are known TRPO (Trust Region Policy Optimization) and PPO clip. TRPO being complex and costly, we focus on PPO:</p>
<p><img src="https://miro.medium.com/max/700/0*Dk8XFEOzI7yaSfLE.png" class="img-fluid"></p>
<p>And the final loss will be:</p>
<p><img src="https://miro.medium.com/max/700/1*T0D50EPz-oqGDn55uHv9IA.png" class="img-fluid"></p>
<p><strong>Now the <a href="https://github.com/castorfou/Deep_reinforcement_learning_Course/tree/master/PPO%20with%20Sonic%20the%20Hedgehog">implementation</a></strong></p>
<p>By looking at the implementation, I ran into <a href="https://github.com/DLR-RM/stable-baselines3">Stable baselines3</a>.</p>
<p>This is a major update of Stable Baselines based on pytorch. It seems interesting!</p>
<p>I like this comment from Stable Baselines3 in the <a href="https://araffin.github.io/post/sb3/">v1.0 blog post</a>:</p>
<blockquote class="blockquote">
<p><strong>Motivation</strong></p>
<p>Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often <a href="https://arxiv.org/abs/1709.06560">difficult to reproduce</a>. A major challenge is that small implementation details can have a substantial effect on performance – often greater than the <a href="https://iclr.cc/virtual_2020/poster_r1etN1rtPB.html">difference between algorithms</a>. It is particularly important that implementations used as experimental <em>baselines</em> are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements.</p>
<p>To help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the <a href="https://github.com/openai/gym">OpenAI Gym interface</a>.</p>
</blockquote>
<p>I will create a new blog entry about Stable Baselines3.</p>
<p>as for previous notebook, I need to purchase Sonic2-3 to make it worked. Not for now maybe later.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/^(?:http:|https:)\/\/castorfou\.github\.io\//);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>