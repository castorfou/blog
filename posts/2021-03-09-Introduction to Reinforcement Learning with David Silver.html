<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-03-09">
<meta name="description" content="From deepmind. My notes">

<title>Guillaume’s blog - Introduction to Reinforcement Learning with David Silver</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon_small.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-12YC1FPHWN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-12YC1FPHWN', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Guillaume’s blog - Introduction to Reinforcement Learning with David Silver">
<meta name="twitter:description" content="From deepmind. My notes">
<meta name="twitter:image" content="https://castorfou.github.io/posts/images/RL.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Guillaume’s blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/castorfou" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/GuillaumeRamel1" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Introduction to Reinforcement Learning with David Silver</h1>
                  <div>
        <div class="description">
          From deepmind. My notes
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">reinforcement learning</div>
                <div class="quarto-category">deepmind</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 9, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#lecture-1-introduction-to-reinforcement-learning" id="toc-lecture-1-introduction-to-reinforcement-learning" class="nav-link active" data-scroll-target="#lecture-1-introduction-to-reinforcement-learning">3/9/21 - Lecture 1: Introduction to Reinforcement Learning</a></li>
  <li><a href="#lecture-2-markov-decision-processes" id="toc-lecture-2-markov-decision-processes" class="nav-link" data-scroll-target="#lecture-2-markov-decision-processes">3/10/21 - Lecture 2: Markov Decision Processes</a></li>
  <li><a href="#lecture-3-planning-by-dynamic-programming" id="toc-lecture-3-planning-by-dynamic-programming" class="nav-link" data-scroll-target="#lecture-3-planning-by-dynamic-programming">3/12/21 - Lecture 3: Planning by Dynamic Programming</a></li>
  <li><a href="#lecture-4-model-free-prediction" id="toc-lecture-4-model-free-prediction" class="nav-link" data-scroll-target="#lecture-4-model-free-prediction">3/15/21 - Lecture 4: Model-Free Prediction</a></li>
  <li><a href="#lecture-5-model-free-control" id="toc-lecture-5-model-free-control" class="nav-link" data-scroll-target="#lecture-5-model-free-control">3/18/21 - Lecture 5: Model-Free Control</a></li>
  <li><a href="#lecture-6-value-function-approximation" id="toc-lecture-6-value-function-approximation" class="nav-link" data-scroll-target="#lecture-6-value-function-approximation">4/27/21 - Lecture 6: Value Function Approximation</a></li>
  <li><a href="#lecture-7-policy-gradient-methods" id="toc-lecture-7-policy-gradient-methods" class="nav-link" data-scroll-target="#lecture-7-policy-gradient-methods">5/4/21 - Lecture 7: Policy Gradient Methods</a></li>
  <li><a href="#lecture-8-integrating-learning-and-planning" id="toc-lecture-8-integrating-learning-and-planning" class="nav-link" data-scroll-target="#lecture-8-integrating-learning-and-planning">6/21/21 - Lecture 8: Integrating Learning and Planning</a></li>
  <li><a href="#lecture-9-exploration-and-exploitation" id="toc-lecture-9-exploration-and-exploitation" class="nav-link" data-scroll-target="#lecture-9-exploration-and-exploitation">7/1/21 - Lecture 9: Exploration and exploitation</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.</p>
<p><a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver">Website with 10 lectures: videos and slides</a></p>
<p><a href="https://github.com/castorfou/introduction-reinforcement-learning-david-silver">My repo with slides</a></p>
<p><img src="../images/deepmind_sylabus.png" class="img-fluid"></p>
<section id="lecture-1-introduction-to-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="lecture-1-introduction-to-reinforcement-learning">3/9/21 - Lecture 1: Introduction to Reinforcement Learning</h2>
<p>This introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context:</p>
<ul>
<li>Definitions: rewards, actions, agent, environment, state (and history)</li>
<li>Major components: policy, value function, model</li>
<li>Categorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based</li>
<li>Learning and planning</li>
<li>Prediction and control</li>
</ul>
<p>And David gives 2 references:</p>
<ul>
<li><a href="../guillaume_blog/blog/reinforcement-learning-readings.html">well known</a> Introduction to Reinforcement Learning, Sutton and Barto, 1998</li>
<li>Algorithms for Reinforcement Learning, Szepesvari. Available <a href="http://www.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">online</a>.</li>
</ul>
<p>Policy <span class="math display">\[\pi\]</span>(s): essentially a map from state to action. Can be deterministic <span class="math display">\[\pi\]</span>(s) or stochastic <span class="math display">\[\pi\]</span>(a|s).</p>
<p>Value function v<sub><span class="math inline">\(\pi\)</span></sub>(s): is a prediction of expected future reward.</p>
<p>Model: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g.&nbsp;based on dynamics). Reward model predicts the next immediate reward.</p>
<p>A lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL.</p>
<p><img src="../images/deepmind_lec1_taxonomy.png" class="img-fluid"></p>
<p>And then David explains 2 fundamental different problems with Learning vs Planning.</p>
<p>With Learning, environment is unknown, agent interacts directly with the environment and improves its policy.</p>
<p>With Planning, a model of environment is known, and agent “plays” with this model and improves its policy.</p>
<p>These 2 problems may be linked where you start to learn from the environment and apply planning then.</p>
<p>2 examples based on atari games.</p>
<p>Another topic is exploration vs exploitation then prediction and control.</p>
</section>
<section id="lecture-2-markov-decision-processes" class="level2">
<h2 class="anchored" data-anchor-id="lecture-2-markov-decision-processes">3/10/21 - Lecture 2: Markov Decision Processes</h2>
<p><strong><em>Markov decision processes</em></strong> formally describe an environment for reinforcement learning.</p>
<p><strong><em>Markov property</em></strong>: the future is independent of the past given the present.</p>
<p><strong><em>Markov Process</em></strong> (or <strong><em>Markov Chain</em></strong>) is the tuple (S, P)</p>
<p><img src="../images/deepmind_lec2_markovchain.png" class="img-fluid"></p>
<p>We can take sample episodes from this chain. (e.g.&nbsp;C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep)</p>
<p>We can formalize the transition matrix from s to s’.</p>
<p>When you add reward you get <strong><em>Markov reward process</em></strong> (S, P, R, <span class="math display">\[\gamma\]</span>)</p>
<p>Reward here is a function to map for each state the immediate reward.</p>
<p><span class="math display">\[\gamma\]</span> is the discounted factor, <span class="math display">\[\epsilon\]</span> [0,1]. David explains why we could need such discount.</p>
<p><strong><em>Return</em></strong> Gt is the total discounted reward at time-step t for a given sample.</p>
<p><img src="../images/deepmind_lec2_return.png" class="img-fluid"></p>
<p><strong><em>Value function</em></strong> v(s) is really what we care about, it is the long-term value of state s.</p>
<p><img src="../images/deepmind_lec2_valuefunction.png" class="img-fluid"></p>
<p><strong><em>Bellman Equation for MRPs</em></strong></p>
<p>The value function can be decomposed into two parts: - immediate reward R<sub>t+1</sub> - discounted value of next state <span class="math display">\[\gamma\]</span>.v (S<sub>t+1</sub>)</p>
<p><img src="../images/deepmind_lec2_bellman.png" class="img-fluid"></p>
<p>We use that to calculate value function with <span class="math display">\[\gamma\]</span> <span class="math inline">\(\neq\)</span> 0.</p>
<p>And calculating value function can be seen as the resolution of this linear equation:</p>
<p><img src="../images/deepmind_lec2_bellman_solving.png" class="img-fluid"></p>
<p>And now we introduce actions and it gives <strong><em>Markov Decision Process</em></strong></p>
<p><img src="../images/deepmind_lec2_mdp.png" class="img-fluid"></p>
<p>And we introduce policy</p>
<p><img src="../images/deepmind_lec2_policy.png" class="img-fluid"></p>
<p>Then we can define the <em>state-value function</em> v<sub><span class="math inline">\(\pi\)</span></sub>(s,a) for a given policy <span class="math display">\[\pi\]</span></p>
<p><img src="../images/deepmind_lec2_statevaluefunction.png" class="img-fluid"></p>
<p>and <em>action-value function</em> q<sub><span class="math inline">\(\pi\)</span></sub>(s,a) for a given policy <span class="math display">\[\pi\]</span></p>
<p><img src="../images/deepmind_lec2_actionvaluefunction.png" class="img-fluid"></p>
<p>And impact on Bellman Equation ends like that:</p>
<p><img src="../images/deepmind_lec2_bellman_mdp.png" class="img-fluid"></p>
<p>v is giving us how good it is to be in a state. q is giving us how good is it to take an action.</p>
<p>And then we have the Bellman equation expressed with v and q.</p>
<p>We don’t care much about a given v<sub><span class="math inline">\(\pi\)</span></sub>, we want to get the best policy. And ultimately to get q<sub>*</sub> which is the <strong>optimal action value function</strong>.</p>
<p><img src="../home/explore/git/guillaume/blog/images/deepmind_lec2_optimal_value_function.png" class="img-fluid"></p>
<p>The optimal value function specifies the best possible performance in the MDP. A MDP is “solved” when we know the optimal value function q<sub>*</sub>.</p>
<p>What we really care about is <strong>optimal policy</strong> <span class="math display">\[\pi\]</span><sub>*</sub>. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy.</p>
<p>So the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average.</p>
<p>So no we can write the <strong>Bellman Optimality Equation</strong>. Unfortunately this is non-linear.</p>
<p>There are many approaches such as iterative ones.</p>
<ul>
<li>Value Iteration</li>
<li>Policy Iteration</li>
<li>Q-learning</li>
<li>Sarsa</li>
</ul>
</section>
<section id="lecture-3-planning-by-dynamic-programming" class="level2">
<h2 class="anchored" data-anchor-id="lecture-3-planning-by-dynamic-programming">3/12/21 - Lecture 3: Planning by Dynamic Programming</h2>
<p>Will discuss from the agent side: how to solve these MDP problems.</p>
<p>David starts with general ideas on dynamic programming. (programming in a sense of policy)</p>
<p>Value function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition.</p>
<p><strong>Planning by Dynamic Programming</strong></p>
<p>We assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages:</p>
<ul>
<li>prediction: given MDP and policy <span class="math display">\[\pi\]</span>, we predict the value of this policy v<sub><span class="math inline">\(\pi\)</span></sub>.</li>
<li>control: given MDP, we get optimal value function v<sub>*</sub> and optimal policy <span class="math inline">\(\pi\)</span><sub>*</sub>.</li>
</ul>
<p>And by full MDP it would mean for an atari game to have access to internal code to calculate everything.</p>
<p>We need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one.</p>
<p><strong>Policy Evaluation</strong></p>
<p>Problem: evaluate a given policy π Solution: iterative application of Bellman expectation backup</p>
<p>(Bellman expectation is used in prediction, Bellman optimality is used in control)</p>
<p>David takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column)</p>
<p><strong>Policy Iteration</strong></p>
<p>In small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of evaluation (iterative policy evaluation) / improvement (greedy policy). But this process of policy iteration always converges to π∗</p>
<p>David uses Jack’s Car Rental where it needs 4 steps to get the optimal policy. And explains why acting greedy improves the policy. And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy.</p>
<p>Some question then about convergence of v<sub><span class="math inline">\(\pi\)</span></sub> . Why not update policy at each step of evaluation -&gt; this is value iteration.</p>
<p><strong>Value Iteration</strong></p>
<p>Problem: find optimal policy π Solution: iterative application of Bellman optimality backup</p>
<p><strong>Extensions to dynamic programming</strong></p>
<p>DP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive.</p>
<p>One solution is to <strong>sample backups</strong>.</p>
<p>Advantages: Model-free: no advance knowledge of MDP required Breaks the curse of dimensionality through sampling Cost of backup is constant, independent of n = |S|</p>
</section>
<section id="lecture-4-model-free-prediction" class="level2">
<h2 class="anchored" data-anchor-id="lecture-4-model-free-prediction">3/15/21 - Lecture 4: Model-Free Prediction</h2>
<p>Model-Free: no-one gives us the MDP. And we still want to solve it.</p>
<ul>
<li><p><strong>Monte-Carlo learning</strong>: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns.</p></li>
<li><p><strong>Temporal-Difference learning</strong>: goes one step ahead and estimates after one step</p></li>
<li><p><strong>TD(<span class="math display">\[\lambda\]</span>)</strong>: unify both approaches</p></li>
</ul>
<p>We give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning):</p>
<ul>
<li>policy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt)</li>
<li>control (next lecture) - find the optimum value function and then optimum policy</li>
</ul>
<p><strong>Monte-Carlo Reinforcement Learning</strong></p>
<p>We go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean.</p>
<p>It means we use the <em>empirical mean return</em> in place of <em>expected return</em>. (by <em>law of large numbers</em>, this average returns will converge to value function as the number of episodes for that state tends to infinity)</p>
<p><strong>Temporal-Difference Reinforcement Learning</strong></p>
<p>TD learns from incomplete episodes, by bootstrapping</p>
<p>David takes an example from Sutton about predicting time to commute home, comparing MC and TD.</p>
<p>TD target (R<sub>t+1</sub>+<span class="math display">\[\gamma\]</span>V<sub>t+1</sub>) is biased estimate of v<sub><span class="math inline">\(\pi\)</span></sub>(S<sub>t</sub>), but has lower variance than the return G<sub>t</sub>.</p>
<p>David compares perf of MC, TD(0), … using Random Walk example and different values of <span class="math display">\[\alpha\]</span>.</p>
</section>
<section id="lecture-5-model-free-control" class="level2">
<h2 class="anchored" data-anchor-id="lecture-5-model-free-control">3/18/21 - Lecture 5: Model-Free Control</h2>
<p>Distinction between on-policy (learning by doing the job) and off-policy (following someone else behavior)</p>
<p><strong>on-policy</strong></p>
<p>In Monte-Carlo approach, we have 2 issues. First is that we don’t have access to model so we should use Q(s, a) instead of v(s). Second is lack of exploration so we should use <span class="math display">\[\epsilon\]</span>-greedy policy.</p>
<p>With GLIE (Greedy in the Limit with Infinite Exploration), we can update Q after each episodes.</p>
<p>We will now use TD:</p>
<p>Natural idea: use TD instead of MC in our control loop</p>
<ul>
<li>Apply TD to Q(S, A)</li>
<li>Use <span class="math display">\[\epsilon\]</span>-greedy policy improvement</li>
<li>Update every time-step</li>
</ul>
<p>This is SARSA update. Every single time-step we update our diagram.</p>
<p>A generalisation is n-step Sarsa. n=1 is standard Sarsa. n=<span class="math display">\[\infty\]</span> is MC.</p>
<p>To get the best of both worlds, we consider Sarsa(<span class="math display">\[\lambda\]</span>). We have a forward version</p>
<p><img src="../home/explore/git/guillaume/blog/images/deepmind_lec5_sarsal.png" class="img-fluid"></p>
<p>And a backward version which allows online experience. Thanks to eligibility traces.</p>
<p><strong>off-policy</strong></p>
<p>Why is this important?</p>
<ul>
<li>Learn from observing humans or other agents</li>
<li>Re-use experience generated from old policies π 1 , π 2 , …, π t−1</li>
<li>Learn about optimal policy while following exploratory policy</li>
<li>Learn about multiple policies while following one policy</li>
</ul>
<p>We can apply it in importance sampling for off-policy. With Monte-Carlo it is however useless due to high variance. It is imperative to to TD.</p>
<p>We can apply that to Q-learning. We can use greedy slection on target policy <span class="math display">\[\pi\]</span> and <span class="math display">\[\epsilon\]</span> greedy on behaviour policy <span class="math display">\[\mu\]</span>.</p>
</section>
<section id="lecture-6-value-function-approximation" class="level2">
<h2 class="anchored" data-anchor-id="lecture-6-value-function-approximation">4/27/21 - Lecture 6: Value Function Approximation</h2>
<p>How to scale up value function approach.</p>
<p><strong>Value Function Approximation</strong></p>
<p>So far we have represented value function by a lookup table Every state s has an entry V (s) Or every state-action pair s, a has an entry Q(s, a)</p>
<p>Solution for large MDPs: Estimate value function with function approximation v̂ (s, w) ≈ v π (s) or q̂(s, a, w) ≈ q π (s, a) Generalise from seen states to unseen states Update parameter w using MC or TD learning</p>
<p>There are many function approximators, e.g.</p>
<ul>
<li><strong>Linear combinations of features</strong></li>
<li><strong>Neural network</strong></li>
<li>Decision tree</li>
<li>Nearest neighbour</li>
<li>Fourier / wavelet bases</li>
</ul>
<p>We focus on <strong>differentiable</strong> function approximators.</p>
</section>
<section id="lecture-7-policy-gradient-methods" class="level2">
<h2 class="anchored" data-anchor-id="lecture-7-policy-gradient-methods">5/4/21 - Lecture 7: Policy Gradient Methods</h2>
<p>3 methods:</p>
<ul>
<li>finite difference</li>
<li>MC policy gradient</li>
<li>Actor-Critic Policy Gradient</li>
</ul>
<p>advantages of policy based RL vs value based RL:</p>
<ul>
<li>convergence (w/o oscillation that one can see in value based)</li>
<li>effective in continuous action spaces (in some cases taking the max (of q value) can be quite expensive)</li>
<li>policy based RL can learn stochastic policies which can be beneficial in some cases (e.g.&nbsp;rock scissor paper) (usually where you don’t fall into MDP with perfect states representation but we get partially observed environments)</li>
</ul>
<p>Some examples of policy: softmax policy and gaussian policy.</p>
<p>One-step MDP: terminating after 1 time-step. No sequence. In that case we have <span class="math display">\[
J(\theta)=\mathbb{E}_{\pi_\theta}[r]=\sum_{s \in \mathcal{S}}d(s)\sum_{a \in \mathcal{A}}\pi_\theta(s, a)\mathcal{R}_{s,a}
\\
and\\
\nabla_\theta J(\theta) = \sum_{s \in \mathcal{S}}d(s)\sum_{a \in \mathcal{A}}\pi_\theta(s, a)\nabla_\theta \log\pi_\theta(s, a)\mathcal{R}_{s,a}
\\
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log\pi_\theta(s, a)r]
\]</span> Generalization is to replace instantaneous reward r with long-term value <span class="math inline">\(Q_\pi(s,a)\)</span></p>
<p>1st algorithm is <strong>Monte-Carlo policy gradient</strong> (REINFORCE) - tend to be slow, very high variance</p>
<p>Where we sample <span class="math inline">\(Q_\pi(s,a)\)</span> in <span class="math inline">\(v_t\)</span> and regularly update <span class="math inline">\(\theta\)</span>.</p>
<p>Reducing variance using a critic.</p>
<p>We use a critic to estimate the action-value function, <span class="math inline">\(Q_w (s, a) ≈ Q_{π_θ} (s, a)\)</span> Actor-critic algorithms maintain two sets of parameters:</p>
<ul>
<li><strong>Critic</strong> Updates action-value function parameters w</li>
<li><strong>Actor</strong> Updates policy parameters θ, in direction suggested by critic</li>
</ul>
<p>Actor-critic algorithms follow an approximate policy gradient <span class="math display">\[
\nabla_\theta J(\theta) \approx \mathbb{E}_{\pi_\theta}[\nabla_\theta \log\pi_\theta(s, a)Q_w(s, a)]
\\
\Delta\theta=\alpha\nabla_\theta\log\pi_\theta(s, a)Q_w(s, a)
\]</span> Critic will use a policy evaluation (several options seen so far: monte-carlo policy evaluation, TD, TD(<span class="math inline">\(\lambda\)</span>))</p>
</section>
<section id="lecture-8-integrating-learning-and-planning" class="level2">
<h2 class="anchored" data-anchor-id="lecture-8-integrating-learning-and-planning">6/21/21 - Lecture 8: Integrating Learning and Planning</h2>
<p>3 parts in this lecture:</p>
<ul>
<li>model based reinforcement learning</li>
<li>integrated architecture</li>
<li>simulation-based search</li>
</ul>
<p>Learn by model. What we mean by the model is 2 parts: understand transitions (how one state will transition to another state) and reward. If the agent has this understanding, then one can plan with that.</p>
<p>Model-Free RL</p>
<ul>
<li>No model</li>
<li>Learn value function (and/or policy) from experience</li>
</ul>
<p>Model-Based RL</p>
<ul>
<li>Learn a model from experience</li>
<li>Plan value function (and/or policy) from model</li>
</ul>
<p>Model-Based RL (using Sample-Based Planning)</p>
<ul>
<li>Learn a model from real experience</li>
<li>Plan value function (and/or policy) from simulated experience</li>
</ul>
<p>Dyna-Q is a way to combine real experience with simulation.</p>
<p>Simulation-Based Search</p>
<ul>
<li>Forward search paradigm using sample-based planning</li>
<li>Simulate episodes of experience from now with the model</li>
<li>Apply model-free RL to simulated episodes</li>
</ul>
<p>Simulate episodes of experience from now with the model</p>
<p><span class="math display">\[
\Big\{ S_t^k, A_t^k, R_{t+1}^k, ..., S_T^k \Big\}_{k=1}^K \sim \mathcal{M}_v
\]</span></p>
<p>Apply model-free RL to simulated episodes</p>
<ul>
<li>Monte-Carlo control → Monte-Carlo search</li>
<li>Sarsa → TD search</li>
</ul>
<p><img src="../images/silver_mc_search.png" class="img-fluid"></p>
<p><img src="../images/silver_td_search.png" class="img-fluid"></p>
</section>
<section id="lecture-9-exploration-and-exploitation" class="level2">
<h2 class="anchored" data-anchor-id="lecture-9-exploration-and-exploitation">7/1/21 - Lecture 9: Exploration and exploitation</h2>
<p>David starts with a multi-armed bandit case. We can think about it as a one-step MDP.</p>
<p>But in that case we don’t have states anymore.</p>
<p>Definition of regret as the total opportunity loss (how far we are from the best value). And maximizing cumulative reward is the same as minimizing total regret.</p>
<p>Greedy and <span class="math inline">\(\epsilon\)</span>-greedy have linear total regret.</p>
<p><strong>Optimism in face of uncertainty</strong>: don’t play the one with best mean value but play the one with best potential (characterized with the highest tail) = select action maximising Upper Confidence Bound (UCB)</p>
<p><span class="math inline">\(\epsilon\)</span>-greedy is behaving right when properly tuned or can be a disaster otherwise. UCB is comparable to properly tuned <span class="math inline">\(\epsilon\)</span>-greedy.</p>
<p>thesis from a French guy about thompson sampling in optimisation control problems: <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiUiqCfh8TxAhUG14UKHXK1C_AQFjACegQIBxAD&amp;url=https%3A%2F%2Ftel.archives-ouvertes.fr%2Ftel-01816069%2Fdocument&amp;usg=AOvVaw3hFUV24uDy-3nKFtTUW7Ea">Exploration-Exploitation with Thompson Sampling in Linear</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/castorfou\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>