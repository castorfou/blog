<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.159">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-06-07">
<meta name="description" content="From University of Alberta. My notes on course 3.">

<title>Guillaume’s blog - Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon_small.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-12YC1FPHWN"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-12YC1FPHWN', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Guillaume’s blog - Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation">
<meta name="twitter:description" content="From University of Alberta. My notes on course 3.">
<meta name="twitter:image" content="https://castorfou.github.io/posts/images/RL.png">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Guillaume’s blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/castorfou" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/GuillaumeRamel1" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-color-scheme-toggle nav-link" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation</h1>
                  <div>
        <div class="description">
          From University of Alberta. My notes on course 3.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">reinforcement learning</div>
                <div class="quarto-category">deepmind</div>
                <div class="quarto-category">coursera</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 7, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#course-3---week-1---on-policy-prediction-with-approximation" id="toc-course-3---week-1---on-policy-prediction-with-approximation" class="nav-link active" data-scroll-target="#course-3---week-1---on-policy-prediction-with-approximation">Course 3 - Week 1 - On-policy Prediction with Approximation</a></li>
  <li><a href="#course-3---week-2---constructing-features-for-prediction" id="toc-course-3---week-2---constructing-features-for-prediction" class="nav-link" data-scroll-target="#course-3---week-2---constructing-features-for-prediction">Course 3 - Week 2 - Constructing Features for Prediction</a></li>
  <li><a href="#course-3---week-3---control-with-approximation" id="toc-course-3---week-3---control-with-approximation" class="nav-link" data-scroll-target="#course-3---week-3---control-with-approximation">Course 3 - Week 3 - Control with Approximation</a></li>
  <li><a href="#course-3---week-4---policy-gradient" id="toc-course-3---week-4---policy-gradient" class="nav-link" data-scroll-target="#course-3---week-4---policy-gradient">Course 3 - Week 4 - Policy Gradient</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Coursera website: <a href="https://www.coursera.org/learn/prediction-control-function-approximation/home/welcome">course 3 - Prediction and Control with Function Approximation</a> of <a href="https://www.coursera.org/specializations/reinforcement-learning">Reinforcement Learning Specialization</a></p>
<p>my notes on <a href="../guillaume_blog/blog/reinforcement-learning-specialization-coursera.html">course 1 - Fundamentals of Reinforcement Learning</a>, <a href="../guillaume_blog/blog/reinforcement-learning-specialization-coursera-course2.html">course 2 - Sample-based Learning Methods</a>, <a href="../guillaume_blog/blog/reinforcement-learning-specialization-coursera-course4.html">course 4 - A Complete Reinforcement Learning System (Capstone)</a></p>
<p><strong>specialization roadmap</strong> - course 3 - <strong>Prediction and Control with Function Approximation</strong> <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/blob/main/course%203%20-%20function%20approximation/Course-3_Prediction-and-Control-with-Function-Approximation-Learning-Objectives.pdf">(syllabus)</a></p>
<p><strong>course 3</strong> - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with <strong>function approximation</strong>. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger <strong>infinite state spaces</strong>. We will cover <strong>feature construction</strong>, <strong>neural network learning</strong>, <strong>policy gradient methods</strong>, and other particularities of the function approximation setting.</p>
<p>Week 1 - On-policy Prediction with Approximation</p>
<p>Week 2 - Constructing Features for Prediction</p>
<p>Week 3 - Control with Approximation</p>
<p>Week 4 - Policy Gradient</p>
<section id="course-introduction" class="level6">
<h6 class="anchored" data-anchor-id="course-introduction">Course introduction</h6>
<p><strong>Video by Adam and Martha</strong></p>
<p><img src="../images/RL_algo_in_a_tree.png" class="img-fluid"></p>
<p>In course 2:</p>
<p><img src="../images/RL_with_a_table.png" class="img-fluid"></p>
<p>In course 3:</p>
<p><img src="../images/RL_without_a_table.png" class="img-fluid"></p>
</section>
<section id="course-3---week-1---on-policy-prediction-with-approximation" class="level2">
<h2 class="anchored" data-anchor-id="course-3---week-1---on-policy-prediction-with-approximation">Course 3 - Week 1 - On-policy Prediction with Approximation</h2>
<section id="module-1-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-1-learning-objectives">Module 1 Learning Objectives</h6>
<p><strong>Lesson 1: Estimating Value Functions as Supervised Learning</strong></p>
<ul>
<li>Understand how we can use parameterized functions to approximate value functions</li>
<li>Explain the meaning of linear value function approximation</li>
<li>Recognize that the tabular case is a special case of linear value function approximation.</li>
<li>Understand that there are many ways to parameterize an approximate value function</li>
<li>Understand what is meant by generalization and discrimination</li>
<li>Understand how generalization can be beneficial</li>
<li>Explain why we want both generalization and discrimination from our function approximation</li>
<li>Understand how value estimation can be framed as a supervised learning problem</li>
<li>Recognize not all function approximation methods are well suited for reinforcement learning</li>
</ul>
<p><strong>Lesson 2: The Objective for On-policy Prediction</strong></p>
<ul>
<li>Understand the mean-squared value error objective for policy evaluation</li>
<li>Explain the role of the state distribution in the objective</li>
<li>Understand the idea behind gradient descent and stochastic gradient descent</li>
<li>Outline the gradient Monte Carlo algorithm for value estimation</li>
<li>Understand how state aggregation can be used to approximate the value function</li>
<li>Apply Gradient Monte-Carlo with state aggregation</li>
</ul>
<p><strong>Lesson 3: The Objective for TD</strong></p>
<ul>
<li>Understand the TD-update for function approximation</li>
<li>Highlight the advantages of TD compared to Monte-Carlo</li>
<li>Outline the Semi-gradient TD(0) algorithm for value estimation</li>
<li>Understand that TD converges to a biased value estimate</li>
<li>Understand that TD converges much faster than Gradient Monte Carlo</li>
</ul>
<p><strong>Lesson 4: Linear TD</strong></p>
<ul>
<li>Derive the TD-update with linear function approximation</li>
<li>Understand that tabular TD(0) is a special case of linear semi-gradient TD(0)</li>
<li>Highlight the advantages of linear value function approximation over nonlinear</li>
<li>Understand the fixed point of linear TD learning</li>
<li>Describe a theoretical guarantee on the mean squared value error at the TD fixed point</li>
</ul>
</section>
<section id="lesson-1-estimating-value-functions-as-supervised-learning" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-estimating-value-functions-as-supervised-learning">Lesson 1: Estimating Value Functions as Supervised Learning</h6>
<p><strong>Reading</strong> Chapter 9.1-9.4 <strong>(pp.&nbsp;197-209)</strong> in the Reinforcement Learning textbook</p>
<blockquote class="blockquote">
<p>In many of the tasks to which we would like to apply reinforcement learning the state space is combinatorial and enormous; the number of possible camera images, for example, is much larger than the number of atoms in the universe.</p>
<p>In many of our target tasks, almost every state encountered will never have been seen before. To make sensible decisions in such states it is necessary to generalize from previous encounters with different states that are in some sense similar to the current one. In other words, the key issue is that of <strong>generalization</strong>. How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset?</p>
<p>Fortunately, generalization from examples has already been extensively studied, and we do not need to invent totally new methods for use in reinforcement learning. To some extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called function approximation because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function. Function approximation is an instance of supervised learning, the primary topic studied in machine learning, artificial neural networks, pattern recognition, and statistical curve fitting.</p>
</blockquote>
<p><strong>Video Moving to Parameterized Functions</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>understand</em> how we can use <strong>parameterized functions</strong> to approximate values, <em>explain</em> <strong>linear value function approximation</strong>, <em>recognize</em> that the tabular case is a special case of linear value function approximation, and <em>understand</em> that there are many ways to parameterize an approximate value function.</p>
<p><strong>Video Generalization and Discrimination</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>understand</em> what is meant by <strong>generalization</strong> and <strong>discrimination</strong>, <em>understand</em> how generalization can be beneficial, and <em>explain</em> why we want both generalization and discrimination from our function approximation.</p>
<p><strong>Video Framing Value Estimation as Supervised Learning</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>understand</em> how value estimation can be framed as a <strong>supervised learning</strong> problem, and <em>recognize</em> that not all function approximation methods are well suited for reinforcement learning.</p>
</section>
<section id="lesson-2-the-objective-for-on-policy-prediction" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-the-objective-for-on-policy-prediction"><strong>Lesson 2: The Objective for On-policy Prediction</strong></h6>
<p><strong>Video The Value Error Objective</strong> by Adam</p>
<p>By the end of this video you will be able to <em>understand</em> the <strong>Mean Squared Value Error objective</strong> for policy evaluation and <em>explain</em> the role of the <strong>state distribution</strong> in the objective.</p>
<p><span class="math display">\[
\overline{VE}=\displaystyle\sum_{s}\mu(s)[v_\pi(s)-\hat{v}(s,w)]^2
\]</span> This is the <strong>Mean Squared Value Error Objective</strong> where <span class="math inline">\(\mu\)</span> reflects how much we care about each state (a probability distribution)</p>
<p><strong>Video Introducing Gradient Descent</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>understand</em> the idea of <strong>gradient descent</strong>, and <em>understand</em> that gradient descent converges to stationary points.</p>
<p><strong>Video Gradient Monte for Policy Evaluation</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>understand</em> how to use gradient descent and <strong>stochastic gradient descent</strong> to minimize value error and <em>outline</em> the <strong>Gradient Monte Carlo</strong> algorithm for value estimation.</p>
<p><img src="../images/C3W1_gradient_monte_carlo.png" class="img-fluid"></p>
<p><strong>Video State Aggregation with Monte Carlo</strong> by Adam</p>
<p>By the end of this video, you will be able to <em>understand</em> how <strong>state aggregation</strong> can be used to approximate the value function and <em>apply</em> gradient Monte Carlo with state aggregation.</p>
</section>
<section id="lesson-3-the-objective-for-td" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-the-objective-for-td"><strong>Lesson 3: The Objective for TD</strong></h6>
<p><strong>Video Semi-Gradient TD for Policy Evaluation</strong> by Adam</p>
<p>By the end of this video you will be able to <em>understand</em> the <strong>TD update</strong> for function approximation, and <em>outline</em> the <strong>semi-gradient TD(0)</strong> algorithm for value estimation.</p>
<p><img src="../images/C3W1_semi_gradient_TD0.png" class="img-fluid"></p>
<p><strong>Video Comparing TD and Monte Carlo with State Aggregation</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>understand</em> that TD converges to a bias value estimate and <em>understand</em> that TD can learn faster than Gradient Monte Carlo.</p>
<p><strong>Video Doina Precup: Building Knowledge for AI Agents with Reinforcement Learning</strong></p>
</section>
<section id="lesson-4-linear-td" class="level6">
<h6 class="anchored" data-anchor-id="lesson-4-linear-td"><strong>Lesson 4: Linear TD</strong></h6>
<p><strong>Video The Linear TD Update</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>derive</em> the TD update with linear function approximation, <em>understand</em> that tabular TD(0) as a special case of <strong>linear semi gradient TD(0)</strong>, and <em>understand</em> why we care about linear TD as a special case.</p>
<p><strong>Video The True Objective for TD</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>understand</em> the <strong>fixed point</strong> of linear TD and <em>describe</em> a theoretical guarantee on the mean squared value error at the TD fixed point.</p>
<p><strong>Video Week 1 Summary</strong> by Adam</p>
</section>
<section id="assignment" class="level6">
<h6 class="anchored" data-anchor-id="assignment">Assignment</h6>
<p>TD with State Aggregation</p>
<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%201">github</a></p>
</section>
</section>
<section id="course-3---week-2---constructing-features-for-prediction" class="level2">
<h2 class="anchored" data-anchor-id="course-3---week-2---constructing-features-for-prediction">Course 3 - Week 2 - Constructing Features for Prediction</h2>
<section id="module-2-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-2-learning-objectives">Module 2 Learning Objectives</h6>
<p><strong>Lesson 1: Feature Construction for Linear Methods</strong></p>
<ul>
<li>Describe the difference between coarse coding and tabular representations</li>
<li>Explain the trade-off when designing representations between discrimination and generalization</li>
<li>Understand how different coarse coding schemes affect the functions that can be represented</li>
<li>Explain how tile coding is a (computationally?) convenient case of coarse coding</li>
<li>Describe how designing the tilings affects the resultant representation</li>
<li>Understand that tile coding is a computationally efficient implementation of coarse coding</li>
</ul>
<p><strong>Lesson 2: Neural Networks</strong></p>
<ul>
<li>Define a neural network</li>
<li>Define activation functions</li>
<li>Define a feedforward architecture</li>
<li>Understand how neural networks are doing feature construction</li>
<li>Understand how neural networks are a non-linear function of state</li>
<li>Understand how deep networks are a composition of layers</li>
<li>Understand the tradeoff between learning capacity and challenges presented by deeper networks</li>
</ul>
<p><strong>Lesson 3: Training Neural Networks</strong></p>
<ul>
<li>Compute the gradient of a single hidden layer neural network</li>
<li>Understand how to compute the gradient for arbitrarily deep networks</li>
<li>Understand the importance of initialization for neural networks</li>
<li>Describe strategies for initializing neural networks</li>
<li>Describe optimization techniques for training neural networks</li>
</ul>
</section>
<section id="lesson-1-feature-construction-for-linear-methods" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-feature-construction-for-linear-methods"><strong>Lesson 1: Feature Construction for Linear Methods</strong></h6>
<p><strong>Reading</strong> Chapter 9.4-9.5.0 <strong>(pp.&nbsp;204-210)</strong>, 9.5.3-9.5.4 <strong>(pp.&nbsp;215-222)</strong> and 9.7 <strong>(pp.&nbsp;223-228)</strong> in the Reinforcement Learning textbook</p>
<p><strong>Video Coarse Coding</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>describe</em> <strong>coarse coding</strong> and <em>describe</em> how it relates to <strong>state aggregation</strong>.</p>
<p><img src="../images/C3W2_coarse_coding.png" class="img-fluid"></p>
<p><strong>Video Generalization Properties of Coarse Coding</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>describe</em> how <strong>coarse coding parameters</strong> affect <strong>generalization</strong> and <strong>discrimination</strong>, and <em>understand</em> how that affects <strong>learning speed and accuracy</strong>.</p>
<p><strong>Video Tile Coding</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>explain</em> how <strong>tile coding</strong> achieves both <strong>generalization</strong> and <strong>discrimination</strong>, and <em>understand</em> the benefits and limitations of tile coding.</p>
<p><strong>Video Using Tile Coding in TD</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>explain</em> how to use <strong>tile coding</strong> with <strong>TD learning</strong> and <em>identify</em> important properties of <strong>tile code representations</strong>.</p>
</section>
<section id="lesson-2-neural-networks" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-neural-networks"><strong>Lesson 2: Neural Networks</strong></h6>
<p><strong>Video What is a Neural Network?</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>define</em> a <strong>neural network</strong>, <em>define</em> an <strong>activation function</strong> and <em>understand</em> how a neural network is a <strong>parameterized function</strong>.</p>
<p><strong>Video Non-linear Approximation with Neural Networks</strong> by Martha</p>
<p>By the end of this video, you will <em>understand</em> how neural networks do <strong>feature construction</strong>, and <em>understand</em> how neural networks are a <strong>non-linear function</strong> of state.</p>
<p><strong>Video Deep Neural Networks</strong> by Adam</p>
<p>By the end of this video, you will <em>understand</em> how <strong>deep neural networks</strong> are composed of <strong>many layers</strong> and <em>understand</em> that <strong>depth</strong> can facilitate learning <strong>features</strong> through <strong>composition</strong> and <strong>abstraction</strong>.</p>
</section>
<section id="lesson-3-training-neural-networks" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-training-neural-networks"><strong>Lesson 3: Training Neural Networks</strong></h6>
<p><strong>Video Gradient Descent for Training Neural Networks</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>derive</em> the <strong>gradient</strong> of a neural network and <em>implement</em> <strong>gradient descent</strong> on a neural network.</p>
<p><strong>Video Optimization Strategies for NNs</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>understand</em> the importance of <strong>initialization</strong> for neural networks and <em>describe</em> <strong>optimization techniques</strong> for training neural networks.</p>
<p><strong>Video David Silver on Deep Learning + RL = AI?</strong></p>
<p><strong>Video Week 2 Review</strong> by Adam</p>
</section>
<section id="assignment-1" class="level6">
<h6 class="anchored" data-anchor-id="assignment-1">Assignment</h6>
<p>Semi-gradient TD with a Neural Network</p>
<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%202">github</a></p>
</section>
</section>
<section id="course-3---week-3---control-with-approximation" class="level2">
<h2 class="anchored" data-anchor-id="course-3---week-3---control-with-approximation">Course 3 - Week 3 - Control with Approximation</h2>
<section id="module-3-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-3-learning-objectives">Module 3 Learning Objectives</h6>
<p><strong>Lesson 1: Episodic Sarsa with Function Approximation</strong></p>
<ul>
<li>Explain the update for Episodic Sarsa with function approximation</li>
<li>Introduce the feature choices, including passing actions to features or stacking state features</li>
<li>Visualize value function and learning curves</li>
<li>Discuss how this extends to Q-learning easily, since it is a subset of Expected Sarsa</li>
</ul>
<p><strong>Lesson 2: Exploration under Function Approximation</strong></p>
<ul>
<li>Understanding optimistically initializing your value function as a form of exploration</li>
</ul>
<p><strong>Lesson 3: Average Reward</strong></p>
<ul>
<li>Describe the average reward setting</li>
<li>Explain when average reward optimal policies are different from discounted solutions</li>
<li>Understand how differential value functions are different from discounted value functions</li>
</ul>
</section>
<section id="lesson-1-episodic-sarsa-with-function-approximation" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-episodic-sarsa-with-function-approximation"><strong>Lesson 1: Episodic Sarsa with Function Approximation</strong></h6>
<p><strong>Reading</strong> Chapter 10 <strong>(pp.&nbsp;243-246)</strong> and 10.3 <strong>(pp.&nbsp;249-252)</strong> in the Reinforcement Learning textbook</p>
<p><strong>Video Episodic Sarsa with Function Approximation</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>understand</em> how to construct <strong>action-dependent features</strong> for approximate action values and <em>explain</em> how to use <strong>Sarsa</strong> in <strong>episodic tasks</strong> with <strong>function approximation</strong>.</p>
<p><img src="../images/C3W3_episodic_sarsa_appro.png" class="img-fluid"></p>
<p><strong>Video Episodic Sarsa in Mountain Car</strong> by Adam</p>
<p>By the end of this video, you will <em>gain experience</em> analyzing the performance of an <strong>approximate TD control</strong> method.</p>
<p><strong>Video Expected Sarsa with Function Approximation</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>explain</em> the update for <strong>expected Sarsa</strong> with <strong>function approximation</strong>, and <em>explain</em> the update for <strong>Q-learning</strong> with <strong>function approximation</strong>.</p>
<p><img src="../images/C3W3_expected_sarsa_q_learning.png" class="img-fluid"></p>
</section>
<section id="lesson-2-exploration-under-function-approximation" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-exploration-under-function-approximation"><strong>Lesson 2: Exploration under Function Approximation</strong></h6>
<p><strong>Video Exploration under Function Approximation</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>describe</em> how <strong>optimistic initial values</strong> and <strong><span class="math inline">\(\epsilon\)</span>-greedy</strong> can be used with <strong>function approximation</strong>.</p>
</section>
<section id="lesson-3-average-reward" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-average-reward"><strong>Lesson 3: Average Reward</strong></h6>
<p><strong>Video Average Reward: A New Way of Formulating Control Problems</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>describe</em> the <strong>average reward</strong> setting, <em>explain</em> when <strong>average reward</strong> optimal policies are different from policies obtained under discounting and <em>understand</em> <strong>differential value functions</strong>.</p>
<p><strong>Satinder Singh on Intrinsic Rewards</strong></p>
<p><strong>Video Week 3 Review</strong> by Martha</p>
</section>
<section id="assignment-2" class="level6">
<h6 class="anchored" data-anchor-id="assignment-2">Assignment</h6>
<p>Function Approximation and Control</p>
<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%203">github</a></p>
</section>
</section>
<section id="course-3---week-4---policy-gradient" class="level2">
<h2 class="anchored" data-anchor-id="course-3---week-4---policy-gradient">Course 3 - Week 4 - Policy Gradient</h2>
<section id="module-4-learning-objectives" class="level6">
<h6 class="anchored" data-anchor-id="module-4-learning-objectives">Module 4 Learning Objectives</h6>
<p><strong>Lesson 1: Learning Parameterized Policies</strong></p>
<ul>
<li>Understand how to define policies as parameterized functions</li>
<li>Define one class of parameterized policies based on the softmax function</li>
<li>Understand the advantages of using parameterized policies over action-value based methods</li>
</ul>
<p><strong>Lesson 2: Policy Gradient for Continuing Tasks</strong></p>
<ul>
<li>Describe the objective for policy gradient algorithms</li>
<li>Describe the results of the policy gradient theorem</li>
<li>Understand the importance of the policy gradient theorem</li>
</ul>
<p><strong>Lesson 3: Actor-Critic for Continuing Tasks</strong></p>
<ul>
<li>Derive a sample-based estimate for the gradient of the average reward objective</li>
<li>Describe the actor-critic algorithm for control with function approximation, for continuing tasks</li>
</ul>
<p><strong>Lesson 4: Policy Parameterizations</strong></p>
<ul>
<li>Derive the actor-critic update for a softmax policy with linear action preferences</li>
<li>Implement this algorithm</li>
<li>Design concrete function approximators for an average reward actor-critic algorithm</li>
<li>Analyze the performance of an average reward agent</li>
<li>Derive the actor-critic update for a gaussian policy</li>
<li>Apply average reward actor-critic with a gaussian policy to a particular task with continuous actions</li>
</ul>
</section>
<section id="lesson-1-learning-parameterized-policies" class="level6">
<h6 class="anchored" data-anchor-id="lesson-1-learning-parameterized-policies"><strong>Lesson 1: Learning Parameterized Policies</strong></h6>
<p><strong>Reading</strong> Chapter 13 <strong>(pp.&nbsp;321-336)</strong> in the Reinforcement Learning textbook</p>
<p><strong>Video Learning Policies Directly</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>understand</em> how to define policies as <strong>parameterized functions</strong> and <em>define</em> one class of parametrized policies based on the <strong>softmax</strong> function.</p>
<p><strong>Video Advantages of Policy Parameterization</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>understand</em> some of the advantages of using parameterized policies.</p>
</section>
<section id="lesson-2-policy-gradient-for-continuing-tasks" class="level6">
<h6 class="anchored" data-anchor-id="lesson-2-policy-gradient-for-continuing-tasks"><strong>Lesson 2: Policy Gradient for Continuing Tasks</strong></h6>
<p><strong>Video The Objective for Learning Policies</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>describe</em> the objective for <strong>policy gradient algorithms</strong>.</p>
<p><img src="../images/C3W4_objective_policy_gradient.png" class="img-fluid"></p>
<p><strong>Video The Policy Gradient Theorem</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>describe</em> the result of the <strong>policy gradient theorem</strong> and <em>understand</em> the importance of the policy gradient theorem.</p>
<p><img src="../images/C3W4_policy_gradient_theorem.png" class="img-fluid"></p>
</section>
<section id="lesson-3-actor-critic-for-continuing-tasks" class="level6">
<h6 class="anchored" data-anchor-id="lesson-3-actor-critic-for-continuing-tasks"><strong>Lesson 3: Actor-Critic for Continuing Tasks</strong></h6>
<p><strong>Video Estimating the Policy Gradient</strong> by Martha</p>
<p>By the end of this video, you will be able to <em>derive</em> a <strong>sample-based estimate</strong> for the gradient of the average reward objective.</p>
<p><img src="../images/C3W4_stochastic_gradient_ascent.png" class="img-fluid"></p>
<p><strong>Video Actor-Critic Algorithm</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>describe</em> the <strong>actor-critic algorithm</strong> for control with function approximation for continuing tasks.</p>
<p><img src="../images/C3W4_actor_critic_algorithm.png" class="img-fluid"></p>
</section>
<section id="lesson-4-policy-parameterizations" class="level6">
<h6 class="anchored" data-anchor-id="lesson-4-policy-parameterizations"><strong>Lesson 4: Policy Parameterizations</strong></h6>
<p><strong>Video Actor-Critic with Softmax Policies</strong> by Adam</p>
<p>By the end of this video you’ll be able to <em>derive</em> the actor critic update for a Softmax policy with linear action preferences and <em>implement</em> this algorithm.</p>
<p><strong>Video Demonstration with Actor-Critic</strong> by Adam</p>
<p>By the end of this video, you’ll be able to <em>design</em> a function approximator for an average reward actor-critic algorithm and <em>analyze</em> the performance of an average reward agent.</p>
<p><strong>Video Gaussian Policies for Continuous Actions</strong> by Martha</p>
<p>By the end of this video, you’ll be able to <em>derive</em> the actor-critic update for a <strong>Gaussian policy</strong> and <em>apply</em> average reward actor-critic with a Gaussian policy to task with continuous actions.</p>
<p><strong>Video Week 4 Summary</strong> by Martha</p>
<p><img src="../images/C3W4_actor_critic_actions.png" class="img-fluid"></p>
</section>
<section id="assignment-3" class="level6">
<h6 class="anchored" data-anchor-id="assignment-3">Assignment</h6>
<p>Average Reward Softmax Actor-Critic using Tile-coding</p>
<p>notebooks in <a href="https://github.com/castorfou/Reinforcement-Learning-specialization/tree/main/assignements/course%203%20week%204">github</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/^(?:http:|https:)\/\/castorfou\.github\.io\//);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
</div> <!-- /content -->



</body></html>