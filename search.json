[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is more a journal where I am adding entries about my (baby steps) learnings. It is likely to be centered around python, git, data-science, … I have been strongly inspired by Rachel Thomas explaining why I should blog. Specially when starting such a journey to turn a datascientist.\nMy intent would be to regularly add entries to this blog. Ideally at least once a week. Maybe only short ones, the point being to stick on this frequent activity. If it takes days to write posts I am pretty sure I won’t do it. Those entries are personnal thoughts and not those of my employer Michelin.\n\n\n\nI am Guillaume Ramelet. I am 46 (in 2022). Father of 3. And have been working for a French tire company for 15+ years. I am French and as you can read English is not my mother tongue (but using English is a good exercise, isn’t it)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "nbdev\n\n\nquarto\n\n\n\n\nwhy it works with jupyter labs but not completely with jupyter notebook\n\n\n\n\n\n\nSep 23, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnbdev\n\n\nquarto\n\n\nfastpages\n\n\n\n\nfollowing migration guide from Hamel Hussain\n\n\n\n\n\n\nSep 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnbdev\n\n\njupyter\n\n\nfastai\n\n\n\n\nby Jeremy Howard and Hamel Hussain\n\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nhave to rely on corporate certificate server\n\n\n\n\n\n\nSep 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndocker\n\n\nevalai\n\n\ngitlab\n\n\nwsl\n\n\n\n\nhost evalai fully internally (UI and code)\n\n\n\n\n\n\nJul 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\n\n\ncould be useful to install evalai\n\n\n\n\n\n\nJul 26, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\ndocker\n\n\n\n\nunder ubuntu 20.04\n\n\n\n\n\n\nJul 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\ndocker\n\n\n\n\nbased on my automatic wsl installation\n\n\n\n\n\n\nJul 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nbecause it is easier to search from .py files in text format. Using nbautoexports in jupyter lab\n\n\n\n\n\n\nJul 21, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\nhuggingface\n\n\n\n\npar Thomas Simonini\n\n\n\n\n\n\nJun 15, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\ngit-credential-manager\n\n\n\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastpages\n\n\n\n\nfollowing bug with pyYaml\n\n\n\n\n\n\nMay 2, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nwsl\n\n\nwsl_last\n\n\n\n\neven if not available in Windows Store\n\n\n\n\n\n\nApr 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nas pointed by Jeremy Howard\n\n\n\n\n\n\nApr 7, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwsl\n\n\ncuda\n\n\nconda\n\n\n\n\nbest of breed windows + linux\n\n\n\n\n\n\nJan 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlinux\n\n\n\n\nlaunch Gnome files, firefox\n\n\n\n\n\n\nOct 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\nfastai\n\n\n\n\nusing nbdev/notebook2script from fastai, Jeremy Howard\n\n\n\n\n\n\nSep 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\nconda\n\n\n\n\nthe procedure entry point … could not be located in the dynamic library /pythoncom37.dll\n\n\n\n\n\n\nSep 28, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nwhen copy/paste in jupyter is breaking your notebooks\n\n\n\n\n\n\nSep 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\ngitlab doesn’t like >100MB files - removing crazy big files\n\n\n\n\n\n\nJul 29, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\ngitlab doesn’t like >100MB files\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npytorch\n\n\ncoursera\n\n\n\n\nFrom IBM\n\n\n\n\n\n\nJul 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\npdoc3 - installation, generation, browse documentation\n\n\n\n\n\n\nJun 30, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\ninstallation, run, structure, split cells, hide code, host in github\n\n\n\n\n\n\nJun 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\ndeepmind\n\n\ncoursera\n\n\n\n\nFrom University of Alberta. My notes on course 4.\n\n\n\n\n\n\nJun 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\ndeepmind\n\n\ncoursera\n\n\n\n\nFrom University of Alberta. My notes on course 3.\n\n\n\n\n\n\nJun 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\ndeepmind\n\n\ncoursera\n\n\n\n\nFrom University of Alberta. My notes on course 2.\n\n\n\n\n\n\nMay 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nmachine learning\n\n\nscikit-learn\n\n\n\n\nby Inria team on fun mooc platform\n\n\n\n\n\n\nMay 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\ndeepmind\n\n\ncoursera\n\n\n\n\nFrom University of Alberta. My notes on course 1.\n\n\n\n\n\n\nMay 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nMay 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwsl\n\n\ncuda\n\n\nconda\n\n\n\n\nbest of breed windows + linux\n\n\n\n\n\n\nApr 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nto connect to github behind local firewall\n\n\n\n\n\n\nApr 6, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\n\n\nMy notes\n\n\n\n\n\n\nApr 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nApr 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nsubmit patch, example with clustergit project\n\n\n\n\n\n\nMar 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nraspberry pi\n\n\n\n\nsetup headless raspberry pi to bridge wifi (tethering from phone) to ethernet (for my home wifi-router)\n\n\n\n\n\n\nMar 25, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\npytorch\n\n\nsb3\n\n\n\n\ninstallation, 1st experimentations\n\n\n\n\n\n\nMar 24, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nclustergit, RabbitVCS - installation and usage\n\n\n\n\n\n\nMar 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\ndeepmind\n\n\n\n\nFrom deepmind. My notes\n\n\n\n\n\n\nMar 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngpg\n\n\n\n\nbetween my windows and linx boxes\n\n\n\n\n\n\nMar 3, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nlogbook\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\n\n\nA Free course in Deep Reinforcement Learning from beginner to expert. My notes\n\n\n\n\n\n\nFeb 19, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nconda\n\n\njupyter\n\n\n\n\nsome useful commands I daily use\n\n\n\n\n\n\nFeb 16, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndeep learning\n\n\nmath\n\n\n\n\nMy notes/thoughts about the lecture in French\n\n\n\n\n\n\nFeb 10, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndeep learning\n\n\nMIT\n\n\ntensorflow\n\n\n\n\nMy notes/thoughts about the lecture\n\n\n\n\n\n\nFeb 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nreinforcement learning\n\n\n\n\nMy notes about some readings\n\n\n\n\n\n\nJan 26, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAll men by nature desire to know…\n\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nvisualization\n\n\nseaborn\n\n\ncheatsheet\n\n\n\n\npractical examples from datacamp courses\n\n\n\n\n\n\nJan 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nubuntu\n\n\njava\n\n\n\n\nOracle JRE under Ubuntu focal using linuxuprising/java\n\n\n\n\n\n\nJan 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\nconda\n\n\n\n\nand why this is so messy\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nvisualization\n\n\nmatplotlib\n\n\ncheatsheet\n\n\n\n\nso much options\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npandas\n\n\ncheatsheet\n\n\n\n\nso much different ways to do something with pandas, …\n\n\n\n\n\n\nJan 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\nnbdev\n\n\njupyter\n\n\n\n\nwalkthrough nbdev tutorial (from fastai team)\n\n\n\n\n\n\nJan 12, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndatacamp\n\n\ndata science\n\n\n\n\nkeep lectures, notebooks, progress, … and git structure\n\n\n\n\n\n\nJan 7, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\n\n\nbecause it is easier to search from .py files in text format. Using file save hooks from Jupyter, using nbconvert\n\n\n\n\n\n\nJan 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nubuntu\n\n\ncron\n\n\n\n\nexecute recurrent scripts with anacron to deal with missed jobs (system off)\n\n\n\n\n\n\nDec 13, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nshare data-scientist-skills repo with gitlab and github\n\n\n\n\n\n\nDec 8, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nto answer to exceeding GitHub’s file size limit of 100.00 MB\n\n\n\n\n\n\nDec 2, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\nWSL\n\n\n\n\nusing corkscrew to tunnel ssh through http proxy\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\njupyter\n\n\nWSL\n\n\n\n\nusefull when starting jupyter notebook from WSL\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nedx\n\n\nmit\n\n\ndata science\n\n\npython\n\n\n\n\nabout the UNIT 1\n\n\n\n\n\n\nOct 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nfastpages\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 2 (Build Better GAN) on week 3:\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 2 (Build Better GAN) on week 1: features extraction, FID Frechet Inception Distance\n\n\n\n\n\n\nOct 9, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 2 (Build Better GAN) on week 2:\n\n\n\n\n\n\nOct 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconda\n\n\nbash\n\n\n\n\nhow to launch bash scripts with an effective conda environment: conda activate, …\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\n\n\nusefull to debug\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 1 (Build Basic GAN) on week 3\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 1 (Build Basic GAN) on week 4\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 1 (Build Basic GAN) on week 1\n\n\n\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai. What I have learnt from gan specialization course 1 (Build Basic GAN) on week 2\n\n\n\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nubuntu\n\n\n\n\nFingerprints, sudoers\n\n\n\n\n\n\nOct 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngan\n\n\npytorch\n\n\n\n\nfrom Sharon Zhou, deeplearning.ai\n\n\n\n\n\n\nOct 1, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nubuntu\n\n\n\n\nInstructions to upgrade to last LTS version. Not available yet though (09/28)\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatplotlib\n\n\n\n\nPlaying with subplots, //, %, and animation\n\n\n\n\n\n\nSep 26, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nfastai\n\n\njupyter\n\n\nfastbook\n\n\n\n\ndescription of my learning environment, following fastai reco\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastai\n\n\ncuda\n\n\nlinux\n\n\n\n\nlist of actions to get a fastai workstation ready\n\n\n\n\n\n\nSep 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWSL\n\n\nfastai\n\n\nWSL2\n\n\ncuda\n\n\n\n\nhow to NOT get an operational corporate workstation with windows 10, cuda, wsl2, fastai, pytorch…\n\n\n\n\n\n\nSep 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWSL\n\n\nlinux\n\n\nbash\n\n\n\n\nDepending on my location (home, office, hotspot), I want my network configuration to adapt and set the proper settings: proxy for http, conda, git, etc\n\n\n\n\n\n\nSep 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npandas\n\n\n\n\nmy way to fast read big Excel files with Pandas using cvs cache\n\n\n\n\n\n\nSep 14, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngit\n\n\n\n\nhow to move from https to git to push to github, using ssh keys to authenticate\n\n\n\n\n\n\nSep 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfastpages\n\n\njupyter\n\n\nnotebooks\n\n\n\n\nhow to setup fastpages to blog from jupyter notebook\n\n\n\n\n\n\nSep 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 10, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nblog\n\n\nfastpages\n\n\ngit\n\n\n\n\nhttps://github.com/features/actions\n\n\n\n\n\n\nSep 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nme\n\n\n\n\nOverall resume and why I have turned as a datascientist\n\n\n\n\n\n\nSep 8, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-09-08-becoming-datascientist.html",
    "href": "posts/2020-09-08-becoming-datascientist.html",
    "title": "Becoming a datascientist",
    "section": "",
    "text": "This journey has started about 1 year ago.\nNo wait, that was dormant for a long time before that. I guess I have to go back to my studying time: at that time my days were full of maths and computers. And my days were flying as crazy. It happens to me (to you?) when you’re just in a middle of something you like very much. 10 hours looks like 1. And the opposite is true as well.\n\n\nMost of my days and weekends at that time were dedicated to code in Java and bash. Java mainly for server-side developpement in J2EE at Unilog Management. Bash from time to time to automate some tasks on my personal PC. At that time it was mainly about learning what is an operating system. I had started with LFS (Linux From Scratch). And in 2002 with Gentoo which was a much more powerful way to mimic LFS.\n\n\n\nStrange period. I don’t remember exactly why but I had a shift in my professional orientation. I moved away from software development and turned into a project manager. In 2005 I entered into Michelin company. And sofwtare technical matters at that time were considered as unimportant (and embarrassing subjects) Fortunately in 2009 I have been started my agile journey. A lot to learn, and it was less about software than human relations and empathy. It was like a start from scratch.\n\n\n\nQuite a new world for me. I had some basic knowledge by following Jono Bacon. At that time he was a community manager|release leader at Ubuntu. And was reporting progress using burndown charts. In 2009 I launched a project to create an employee portal (closed to what netvibes and igoogle were at that time). Using standard java portal technologies and more importantly using agile approach. A lot to learn about Agile, Scrum, and endless discussions about how to introduce Agile into a non-Agile organization. In 2010 I started another more ambitious project, with many colleagues (~30 persons) and a vague vision. It was about to create a product lifecycle management solution for semi-finished products.\n\n\n\nIn 2015, I met lean approaches for office. I was immediately convinced there was powerful and deep roots within lean. And it could bring a lot to people and organizations. I turned into a lean coach, to work with teams identifying what they could improve, how they could work better, with more pleasure.\n\n\n\nNice opportunity at that time to move from Clermont-Ferrand (France) to Greenville, South Carolina (USA). I have loved every part of it. Except maybe that 2 years were too short to make a full tour of this amazing country. It is crazy to think how different we are when we look like the same.\n\n\n\nSept 2019 - back to France and for 4 months to prepare for a complete new position: datascientist for Manufacturing within Michelin. I spent many days to learn from various sources specially datacamp and Andrew Ng. That was just the beginning. My intent was to move away from project management, team leadership and focus about what I can do by myself. I wanted to return to math domains without giving up an IT landscape. My colleague Francois Deheeger told me about data science and Artificial Intelligence. That looked as interesting as terrifying. I was in. I was not afraid to learn a new language, and to restart my career from scratch."
  },
  {
    "objectID": "posts/2020-09-09-blogging-from-github.html",
    "href": "posts/2020-09-09-blogging-from-github.html",
    "title": "Blogging from github",
    "section": "",
    "text": "I am a big fan of fastai’s spirit and even more of their leaders: - Jeremy Howards - Rachel Thomas - Sylvain Gugger who is know at huggingface.\nThey are commited to beautiful ideas, and are inspiring people. I like their courses. I like their softwares. For sure I will discuss about fastai. They have created fastpages. It turns github into a blogging platform. I don’t have the full detail but it is explained in fastpages github repo It is based on github actions, and by just creating a repo from a fastpages template https://github.com/fastai/fast_template/generate and giving a couple of settings, you are ready to go.\nAnd here I have to thank Hamel Husain. He is from github company and I think he is behing github actions and helped fastai to release fastpages. I don’t know Hamel but he looks like a humble, terribly skilled guy, with tons of energy. Thanks Hamel.\n\n\n\nMy main audience is the future me. (maybe not entirely true otherwise I would have written in French) In 1 year, I want to turn back to this blog and I would like to see all the learning peaces I went through. I want this platform to be as easy as possible.\n\n\nFor the moment it cannot be easier. I have setup the about page. And each blog entry is just a new markdown page into _posts. github _posts:\n\n\n\n\n\ngithub _posts\n\n\nBy commiting this page, there are internal actions being run automatically (through github actions magic) and after a couple of minutes the new blog pages are generated (using Jekyl and ruby if I am not wrong). For the moment I use github web interface. But I guess it is easier to have a local repo of my blog, create new entries and when satisfied git push to github. (to be tested later)\n\n\n\nFor a reason I used my personal github account (guillaume.ramelet@gmail.com) and not my professional one (guillaume.ramelet@michelin.com). I will see later if I have to move to another account. I had some troubles to setup actions into github. For a reason I thought it was available only for organization account. So I have turned my michelin github account to an organization, and I cannot login anymore. To be fixed later.\n\n\n\nOk I am not a huge fan of markdown. I use it as a basic text system specially within notebooks. But it is not as easy to insert images. Currently I screenshot what I want to share, insert into images folder of my repo and reference this image from my blog post using markdown language. I definitely have to improve my practice of markdown, and there are multiple cheatsheets to be used.\n\n\n\nThere are options within fastpages to blog from jupyter notebooks. I have to do it. My intent will be to use this place to share my knowledge. Today most of my knowledge comes from experiences I make within jupyter. If I could directly blog from that it will be great.\n\n\n\nOK as the sole reader this is maybe a minor concern but there is no commenting system associated with fastpages. I cannot get any feedback from these entries. Would love to get advices, create discussions within that blog. Not for today."
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html",
    "title": "Blog from jupyter notebook",
    "section": "",
    "text": "That will be great if I can simply write blog entries using Jupyter Notebook.\nI usually paste inner images into jupyter cells. But this feature is not available yet into fastpages. So for the moment I won’t include images into these posts.\nThat way I could simply use markdown and insert images\nAnd directly see rendered impact before commiting and pushing to my blog."
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html#get-local-repo-from-github",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html#get-local-repo-from-github",
    "title": "Blog from jupyter notebook",
    "section": "get local repo from github",
    "text": "get local repo from github\nAs I am behind a proxy most of my time when working from office, the easiest way for me is to work from WSL.\n\nWSL\nI won’t detail how to install WSL on Windows.\nI use ubuntu images (18.04) on my PC.\n\n\nset unset proxy in WSL\nI have just added some bash commands at the end of my .bashrc file.\n# Set Proxy\nfunction setproxy() {\n    export {http,https,ftp}_proxy=\"http://<my proxy ip address>:80\"\n    export {HTTP,HTTPS,FTP}_PROXY=\"http://<my proxy ip address>:80\"\n}\n\n# Unset Proxy\nfunction unsetproxy() {\n    unset {http,https,ftp}_proxy\n    unset {HTTP,HTTPS,FTP}_PROXY\n}\n\n\ngit clone castorfou.github.io\nI keep most of my local repos under ~/git/\ncd ~/git\nsetproxy\ngit clone https://github.com/castorfou/castorfou.github.io.git"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html#create-a-blog-entry-with-jupyter-notebook",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html#create-a-blog-entry-with-jupyter-notebook",
    "title": "Blog from jupyter notebook",
    "section": "create a blog entry with Jupyter Notebook",
    "text": "create a blog entry with Jupyter Notebook"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html#commit-and-push-to-github",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html#commit-and-push-to-github",
    "title": "Blog from jupyter notebook",
    "section": "commit and push to github",
    "text": "commit and push to github\n(base) guillaume@LL11LPC0PQARQ:~$ cd git\n(base) guillaume@LL11LPC0PQARQ:~/git$ cd castorfou.github.io/\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git add .\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git commit -m 'new blog entry: blog from jupyter'\n[master 6b7460a] new blog entry: blog from jupyter\n 4 files changed, 516 insertions(+)\n create mode 100644 _posts/.ipynb_checkpoints/2020-09-10-blog-from-jupyter-checkpoint.ipynb\n create mode 100644 _posts/2020-09-10-blog-from-jupyter.ipynb\n create mode 100644 _posts/2020-09-10-blog-from-jupyter.py\n create mode 100644 _posts/Untitled.txt\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push\nfatal: unable to access 'https://github.com/castorfou/castorfou.github.io.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\n\nerror: gnutls_handshake() failed: The TLS connection was non-properly terminated.\nJust googling this error gives some insight: https://github.community/t/unable-to-push-to-repo-gnutls-handshake-failed/885\nIt is likely some local firewell issue.\n{% include alert.html text=“To be fixed later” %}\n\n\nswitch to mobile wifi without need of proxy\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ unsetproxy\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push\nUsername for 'https://github.com': castorfou\nPassword for 'https://castorfou@github.com':\nCounting objects: 7, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (7/7), 141.26 KiB | 10.09 MiB/s, done.\nTotal 7 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: GitHub found 3 vulnerabilities on castorfou/castorfou.github.io's default branch (2 high, 1 moderate). To find out more, visit:\nremote:      https://github.com/castorfou/castorfou.github.io/network/alerts\nremote:\nTo https://github.com/castorfou/castorfou.github.io.git\n   6adeb02..6b7460a  master -> master"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html#check-entries-into-blog",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html#check-entries-into-blog",
    "title": "Blog from jupyter notebook",
    "section": "check entries into blog",
    "text": "check entries into blog\n\ndouble entries\nDouble entries: one for the notebook (.ipynb) and one for the auto python export (.py). I will have to update my jupyter settings to avoid this python file creation. In the meantime I can just delete the python file, and commit.\n{% include alert.html text=“Change settings of jupyter + .gitignore to avoid these double entries” %}\n\n\ncannot open notebook into browser\nClicking just ask me to download the notebook, it doesn’t display it into the browser.\n\n\nchecking .gitignore\nJust by looking into .gitignore, there is an interesting entry:\n*.swp\n~*\n*~\n_site\n.sass-cache\n.jekyll-cache\n.jekyll-metadata\nvendor\n_notebooks/.ipynb_checkpoints\nWait what is in this last line.\nLet’s create _notebooks directory and move my notebook in that directory.\n\n\nnotebooks from _notebooks not rendered\nNo entries, I guess there is some additional settings to do…\n{% include alert.html text=“Why notebooks are not rendered by Jekyl” %}\n\n\ntest entry from md using local repo\nThere is no problem with that.\nCreating a local md file in _poststhen pushing to github is creating the right entry blog.\n\n\nfollowing fastpages troubleshooting guide\n\nupgrade fastpages\nTry the automated upgrade as described in https://github.com/fastai/fastpages/blob/master/_fastpages_docs/UPGRADE.md\nUnfortunately I don’t see\nI have to follow the manual upgrade.\n\n\nmanual fastpages upgrade\nI am surprised because the 1st step from manual upgrade is to copy the fastpages repo. It is what I did 2 days ago. I doubt having an outdated version of fastpages.\n\n\n\nfastai forum: fastpages category\nI will browse through nbdev & faspages category in fastai forums. I should see people with the same issue.\nI have created an entry, into fastai forums: Fastpages - cannot see build process of GitHub Actions\nAnd quite immediately Hamel Hussain answered guiding to the write direction:\nI misread the Settings instruction: my github repo should explicitely NOT include my github username and I did exactly the opposite.\n{% include alert.html text=“I have to create a new repo: guillaume_blog” %}\n\n\nnothing visible from Actions tab\nAnd another surprising subject: at github in Actions tab. I have a kind of default page. I expect something like an execution journal of Actions.\n\n\nPage build failure\nReceived a notification by email:\nThe page build failed for the master branch with the following error:\nPage build failed. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#troubleshooting-build-errors.\nFor information on troubleshooting Jekyll see:\nhttps://docs.github.com/articles/troubleshooting-jekyll-builds\nIf you have any questions you can submit a request on the Contact GitHub page at https://support.github.com/contact?repo_id=293820308&page_build_id=202240535"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html#move-to-another-repo",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html#move-to-another-repo",
    "title": "Blog from jupyter notebook",
    "section": "Move to another repo",
    "text": "Move to another repo\n\nrepo creation\nIt was just a matter of creating a new repo:\n\n\nactions monitoring\nMonitoring is effective\n\n\nmerge pull request\n\nactions around ssh keys\nFollowing the steps: - Create keys using ssh utility - Enter Secret Key - Enter Deploy Key\n\n\nmerge PR\nThere are conflicts to be fixed before that.\n\n\n\nAnd it works: https://castorfou.github.io/guillaume_blog/"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter-sans-images.html#get-local-repo",
    "href": "posts/2020-09-10-blog-from-jupyter-sans-images.html#get-local-repo",
    "title": "Blog from jupyter notebook",
    "section": "Get local repo",
    "text": "Get local repo\ncd ~/git\nunsetproxy\ngit clone https://github.com/castorfou/guillaume_blog.git"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html",
    "href": "posts/2020-09-10-blog-from-jupyter.html",
    "title": "Blog from jupyter notebook",
    "section": "",
    "text": "That will be great if I can simply write blog entries using Jupyter Notebook.\nThat way I could simply use markdown and insert images\nAnd directly see rendered impact before commiting and pushing to my blog."
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html#get-local-repo-from-github",
    "href": "posts/2020-09-10-blog-from-jupyter.html#get-local-repo-from-github",
    "title": "Blog from jupyter notebook",
    "section": "get local repo from github",
    "text": "get local repo from github\nAs I am behind a proxy most of my time when working from office, the easiest way for me is to work from WSL.\n\nWSL\nI won’t detail how to install WSL on Windows.\nI use ubuntu images (18.04) on my PC.\n\n\n\nimage.png\n\n\n\n\nset unset proxy in WSL\nI have just added some bash commands at the end of my .bashrc file.\n# Set Proxy\nfunction setproxy() {\n    export {http,https,ftp}_proxy=\"http://<my proxy ip address>:80\"\n    export {HTTP,HTTPS,FTP}_PROXY=\"http://<my proxy ip address>:80\"\n}\n\n# Unset Proxy\nfunction unsetproxy() {\n    unset {http,https,ftp}_proxy\n    unset {HTTP,HTTPS,FTP}_PROXY\n}\n\n\ngit clone castorfou.github.io\nI keep most of my local repos under ~/git/\ncd ~/git\nsetproxy\ngit clone https://github.com/castorfou/castorfou.github.io.git\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html#create-a-blog-entry-with-jupyter-notebook",
    "href": "posts/2020-09-10-blog-from-jupyter.html#create-a-blog-entry-with-jupyter-notebook",
    "title": "Blog from jupyter notebook",
    "section": "create a blog entry with Jupyter Notebook",
    "text": "create a blog entry with Jupyter Notebook"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html#commit-and-push-to-github",
    "href": "posts/2020-09-10-blog-from-jupyter.html#commit-and-push-to-github",
    "title": "Blog from jupyter notebook",
    "section": "commit and push to github",
    "text": "commit and push to github\n(base) guillaume@LL11LPC0PQARQ:~$ cd git\n(base) guillaume@LL11LPC0PQARQ:~/git$ cd castorfou.github.io/\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git add .\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git commit -m 'new blog entry: blog from jupyter'\n[master 6b7460a] new blog entry: blog from jupyter\n 4 files changed, 516 insertions(+)\n create mode 100644 _posts/.ipynb_checkpoints/2020-09-10-blog-from-jupyter-checkpoint.ipynb\n create mode 100644 _posts/2020-09-10-blog-from-jupyter.ipynb\n create mode 100644 _posts/2020-09-10-blog-from-jupyter.py\n create mode 100644 _posts/Untitled.txt\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push\nfatal: unable to access 'https://github.com/castorfou/castorfou.github.io.git/': gnutls_handshake() failed: The TLS connection was non-properly terminated.\n\nerror: gnutls_handshake() failed: The TLS connection was non-properly terminated.\nJust googling this error gives some insight: https://github.community/t/unable-to-push-to-repo-gnutls-handshake-failed/885\nIt is likely some local firewell issue.\n{% include alert.html text=“To be fixed later” %}\n\n\nswitch to mobile wifi without need of proxy\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ unsetproxy\n(base) guillaume@LL11LPC0PQARQ:~/git/castorfou.github.io$ git push\nUsername for 'https://github.com': castorfou\nPassword for 'https://castorfou@github.com':\nCounting objects: 7, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (7/7), 141.26 KiB | 10.09 MiB/s, done.\nTotal 7 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote:\nremote: GitHub found 3 vulnerabilities on castorfou/castorfou.github.io's default branch (2 high, 1 moderate). To find out more, visit:\nremote:      https://github.com/castorfou/castorfou.github.io/network/alerts\nremote:\nTo https://github.com/castorfou/castorfou.github.io.git\n   6adeb02..6b7460a  master -> master"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html#check-entries-into-blog",
    "href": "posts/2020-09-10-blog-from-jupyter.html#check-entries-into-blog",
    "title": "Blog from jupyter notebook",
    "section": "check entries into blog",
    "text": "check entries into blog\n\ndouble entries\n\n\n\nimage.png\n\n\nDouble entries: one for the notebook (.ipynb) and one for the auto python export (.py). I will have to update my jupyter settings to avoid this python file creation. In the meantime I can just delete the python file, and commit.\n{% include alert.html text=“Change settings of jupyter + .gitignore to avoid these double entries” %}\n\n\ncannot open notebook into browser\n\n\n\nimage.png\n\n\nClicking just ask me to download the notebook, it doesn’t display it into the browser.\n\n\nchecking .gitignore\nJust by looking into .gitignore, there is an interesting entry:\n*.swp\n~*\n*~\n_site\n.sass-cache\n.jekyll-cache\n.jekyll-metadata\nvendor\n_notebooks/.ipynb_checkpoints\nWait what is in this last line.\nLet’s create _notebooks directory and move my notebook in that directory.\n\n\nnotebooks from _notebooks not rendered\n No entries, I guess there is some additional settings to do…\n{% include alert.html text=“Why notebooks are not rendered by Jekyl” %}\n\n\ntest entry from md using local repo\nThere is no problem with that.\nCreating a local md file in _poststhen pushing to github is creating the right entry blog.\n\n\n\nimage.png\n\n\n\n\nfollowing fastpages troubleshooting guide\n\nupgrade fastpages\nTry the automated upgrade as described in https://github.com/fastai/fastpages/blob/master/_fastpages_docs/UPGRADE.md\nUnfortunately I don’t see \nI have to follow the manual upgrade.\n\n\nmanual fastpages upgrade\nI am surprised because the 1st step from manual upgrade is to copy the fastpages repo. It is what I did 2 days ago. I doubt having an outdated version of fastpages.\n\n\n\nfastai forum: fastpages category\nI will browse through nbdev & faspages category in fastai forums. I should see people with the same issue.\nI have created an entry, into fastai forums: Fastpages - cannot see build process of GitHub Actions\nAnd quite immediately Hamel Hussain answered guiding to the write direction:\nI misread the Settings instruction: my github repo should explicitely NOT include my github username and I did exactly the opposite.\n{% include alert.html text=“I have to create a new repo: guillaume_blog” %}\n\n\nnothing visible from Actions tab\nAnd another surprising subject: at github in Actions tab. I have a kind of default page. I expect something like an execution journal of Actions. \n\n\nPage build failure\nReceived a notification by email:\n\n\n\nimage.png\n\n\nThe page build failed for the master branch with the following error:\nPage build failed. For more information, see https://docs.github.com/github/working-with-github-pages/troubleshooting-jekyll-build-errors-for-github-pages-sites#troubleshooting-build-errors.\nFor information on troubleshooting Jekyll see:\nhttps://docs.github.com/articles/troubleshooting-jekyll-builds\nIf you have any questions you can submit a request on the Contact GitHub page at https://support.github.com/contact?repo_id=293820308&page_build_id=202240535"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html#move-to-another-repo",
    "href": "posts/2020-09-10-blog-from-jupyter.html#move-to-another-repo",
    "title": "Blog from jupyter notebook",
    "section": "Move to another repo",
    "text": "Move to another repo\n\nrepo creation\nIt was just a matter of creating a new repo: \n\n\nactions monitoring\nMonitoring is effective \n\n\nmerge pull request\n\n\n\nimage.png\n\n\n\nactions around ssh keys\nFollowing the steps: - Create keys using ssh utility - Enter Secret Key - Enter Deploy Key\n\n\nmerge PR\nThere are conflicts to be fixed before that.\n\n\n\nAnd it works: https://castorfou.github.io/guillaume_blog/\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2020-09-10-blog-from-jupyter.html#get-local-repo",
    "href": "posts/2020-09-10-blog-from-jupyter.html#get-local-repo",
    "title": "Blog from jupyter notebook",
    "section": "Get local repo",
    "text": "Get local repo\ncd ~/git\nunsetproxy\ngit clone https://github.com/castorfou/guillaume_blog.git"
  },
  {
    "objectID": "posts/2020-09-11-git-commit-without-password.html",
    "href": "posts/2020-09-11-git-commit-without-password.html",
    "title": "Git push to github without password",
    "section": "",
    "text": "By default everytime I push to github, I have a prompt asking for password.\nWould be great if I could leverage ssh keys to authenticate."
  },
  {
    "objectID": "posts/2020-09-11-git-commit-without-password.html#update-remote-from-https-to-ssh",
    "href": "posts/2020-09-11-git-commit-without-password.html#update-remote-from-https-to-ssh",
    "title": "Git push to github without password",
    "section": "Update remote from https to ssh",
    "text": "Update remote from https to ssh\nFrom https://stackoverflow.com/questions/14762034/push-to-github-without-a-password-using-ssh-key,\nFor example, a GitHub project like Git will have an HTTPS URL:\n\nhttps://github.com/<Username>/<Project>.git\nAnd the SSH one:\n\ngit@github.com:<Username>/<Project>.git\nYou can do:\n\ngit remote set-url origin git@github.com:<Username>/<Project>.git\nto change the URL.\nIn my case I have\n(xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git remote -v\norigin  https://github.com/castorfou/guillaume_blog.git (fetch)\norigin  https://github.com/castorfou/guillaume_blog.git (push)\nI have just to modify:\ngit remote set-url origin git@github.com:castorfou/guillaume_blog.git"
  },
  {
    "objectID": "posts/2020-09-11-git-commit-without-password.html#results",
    "href": "posts/2020-09-11-git-commit-without-password.html#results",
    "title": "Git push to github without password",
    "section": "Results",
    "text": "Results\nIt looks like working:\n(xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ git push\nCounting objects: 4, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (4/4), 1.59 KiB | 812.00 KiB/s, done.\nTotal 4 (delta 2), reused 0 (delta 0)\nremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\nTo github.com:castorfou/guillaume_blog.git\n   4013108..ae78b99  master -> master"
  },
  {
    "objectID": "posts/2020-09-11-git-commit-without-password.html#drawback-doesnt-work-behing-a-firewall",
    "href": "posts/2020-09-11-git-commit-without-password.html#drawback-doesnt-work-behing-a-firewall",
    "title": "Git push to github without password",
    "section": "Drawback: doesn’t work behing a firewall",
    "text": "Drawback: doesn’t work behing a firewall\n{% include alert.html text=“To find a solution to use a proxy” %}\nHere are 2 ways to be tested: https://stackoverflow.com/questions/1728934/accessing-a-git-repository-via-ssh-behind-a-firewall https://stackoverflow.com/questions/18604719/how-to-configure-git-to-clone-repo-from-github-behind-a-proxy-server?noredirect=1&lq=1"
  },
  {
    "objectID": "posts/2020-09-14-fast-read-excel-pandas.html",
    "href": "posts/2020-09-14-fast-read-excel-pandas.html",
    "title": "Fast read Excel files with pandas",
    "section": "",
    "text": "from functools import wraps\nfrom time import time\ndef measure_time(func):\n    @wraps(func)\n    def _time_it(*args, **kwargs):\n        start = int(round(time() * 1000))\n        try:\n            return func(*args, **kwargs)\n        finally:\n            end_ = int(round(time() * 1000)) - start\n            print(f\"Total execution time: {end_ if end_ > 0 else 0} ms\")\n    return _time_it\n\n\n\n\n\nbig_excel_file = root_data+'/pandas-caching/big_excel_file.xlsx'\n\n\n@measure_time\ndef load_excel(file):\n    dataframe = pd.read_excel(file)\n    return dataframe\n\n\ndataframe = load_excel(big_excel_file)\n\nTotal execution time: 36196 ms\n\n\n{% include alert.html text=“Waouh, 36 sec to read this file!” %}\n\n\n\n\ncsv_file = root_data+'/pandas-caching/big_csv_file_turned_from_excel.csv'\n\n\n@measure_time\ndef load_csv(file):\n    dataframe = pd.read_csv(file, sep=';', decimal=',')\n    return dataframe\n\n\ndf_csv = load_csv(csv_file)\n\nTotal execution time: 836 ms\n\n\n{% include alert.html text=“Much better, 0.8 sec!” %}"
  },
  {
    "objectID": "posts/2020-09-14-fast-read-excel-pandas.html#caching-library",
    "href": "posts/2020-09-14-fast-read-excel-pandas.html#caching-library",
    "title": "Fast read Excel files with pandas",
    "section": "Caching library",
    "text": "Caching library\n\nimport os\n\ndef read_CachedXLS(filename, forceReload = False, **options):\n    \"\"\"\n    Part d'un fichier excel natif (filename).\n    Si le dataframe caché correspondant n'existe pas encore, alors sauve le dataframe caché au format csv dans le rep source.\n    (s'il existe et si forceReload==True, alors écrase le dataframe caché existant par une nouvelle version)\n    Lit le dataframe caché correspondant avec les **options et retourne le dataframe.\n    \n    Examples\n    --------\n    >>> filename = '/mnt/z/data/Stam-CC/ExportData 25625.xlsx'\n        forceReload = False\n        option={'dayfirst':True, 'parse_dates':['Fecha de Medida', 'Fecha de Fabricacion'], 'sheetname':0}\n        getCachedXLSRaw(filename, forceRelead, **option).info()\n\n    Parameters\n    ----------\n    filename : string\n        Emplacement du fichier XLS. Avec l'extension. Format complet\n        Ex: '/mnt/z/data/Stam-CC/ExportData 25625.xlsx'\n    forceReload : boolean, optional, default value = False\n        Si forceReload == True, le fichier sera relu et sauvé même s'il existe déjà en cache\n    options : **keyword args, optional\n        Arguments de lecture du fichier XLS :  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n        Ex: sheetname=1\n\n    Returns\n    -------\n    dataframe\n        Dataframe correspondant \n    \"\"\"\n    \n    #split pour ne garder que le nom sans le chemin de filename : Stam-CC/ExportData 25625 --> ExportData 25625\n    dataframe_filename = os.path.dirname(filename)+'/'+os.path.basename(filename)+'.csv'\n    #bug de pandas.to_csv quand il y a des espaces ?\n    dataframe_filename = dataframe_filename.replace(\" \", \"_\")\n\n    dataframe=[]\n    xls_toget = False\n    \n    #print(dataframe_filename)\n    if (forceReload and os.path.exists(dataframe_filename)):\n        print(\"Cached file \"+dataframe_filename+\" déjà existant mais forceReload=True - FORCE RELOAD\")\n        xls_toget = True\n        \n    if (not os.path.exists(dataframe_filename)):\n        print(\"Cached file  \"+dataframe_filename+\" inexistant - read_CachedXLS\")\n        xls_toget = True\n        \n    if (xls_toget):\n        dataframe = pd.read_excel(filename, **options)\n        dataframe.to_csv(dataframe_filename)\n    else:\n        print(\"Cached file \"+dataframe_filename+\" existe en cache, relecture\")\n    \n    #index_col pour ignorer les n° de lignes excel \n    options['sep']=','\n    options['decimal']='.'\n    options['skiprows']=0\n    options.pop('sheet_name')\n    dataframe = pd.read_csv(dataframe_filename,**options)\n    return dataframe\n\n\noption={'sheet_name':0}\nread_CachedXLS(big_excel_file, **option)\nprint(\"et voila\")\n\nCached file /mnt/z/data//pandas-caching/big_excel_file.xlsx.csv existe en cache, relecture\net voila"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "",
    "text": "Command to get IP address is as follow:\nIP=`ifconfig | grep 'inet '| grep -v '127.0.0.1' | cut -d: -f2 | awk '{ print $2}'`\nI can then check how IP is setup: - empty: no network attached, in that case nothing to do - HOME_IP=192.168.1.241: based on MAC I give fixed IP to my computers (out of DHCP scope) - S8_IP=192.168.: hotspot from samsung is using 192.168. addresses - OFFICE_IP=10.: office network uses 10. addresses\nDetect if variable IP is set:\nif [ -z \"$IP\" ]; then\n        echo \"Not connected to any network\"\nfi"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#network-detection-and-proxy-settings",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#network-detection-and-proxy-settings",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "Network detection and proxy settings",
    "text": "Network detection and proxy settings\nDepending on my network, I have to set or unset proxy.\nHere is the 1st version:\n(xgboost) guillaume@LL11LPC0PQARQ:~$ cat my_ip.sh\n#!/bin/bash\n\nIP=`ifconfig | grep 'inet '| grep -v '127.0.0.1' | cut -d: -f2 | awk '{ print $2}'`\nHOME_IP=192.168.1.241\nOFFICE_IP=10.\nS8_IP=192.168.\n\n# Set Proxy\nfunction setproxy() {\n     echo \"Calling setproxy\"\n     export {http,https,ftp}_proxy=\"http://proxy_ip:80\"\n     export {HTTP,HTTPS,FTP}_PROXY=\"http://proxy_ip:80\"\n}\n\n# Unset Proxy\nfunction unsetproxy() {\n     echo \"Calling unsetproxy\"\n     unset {http,https,ftp}_proxy\n     unset {HTTP,HTTPS,FTP}_PROXY\n}\n\nif [ -z \"$IP\" ]; then\n        echo \"Not connected to any network\"\nelse\n        echo \"Connected and IP address is: $IP\"\n\n        if [[ \"$IP\" == \"$HOME_IP\" ]]; then\n                echo \"Connected at home from freebox pop --> no proxy\"\n                unsetproxy\n        else\n                if [[ \"$IP\" == \"$S8_IP\"* ]]; then\n                     echo \"Connected with mobile phone --> no proxy\"\n                     unsetproxy\n                fi\n                if [[ \"$IP\" == \"$OFFICE_IP\"* ]]; then\n                        echo \"Connected from Office --> proxy\"\n                        setproxy\n                fi\n        fi\nfi"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#call-this-script-source",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#call-this-script-source",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "Call this script: source",
    "text": "Call this script: source\nIf I want these environment variables to be available from parent shell, I have to call my script with source.\n(xgboost) guillaume@LL11LPC0PQARQ:~$ source my_ip.sh\nConnected and IP address is: 10.xxx.xxx.xxx\n192.168.1.241\nConnected from Office --> proxy\nCalling setproxy\nAnd I will auto launch this script each time I open a terminal by adding source my_ip.sh at the end of .bashrc"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#git-and-keep-dot-configuration-files-config",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#git-and-keep-dot-configuration-files-config",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "git and keep dot configuration files: config",
    "text": "git and keep dot configuration files: config\nAnother great practice from Jeremy Howard: From https://developer.atlassian.com/blog/2016/02/best-way-to-store-dotfiles-git-bare-repo/ and https://www.atlassian.com/git/tutorials/dotfiles\nI will create a blog entry about that later.\nconfig add .bashrc my_ip.sh\nconfig commit -m 'detect network and set proxy'\nconfig push"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#wget-proxy-no-proxy",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#wget-proxy-no-proxy",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "wget: proxy / no proxy",
    "text": "wget: proxy / no proxy\nI store proxy conf files under ~/proxy_files/\nFor wget: 2 files\n$ cat proxy_files/.wgetrc_noproxy\nuse_proxy=no\n$ cat proxy_files/.wgetrc_proxy\nuse_proxy=yes\nhttp_proxy=proxy_ip:80\nhttps_proxy=proxy_ip:80\nAnd enabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc\nDisabling proxy for wget: ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc\nSo the updated functions setproxy and unsetproxy are:\n# Set Proxy\nfunction setproxy() {\n     echo \"Calling setproxy\"\n     export {http,https,ftp}_proxy=\"http://proxy_ip:80\"\n     export {HTTP,HTTPS,FTP}_PROXY=\"http://proxy_ip:80\"\n     #proxy for wget\n     ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc\n}\n\n# Unset Proxy\nfunction unsetproxy() {\n     echo \"Calling unsetproxy\"\n     unset {http,https,ftp}_proxy\n     unset {HTTP,HTTPS,FTP}_PROXY\n     #no proxy for wget\n     ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc\n}"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#apt-get-proxy-no-proxy",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#apt-get-proxy-no-proxy",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "apt-get: proxy / no proxy",
    "text": "apt-get: proxy / no proxy\nI store proxy conf files under ~/proxy_files/\nFor apt, 1 file\n$ cat proxy_files/apt_proxy.conf\nAcquire {\n  HTTP::proxy \"http://proxy_ip:80\";\n  HTTPS::proxy \"http://proxy_ip:80\";\n}\nAnd enabling proxy for apt: sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf\nDisabling proxy for wget: sudo rm -f /etc/apt/apt.conf.d/proxy.conf\n{% include alert.html text=“Refactor to avoid password request each time it is launched” %}\nfor the moment I have just commented out these lines\nSo the updated functions setproxy and unsetproxy are:\n# Set Proxy\nfunction setproxy() {\n     echo \"Calling setproxy\"\n     export {http,https,ftp}_proxy=\"http://proxy_ip:80\"\n     export {HTTP,HTTPS,FTP}_PROXY=\"http://proxy_ip:80\"\n     #proxy for wget\n     ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc\n     #proxy for apt\n     #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf\n}\n\n# Unset Proxy\nfunction unsetproxy() {\n     echo \"Calling unsetproxy\"\n     unset {http,https,ftp}_proxy\n     unset {HTTP,HTTPS,FTP}_PROXY\n     #no proxy for wget\n     ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc\n     #no proxy for apt\n     #sudo rm -f /etc/apt/apt.conf.d/proxy.conf\n}"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#sept-21-2020-ip-detection-to-be-changed-after-wsl2",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#sept-21-2020-ip-detection-to-be-changed-after-wsl2",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "Sept-21 2020: IP detection to be changed after WSL2",
    "text": "Sept-21 2020: IP detection to be changed after WSL2\nWith WSL2, IP address is from 172 network.\nThis looks like a virtual internal address. More detail at that address: https://github.com/microsoft/WSL/issues/4150.\n{% include alert.html text=“to update IP detection” %}"
  },
  {
    "objectID": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#oct-21-2020-use-git-with-github-ssh-behind-corporate-proxy",
    "href": "posts/2020-09-15-autodetect-home-office-network-and-proxy-settings.html#oct-21-2020-use-git-with-github-ssh-behind-corporate-proxy",
    "title": "Autodetect Home / Office network + Proxy",
    "section": "Oct-21 2020: Use git with github (ssh) behind corporate proxy",
    "text": "Oct-21 2020: Use git with github (ssh) behind corporate proxy\nHere is the new configuration explained in my blog entry Use git with github (ssh) behind corporate proxy\nIt is just a matter of linking appropriate files when I am in or out of corporate network.\nAs in my_ip.sh:\n# Set Proxy\nfunction setproxy() {\n     echo \"Calling setproxy\"\n     export {http,https,ftp}_proxy=\"http://proxy_ip:80\"\n     export {HTTP,HTTPS,FTP}_PROXY=\"http://proxy_ip:80\"\n     #proxy for wget\n     ln -sf ~/proxy_files/.wgetrc_proxy ~/.wgetrc\n     #proxy for apt\n     #sudo ln -sf ~/proxy_files/apt_proxy.conf /etc/apt/apt.conf.d/proxy.conf\n     #proxy for conda\n     ln -sf ~/proxy_files/.condarc_proxy ~/.condarc\n     #proxy for git\n     git config --global http.proxy http://proxy_ip:80\n     ln -sf ~/proxy_files/ssh_config_proxy ~/.ssh/config\n}\n\n# Unset Proxy\nfunction unsetproxy() {\n     echo \"Calling unsetproxy\"\n     unset {http,https,ftp}_proxy\n     unset {HTTP,HTTPS,FTP}_PROXY\n     #no proxy for wget\n     ln -sf ~/proxy_files/.wgetrc_noproxy ~/.wgetrc\n     #no proxy for apt\n     #sudo rm -f /etc/apt/apt.conf.d/proxy.conf\n     #no proxy for conda\n     ln -sf ~/proxy_files/.condarc_noproxy ~/.condarc\n     #no proxy for git\n     git config --global --unset http.proxy\n     ln -sf ~/proxy_files/ssh_config_noproxy ~/.ssh/config\n}"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "",
    "text": "This is based on what is explained in https://forums.fast.ai/t/fastai-on-wsl-2-ubuntu-0-7-0-or-any-version/76651"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-update-of-nvidia-drivers",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-update-of-nvidia-drivers",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "install update of nvidia drivers",
    "text": "install update of nvidia drivers\nBased on Deep Learning Course Forums Platform: Windows 10 using WSL2 w/GPU fastai users\n\ncreate nvidia account\ndownload quadro driver from https://developer.nvidia.com/cuda/wsl/download (460.15)\ninstall\n\n(xgboost) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog$ nvidia-smi.exe\nMon Sep 21 16:00:46 2020\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.15       Driver Version: 460.15       CUDA Version: 11.1     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Quadro M1000M      WDDM  | 00000000:01:00.0  On |                  N/A |\n| N/A   59C    P0    N/A /  N/A |    905MiB /  4096MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-of-wsl2-and-convert-existing-images",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-of-wsl2-and-convert-existing-images",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "install of WSL2 and convert existing images",
    "text": "install of WSL2 and convert existing images\nOpen a PowerShell window as an Administrator\nRun wsl –set-default-version 2\n\nupdate KB\n{% include alert.html text=“–set-default-version 2 is not a valid option. KB4566116 should be installed” %}\nThis can be downloaded from Catalog Microsoft Update\n\n\nupdate kernel version\nIf you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel. You still need to install the MSI Linux kernel update package.\nDownload from https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4—download-the-linux-kernel-update-package\n\n\nset default WSL to be version 2\nPS C:\\WINDOWS\\system32> wsl --set-default-version 2\n\n\nconvert existing images\nPS C:\\WINDOWS\\system32> wsl --list --verbose\n  NAME            STATE           VERSION\n* Ubuntu-18.04    Running         1\nPS C:\\WINDOWS\\system32> wsl --set-version Ubuntu-18.04 2\nLa conversion est en cours. Cette opération peut prendre quelques minutes...\nPour plus d’informations sur les différences de clés avec WSL 2, visitez https://aka.ms/wsl2\nLa conversion est terminée.\nIt took a while (~1 hour) for my unique ubuntu image.\nAnd at the end it has worked.\nPS C:\\WINDOWS\\system32> wsl --list --verbose\n  NAME            STATE           VERSION\n* Ubuntu-18.04    Stopped         2"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-of-nvidia-drivers-under-ubuntu",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-of-nvidia-drivers-under-ubuntu",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "install of nvidia drivers under ubuntu",
    "text": "install of nvidia drivers under ubuntu\n[Installation instructions)(https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1804&target_type=deblocal)\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\nsudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb\nsudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub\nsudo apt-get update\nsudo apt-get -y install cuda\n\n!cat /usr/local/cuda/version.txt\n\nCUDA Version 11.0.228\n\n\n\n!/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes\n\n[/usr/local/cuda/samples/4_Finance/BlackScholes/BlackScholes] - Starting...\nCUDA error at ../../common/inc/helper_cuda.h:777 code=35(cudaErrorInsufficientDriver) \"cudaGetDeviceCount(&device_count)\" \n\n\nThere is an error when launching CUDA samples. Googling that error maybe my video card is running on low driver version?\nI have posted on nvidia (cuda+wsl) forum: https://forums.developer.nvidia.com/t/cuda-sample-throwing-error/142537/18\n(update 09-22: it is not possible to have cuda on wsl2 if not in Windows Insider build from Dev Channel. (20145 or higher))\n{% include alert.html text=“because I am in version 1909 (18363.1049), it won’t work for me. ;(” %}\n\ncuda for WSL\nHere is a link that could be interesting: https://docs.nvidia.com/cuda/wsl-user-guide/index.html\nAccording to this, I should not have installed cuda but cuda-toolkit. Do not choose the cuda, cuda-11-0, or cuda-drivers meta-packages under WSL 2 since these packages will result in an attempt to install the Linux NVIDIA driver under WSL 2.\nIs it causing my issue?\napt-get install -y cuda-toolkit-11-0\n\n!/usr/local/cuda/bin/nvcc --version\n\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\nCuda compilation tools, release 11.0, V11.0.221\nBuild cuda_11.0_bu.TC445_37.28845127_0"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-a-new-distro-ubuntu-20.04",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#install-a-new-distro-ubuntu-20.04",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "install a new distro (ubuntu 20.04)",
    "text": "install a new distro (ubuntu 20.04)\nBecause I cannot use windows store, I have to manually install https://docs.microsoft.com/fr-fr/windows/wsl/install-manual\nInstallation by just launching Ubuntu_2004.2020.424.0_x64.appx.\nI have now 2 distros,\nPS C:\\WINDOWS\\system32> wsl --list -v\n  NAME            STATE           VERSION\n* Ubuntu-18.04    Running         2\n  Ubuntu-20.04    Running         2"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#wsl2-and-network",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#wsl2-and-network",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "WSL2 and network",
    "text": "WSL2 and network\nThere is a change of network architecture between WSL 1 and WSL 2. In WSL 2, a new network interface is available:\nCarte Ethernet vEthernet (WSL) :\n\n   Suffixe DNS propre à la connexion. . . :\n   Adresse IPv6 de liaison locale. . . . .: \n   Adresse IPv4. . . . . . . . . . . . . .: 192.168.81.193\n   Masque de sous-réseau. . . . . . . . . : 255.255.255.240\n   Passerelle par défaut. . . . . . . . . :\n\nRevert image to WSL1 to get back network access\nPS C:\\WINDOWS\\system32> wsl --set-version Ubuntu-18.04 1\nLa conversion est en cours. Cette opération peut prendre quelques minutes...\nLa conversion est terminée.\nPS C:\\WINDOWS\\system32> wsl --list -v\n  NAME            STATE           VERSION\n* Ubuntu-18.04    Stopped         1\n  Ubuntu-20.04    Stopped         2"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#access-to-linux-files-from-windows",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#access-to-linux-files-from-windows",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "access to linux files from windows",
    "text": "access to linux files from windows\nFor running state distros:\nFiles are available at \\\\wsl$\\.\nFor stopped state distros:\nFiles are available at C:\\Users\\<users>\\AppData\\Local\\Packages\\CanonicalGroupLimited.Ubuntu*\\LocalState\\rootfs"
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#some-usefull-wsl-commands",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#some-usefull-wsl-commands",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "some usefull wsl commands",
    "text": "some usefull wsl commands\nList distros\n\n!wsl --list --verbose\n\n  NAME            STATE           VERSION\n\n* Ubuntu-18.04    Running         1\n\n  Ubuntu-20.04    Stopped         2\nStop a distro\n\n!wsl --terminate Ubuntu-18.04\n\nUpdate dns settings\nas explained here\njust switch from generateResolvConf = true to generateResolvConf = false in /etc/wsl.conf and edit /etc/resolv.conf\nBut still have issues, mainly I think linked to Symantec Endpoint protection."
  },
  {
    "objectID": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#workaround-network-issue-with-wsl2",
    "href": "posts/2020-09-21-windows10-fastai-wsl2-cuda.html#workaround-network-issue-with-wsl2",
    "title": "Fastai on WSL 2 with Cuda",
    "section": "Workaround network issue with WSL2",
    "text": "Workaround network issue with WSL2\nhttps://github.com/sakai135/wsl-vpnkit\n\ninstallation setup\n\nvpnkit\ninstall docker for windows\ninstall genisoimage in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/c/cdrkit/genisoimage_1.1.11-3.1ubuntu1_amd64.deb)\ninstall vpnkit\nisoinfo -i /mnt/c/Program\\ Files/Docker/Docker/resources/wsl/docker-for-wsl.iso -R -x /containers/services/vpnkit-tap-vsockd/lower/sbin/vpnkit-tap-vsockd > ./vpnkit-tap-vsockd\nchmod +x vpnkit-tap-vsockd\nsudo mv vpnkit-tap-vsockd /sbin/vpnkit-tap-vsockd\nsudo chown root:root /sbin/vpnkit-tap-vsockd\n\n\nnpiperelay\ninstall unzip in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/u/unzip/unzip_6.0-25ubuntu1_amd64.deb)\ndownload npiprelay (from https://github.com/jstarks/npiperelay/releases/download/v0.1.0/npiperelay_windows_amd64.zip )\ninstall npiprelay\nunzip npiperelay_windows_amd64.zip npiperelay.exe\nrm npiperelay_windows_amd64.zip\nmkdir -p /mnt/c/bin\nmv npiperelay.exe /mnt/c/bin/\nsudo ln -s /mnt/c/bin/npiperelay.exe /usr/local/bin/npiperelay.exe\n\n\nsocat\ninstall socat in ubuntu (from http://archive.ubuntu.com/ubuntu/pool/main/s/socat/socat_1.7.3.3-2_amd64.deb)\n\n\nConfigure DNS for WSL\nDisable WSL from generating and overwriting /etc/resolv.conf.\nsudo tee /etc/wsl.conf <<EOL\n[network]\ngenerateResolvConf = false\nEOL\nManually set DNS servers to use when not running this script. 1.1.1.1 is provided as an example.\nsudo tee /etc/resolv.conf <<EOL\nnameserver 1.1.1.1\nEOL\n\n\nwsl-vpnkit\nfrom https://github.com/sakai135/wsl-vpnkit/archive/refs/heads/main.zip\nunzip ~/git/wsl-vpnkit-main.zip -d ~/Applications/wsl-vpnkit\n\n\n\nexecution\n~/Applications/wsl-vpnkit/wsl-vpnkit-main$ sudo ./wsl-vpnkit\n\n\nproxychains\nI can now use proxychains and everything works beautifully ;)\n\n\nclean from local deb install to ubuntu repo\n~/git$ ls *.deb\ngenisoimage_1.1.11-3.1ubuntu1_amd64.deb                proxychains_3.1-8.1_all.deb\nlibproxychains3_3.1-8.1_amd64.deb                      socat_1.7.3.3-2_amd64.deb\nnet-tools_1.60+git20180626.aebd88e-1ubuntu1_amd64.deb  unzip_6.0-25ubuntu1_amd64.deb\nsudo apt remove genisoimage  net-tools  socat unzip\nsudo proxychains apt install genisoimage libproxychains3 net-tools proxychains socat unzip\nI don’t exactly see how to do it with proxychains."
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html",
    "title": "Setup ubuntu box with fastai",
    "section": "",
    "text": "Get miniconda Linux installer.\nCheck sha256sum: sha256sum Miniconda3-latest-Linux-x86_64.sh\nRun install: ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3"
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#install-fastai",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#install-fastai",
    "title": "Setup ubuntu box with fastai",
    "section": "Install fastai",
    "text": "Install fastai\nconda create -n fastai python=3.8\nconda activate fastai\nconda install -c fastai -c pytorch fastai"
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#install-jupyter-within-fastai-environment",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#install-jupyter-within-fastai-environment",
    "title": "Setup ubuntu box with fastai",
    "section": "Install jupyter within fastai environment",
    "text": "Install jupyter within fastai environment\nconda activate fastai\nconda install jupyter"
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#test-fastai-installation-valid-for-v1",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#test-fastai-installation-valid-for-v1",
    "title": "Setup ubuntu box with fastai",
    "section": "Test fastai installation (valid for v1)",
    "text": "Test fastai installation (valid for v1)\nWith fastai v1, there was an easy way to check installation:\nconda activate fastai\npython -m fastai.utils.show_install"
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#get-git-repo-to-learn-from-fastai",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#get-git-repo-to-learn-from-fastai",
    "title": "Setup ubuntu box with fastai",
    "section": "get git repo to learn from fastai",
    "text": "get git repo to learn from fastai\nFrom git folder,\ngit clone https://github.com/fastai/fastai"
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#test-fastai-v2-installation",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#test-fastai-v2-installation",
    "title": "Setup ubuntu box with fastai",
    "section": "Test fastai v2 installation",
    "text": "Test fastai v2 installation\nFrom python environment:\nfrom fastai.vision.all import *\nFrom jupyter notebook\n\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#install-nvidia-drivers-for-ubuntu",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#install-nvidia-drivers-for-ubuntu",
    "title": "Setup ubuntu box with fastai",
    "section": "Install nvidia drivers for ubuntu",
    "text": "Install nvidia drivers for ubuntu\nI tried by downloading a driver from nvidia website. But I was unable to install it (nvidia-drm-drv.c:662:44: error: 'DRIVER_PRIME' undeclared here (not in a function); did you mean 'DRIVER_PCI_DMA'?)\nsudo ubuntu-drivers autoinstall\nthen rebooting fixed the issue."
  },
  {
    "objectID": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#run-courses-from-fastai-github-repo",
    "href": "posts/2020-09-23-setup-ubuntu-box-with-fastai.html#run-courses-from-fastai-github-repo",
    "title": "Setup ubuntu box with fastai",
    "section": "Run courses from fastai github repo",
    "text": "Run courses from fastai github repo\njust run fastai/dev_nbs/course/lesson1-pets.ipynb\nAnd everything is just fined ;)\n\ninstall nbdev\nThis is for rendering reasons: To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev\n\n!pip install nbdev\n\nCollecting nbdev\n  Downloading nbdev-1.0.18-py3-none-any.whl (57 kB)\n     |████████████████████████████████| 57 kB 1.5 MB/s eta 0:00:01\nRequirement already satisfied: fastcore>=1.0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (1.0.13)\nRequirement already satisfied: packaging in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.4)\nRequirement already satisfied: jupyter-client in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (6.1.6)\nRequirement already satisfied: nbconvert<6 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.6.1)\nRequirement already satisfied: nbformat>=4.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.0.7)\nCollecting fastscript>=1.0.0\n  Downloading fastscript-1.0.0-py3-none-any.whl (11 kB)\nRequirement already satisfied: pyyaml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.1)\nRequirement already satisfied: pip in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (20.2.2)\nRequirement already satisfied: ipykernel in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbdev) (5.3.4)\nRequirement already satisfied: pyparsing>=2.0.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging->nbdev) (2.4.7)\nRequirement already satisfied: six in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from packaging->nbdev) (1.15.0)\nRequirement already satisfied: pyzmq>=13 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client->nbdev) (19.0.2)\nRequirement already satisfied: tornado>=4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client->nbdev) (6.0.4)\nRequirement already satisfied: python-dateutil>=2.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client->nbdev) (2.8.1)\nRequirement already satisfied: traitlets in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client->nbdev) (4.3.3)\nRequirement already satisfied: jupyter-core>=4.6.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jupyter-client->nbdev) (4.6.3)\nRequirement already satisfied: pandocfilters>=1.4.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (1.4.2)\nRequirement already satisfied: pygments in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (2.7.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (0.8.4)\nRequirement already satisfied: defusedxml in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (0.6.0)\nRequirement already satisfied: bleach in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (3.2.1)\nRequirement already satisfied: jinja2>=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (2.11.2)\nRequirement already satisfied: testpath in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (0.4.4)\nRequirement already satisfied: entrypoints>=0.2.2 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbconvert<6->nbdev) (0.3)\nRequirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat>=4.4.0->nbdev) (3.0.2)\nRequirement already satisfied: ipython-genutils in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from nbformat>=4.4.0->nbdev) (0.2.0)\nRequirement already satisfied: ipython>=5.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipykernel->nbdev) (7.18.1)\nRequirement already satisfied: decorator in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from traitlets->jupyter-client->nbdev) (4.4.2)\nRequirement already satisfied: webencodings in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from bleach->nbconvert<6->nbdev) (0.5.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert<6->nbdev) (1.1.1)\nRequirement already satisfied: pyrsistent>=0.14.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (0.17.3)\nRequirement already satisfied: attrs>=17.4.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (20.2.0)\nRequirement already satisfied: setuptools in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4.0->nbdev) (49.6.0.post20200814)\nRequirement already satisfied: backcall in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (0.2.0)\nRequirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (4.8.0)\nRequirement already satisfied: pickleshare in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (0.7.5)\nRequirement already satisfied: jedi>=0.10 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (0.17.2)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->nbdev) (3.0.7)\nRequirement already satisfied: ptyprocess>=0.5 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel->nbdev) (0.6.0)\nRequirement already satisfied: parso<0.8.0,>=0.7.0 in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->nbdev) (0.7.0)\nRequirement already satisfied: wcwidth in /home/explore/miniconda3/envs/fastai/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->nbdev) (0.2.5)\nInstalling collected packages: fastscript, nbdev\nSuccessfully installed fastscript-1.0.0 nbdev-1.0.18"
  },
  {
    "objectID": "posts/2020-09-24-fastai-book.html",
    "href": "posts/2020-09-24-fastai-book.html",
    "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
    "section": "",
    "text": "Of course the 1st tep is to purchase this great book:\n\n\n\n\n\nalt text\n\n\nI have liked what Jeremy Howard said about why this is important to purchase it (in video 1). Fastai is offering full access to the book as notebooks. So that we can run all codes from them."
  },
  {
    "objectID": "posts/2020-09-24-fastai-book.html#update-jupyter-to-include-extensions-toc",
    "href": "posts/2020-09-24-fastai-book.html#update-jupyter-to-include-extensions-toc",
    "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
    "section": "update jupyter to include extensions (toc, …)",
    "text": "update jupyter to include extensions (toc, …)\nconda install -c conda-forge jupyter_contrib_nbextensions\n I like table of content, others are quite usefull as well (scratchpad, ExecuteTime…)."
  },
  {
    "objectID": "posts/2020-09-24-fastai-book.html#install-some-libraries-to-run-book-examples",
    "href": "posts/2020-09-24-fastai-book.html#install-some-libraries-to-run-book-examples",
    "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
    "section": "Install some libraries to run book examples",
    "text": "Install some libraries to run book examples\nconda install -c fastai fastbook\nIt will install graphviz, nbdev, and other libraries.\nAnd most of them can be loaded by calling from utils import *"
  },
  {
    "objectID": "posts/2020-09-24-fastai-book.html#launch-jupyter-notebook-and-start-expermenting",
    "href": "posts/2020-09-24-fastai-book.html#launch-jupyter-notebook-and-start-expermenting",
    "title": "Fastai book Deep Learning for Coders with fastai and Pytorch",
    "section": "Launch jupyter notebook and start expermenting",
    "text": "Launch jupyter notebook and start expermenting\ncd ~/git/guillaume\nconda activate fastai\njupyter notebook\nAnd launch several tabs at: blog entries, fastai experiments, fastai courses and fastai videos.\nAnd I keep track of progress with git."
  },
  {
    "objectID": "posts/2020-09-26-matplotlib-multiple-subplots-and-animations.html",
    "href": "posts/2020-09-26-matplotlib-multiple-subplots-and-animations.html",
    "title": "Multiple subplots and animations with matplotlib",
    "section": "",
    "text": "Animation\nimport\n\n%matplotlib inline\n\n# fastai v1 backward compatibility\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\ndef tensor(*argv): return torch.tensor(argv)\n\n# TEST\nassert torch.all(tensor(1,2) == torch.tensor([1,2])), 'Backward compatibility with fastai v1'\n\nfunction and plot\n\nn=100\nx = torch.ones(n,1) \nx.uniform_(-3.14,3.14)\n\ndef my_function(x, a):\n    return ((torch.cat((x**3, x**2, x, torch.ones(n,1) ), 1))@a).reshape((n))\n\na=tensor(4., 2., -12., 5.)\ny = my_function(x, a)\n\na = tensor(-1.,-2., 6., -8)\ny_hat = my_function(x, a)\n\n\nplt.scatter(x[:,0], y)\nplt.scatter(x[:,0],y_hat);\n\ndef mse(y_hat, y): return ((y_hat-y)**2).mean()\n\n\n\n\ngradient descent\n\na = nn.Parameter(a); a\n\ndef update():\n    y_hat = my_function(x, a)\n    loss = mse(y, y_hat)\n    if t % 10 == 0: print(loss)\n    loss.backward()\n    with torch.no_grad():\n        a.sub_(lr * a.grad)\n        a.grad.zero_()\n\n        \nlr = 1e-3\nfor t in range(100): update()\n\ntensor(1967.0251, grad_fn=<MeanBackward0>)\ntensor(559.2718, grad_fn=<MeanBackward0>)\ntensor(365.7207, grad_fn=<MeanBackward0>)\ntensor(282.6393, grad_fn=<MeanBackward0>)\ntensor(245.4054, grad_fn=<MeanBackward0>)\ntensor(227.3450, grad_fn=<MeanBackward0>)\ntensor(217.3324, grad_fn=<MeanBackward0>)\ntensor(210.7267, grad_fn=<MeanBackward0>)\ntensor(205.5912, grad_fn=<MeanBackward0>)\ntensor(201.1171, grad_fn=<MeanBackward0>)\n\n\nanimation\n\nfrom matplotlib import animation, rc\nrc('animation', html='jshtml')\n\na = nn.Parameter(tensor(-1.,1))\n\na=tensor(4., 2., -12., 5.)\ny = my_function(x, a)\n\na = tensor(-1.,-2., 6., -8)\ny_hat = my_function(x, a)\na = nn.Parameter(a); a\n\nfig = plt.figure()\nplt.scatter(x[:,0], y, c='orange')\nline = plt.scatter(x[:,0], y_hat.detach())\nplt.close()\n\ndef animate(i):\n    line.set_offsets(np.c_[x[:,0], (my_function(x,a)).detach()])\n    update()\n\n    return line,\n\nanimation.FuncAnimation(fig, animate, np.arange(0, 300), interval=5)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/2020-09-28-upgrade-ubuntu-18.04-to-20.04.html",
    "href": "posts/2020-09-28-upgrade-ubuntu-18.04-to-20.04.html",
    "title": "Upgrade ubuntu LTS 18.04 to 20.04",
    "section": "",
    "text": "Waiting for blockers to be fixed\nThere is a last blocker before releasing Ubuntu 20.04.1 LTS.\n{% include alert.html text=“Expected around 1st of October 2020.” %}\n\n\n(2020-09-28) blockers are fixed, upgrade in progress\nUnfortunately the upgrade process went uneventful. Nothing broke, nothing to learn ;)\nIt took minutes to do the upgrade.\n\n\n\n\n\nalt text\n\n\nUbuntu releases-code names"
  },
  {
    "objectID": "posts/2020-10-01-fingerprint-authentication-sudoers.html",
    "href": "posts/2020-10-01-fingerprint-authentication-sudoers.html",
    "title": "Use fingerprint to authenticate on Ubuntu, and passwordless on some apps",
    "section": "",
    "text": "Passwordless commands\nBecause I have changed my password for a quite complex one, I am interested to launch some sudo commands without prompt of password.\nHow to run sudo commands without password\nUse visudo to update /etc/sudoers. I understand there is some syntax check to avoid mistake when editing this file. You don’t want to be left with a defective sudo system.\nI have just added this line. explore is my username. I can add additional commands after a comma (e.g. /bin/systemctl restart httpd.service, /bin/kill)\nexplore ALL = NOPASSWD: /usr/bin/apt"
  },
  {
    "objectID": "posts/2020-10-01-gan-pytorch-coursera.html",
    "href": "posts/2020-10-01-gan-pytorch-coursera.html",
    "title": "Generative Adversarial Networks (GANs) Specialization from Coursera",
    "section": "",
    "text": "env installation\nI am just getting the version from coursera to be sure I have the same behaviour.\n\nimport torch\n\n\nprint(torch.__version__)\n\n1.4.0\n\n\nSo I can now create a gan environment with appropriate lib versions.\nconda create -n gan python=3.7\nconda activate gan\nconda install -c pytorch pytorch=1.4.0\nconda install jupyter matplotlib\nconda install -c conda-forge tqdm\nconda install -c pytorch torchvision\n\n\ngit settings\necho \"# gan_specialization from coursera\" >> README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M master\ngit remote add origin git@github.com:castorfou/gan_specialization.git\ngit push -u origin master\nI am pushing notebooks to github\n\n\nIntro to PyTorch\nI have exported Intro to Pytorch notebook from coursera lab.\nTo run it on my machine.\n\n\nIntro to GAN using tensorflow\nVersions of python seems incompatible between each other. (3.7 for pytorch, 3.6 for tensorflow=1.10)\nI create a new python environment:\nconda create -n gan_tensorflow python=3.6\nconda activate gan_tensorflow\nconda install jupyter\nconda install -c conda-forge requests\npip install tensorflow-gpu==1.15"
  },
  {
    "objectID": "posts/2020-10-06-gan-specialization-course1-week1-intro_to_gan.html",
    "href": "posts/2020-10-06-gan-specialization-course1-week1-intro_to_gan.html",
    "title": "GAN Specialization course 1 week 1 - intro to GAN",
    "section": "",
    "text": "alt text\n\n\n\n\n\n\n\nalt text"
  },
  {
    "objectID": "posts/2020-10-06-gan-specialization-course1-week2-deep_convolutional_gan.html",
    "href": "posts/2020-10-06-gan-specialization-course1-week2-deep_convolutional_gan.html",
    "title": "GAN Specialization course 1 week 2 - Deep Convolutional GAN",
    "section": "",
    "text": "alt text"
  },
  {
    "objectID": "posts/2020-10-07-conda-activate-from-bash-script.html",
    "href": "posts/2020-10-07-conda-activate-from-bash-script.html",
    "title": "Conda activate from bash scripts",
    "section": "",
    "text": "source ~/your_conda/etc/profile.d/conda.sh\nIt is just a matter of sourcing the conda bash settings before calling conda activate.\nIn m case I have installed conda in ~/miniconda3, I just have to call source ~/miniconda3/etc/profile.d/conda.sh\n\n\nExample to run my blogging environment\n#!/bin/bash\nsource ~/miniconda3/etc/profile.d/conda.sh\ncd ~/git/guillaume/guillaume_blog/_notebooks\nconda activate fastai\njupyter notebook"
  },
  {
    "objectID": "posts/2020-10-07-decorator-trace-variables.html",
    "href": "posts/2020-10-07-decorator-trace-variables.html",
    "title": "Variables traces using show_guts decorator",
    "section": "",
    "text": "use example\n\nimport torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import CelebA\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\n\n@show_guts\ndef get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n    '''\n    Function to return the score of the current classifications, penalizing changes\n    to other classes with an L2 norm.\n    Parameters:\n        current_classifications: the classifications associated with the current noise\n        original_classifications: the classifications associated with the original noise\n        target_indices: the index of the target class\n        other_indices: the indices of the other classes\n        penalty_weight: the amount that the penalty should be weighted in the overall score\n    '''\n    # Steps: 1) Calculate the change between the original and current classifications (as a tensor)\n    #           by indexing into the other_indices you're trying to preserve, like in x[:, features].\n    #        2) Calculate the norm (magnitude) of changes per example.\n    #        3) Multiply the mean of the example norms by the penalty weight. \n    #           This will be your other_class_penalty.\n    #           Make sure to negate the value since it's a penalty!\n    #        4) Take the mean of the current classifications for the target feature over all the examples.\n    #           This mean will be your target_score.\n    #### START CODE HERE ####\n    change_original_classification = (current_classifications[:,other_indices] - original_classifications[:,other_indices])\n    # Calculate the norm (magnitude) of changes per example and multiply by penalty weight\n    other_class_penalty = - torch.mean(torch.norm(change_original_classification, dim=1) * penalty_weight)\n    # Take the mean of the current classifications for the target feature\n    target_score = torch.mean(current_classifications)\n    #### END CODE HERE ####\n    return target_score + other_class_penalty\n\n\nrows = 10\ncurrent_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\noriginal_class = torch.tensor([[1] * rows, [2] * rows, [3] * rows, [4] * rows]).T.float()\n\n# Must be 3\nassert get_score(current_class, original_class, [1, 3] , [0, 2], 0.2).item() == 3\n\ncurrent_classifications: tensor([[1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.]])\noriginal_classifications: tensor([[1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.],\n        [1., 2., 3., 4.]])\ntarget_indices: [1, 3]\nother_indices: [0, 2]\npenalty_weight: 0.2\nchange_original_classification: tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.],\n        [0., 0.]])\nother_class_penalty: tensor(-0.)\ntarget_score: tensor(2.5000)\nReturned: tensor(2.5000)\n\n\nAssertionError:"
  },
  {
    "objectID": "posts/2020-10-07-gan-specialization-week3-mode_collapse-w_loss.html",
    "href": "posts/2020-10-07-gan-specialization-week3-mode_collapse-w_loss.html",
    "title": "GAN Specialization course 1 week 3 - mode collapse, vanishing gradient, wasserstein loss",
    "section": "",
    "text": "Earth mover’s distance. Wasserstein loss. 1-L continuous condition\n\n\n\n\n\nalt text"
  },
  {
    "objectID": "posts/2020-10-07-gan-specialization-week4-conditional_generation-controllable-generation.html",
    "href": "posts/2020-10-07-gan-specialization-week4-conditional_generation-controllable-generation.html",
    "title": "GAN Specialization course 1 week 4 - conditional generation, controllable generation",
    "section": "",
    "text": "End of course\nNext one is a 3 week course named: Build Better Generative Adversarial Networks (GANs)\nHere is my certificate\n\n\n\n\n\nalt text"
  },
  {
    "objectID": "posts/2020-10-09-gan-course2-week1-evaluations-on-gans.html",
    "href": "posts/2020-10-09-gan-course2-week1-evaluations-on-gans.html",
    "title": "GAN Specialization course 2 week 1 - evaluations on GANs",
    "section": "",
    "text": "alt text"
  },
  {
    "objectID": "posts/2020-10-09-gan-course2-week2-disadventages-bias.html",
    "href": "posts/2020-10-09-gan-course2-week2-disadventages-bias.html",
    "title": "GAN Specialization course 2 week 2 - Disadvantages and Bias",
    "section": "",
    "text": "alt text"
  },
  {
    "objectID": "posts/2020-10-15-gan-course2-week3-certificate.html",
    "href": "posts/2020-10-15-gan-course2-week3-certificate.html",
    "title": "GAN Specialization course 2 week 3 - Apply and certificate",
    "section": "",
    "text": "About next steps\nFor the moment I am hesitating to follow last course for gans. This is a deep dive into gan, with lots of theory (papers) associated to it. My 1st goal was to know better about it and this is met. My 2nd one was to figure out how to use this kind of generative networks for other area such as tabular data + prescription issues. I am not sure this is applicable (or not yet).\nMaybe it is better for now to resume my learning sessions with Jeremy Howard on fastai v2."
  },
  {
    "objectID": "posts/2020-10-19-test-inner-images_in-jupyter.html",
    "href": "posts/2020-10-19-test-inner-images_in-jupyter.html",
    "title": "Validation of jupyter inner images with fastpages",
    "section": "",
    "text": "image.png"
  },
  {
    "objectID": "posts/2020-10-20-mit-edx-introduction-computational-thinking-and-data-science.html",
    "href": "posts/2020-10-20-mit-edx-introduction-computational-thinking-and-data-science.html",
    "title": "edX MIT 6.00.2x Introduction to Computational Thinking and Data Science",
    "section": "",
    "text": "Last summer I have been following a 1st MIT course on python programming. Not that I would need this knowledge but as for Polytechnique courses, I like their way to explain knowledge foundations. Teachers from these schools tend to go back to deep roots, and provide clear and somtimes illuminating examples to help us understand concepts.\nAbout 6 years ago I have completed a Probability introduction from Ecole Polythechnique. That was great. I had always been hermeticly closed to probability and statistics. For a reason I don’t understand, it is not being teached in CPGE (which is a two-or-three-year intensive full-time course preparing top high school graduates for the entrance examination of French engineering and business schools, this is just after high school). It means last time I was exposed to probability was in high school, and probably in engineering school as well but on a light way.\nThat would be great to give back a look to these courses.\n\n\nUnfortunately I registered in September when only a couple of weeks were left to complete this 9-week course. And because I didn’t upgrade to a Verified Certificate, I lost access to materials and progress. It cut when my progress was about 38%.\nI like Eric Grimson’s style. He is calm and has his own way to explain some advanced subjects.\nNext session is planned on Jan 27, 2021. That would be a good idea to complete this course.\n\n\n\nFor this one I have registered on time. And I have purchased the Verified Certificate.\nHere is the full course program and dates:\n\n\n\n\n\nalt text\n\n\n1st lectures are interesting. As said before I like to be back to roots of problems. And on that matter I expect to get a full overview.\nLecture 4 should be released in the coming at the end of October. cannot wait to resume these sessions.\nAs a matter of comparaison with gan specialization from coursera+openAI, I like better the interactions with students offered by openAI. They use slack as a platform to support these interactions and I think it is a smart move."
  },
  {
    "objectID": "posts/2020-10-21-github-ssh-behind-proxy.html",
    "href": "posts/2020-10-21-github-ssh-behind-proxy.html",
    "title": "Use git with github (ssh) behind corporate proxy",
    "section": "",
    "text": "alt text\n\n\nI use 2 kinds of repo. gitlab for internal/corporate projects, hosted inside my company. github for public/pet projects and as a blogging platform. 3 days a week I am inside company, 4 days a week outside.\nGreen lines are the natural path to collaborate.\nWhen outside I don’t have proxy configuration or firewall, and I can directly access github. I cannot access to gitlab but I don’t want to address it now, this is why it is set as a black line. (if this is really needed I have a vpn access and this is as being inside)\nWhen inside, I use internal proxy. I can directly access gitlab. But I want to access github in a transparent way. And yes from both Windows and Linux (WSL). This is the red line."
  },
  {
    "objectID": "posts/2020-10-21-github-ssh-behind-proxy.html#revert-socks5-for-git",
    "href": "posts/2020-10-21-github-ssh-behind-proxy.html#revert-socks5-for-git",
    "title": "Use git with github (ssh) behind corporate proxy",
    "section": "Revert socks5 for git",
    "text": "Revert socks5 for git\nJust by commenting ProxyCommand in ssh for github\n$ cat .ssh/config\nHost github.com\nIdentityFile ~/.ssh/id_rsa_gmail\n#ProxyCommand /bin/nc -X 5 -x 192.168.50.202:1080 %h %p\n\nHost gitlab.michelin.com\nIdentityFile ~/.ssh/id_rsa\nSecond step is to migrate remote-url from ssh to https:"
  },
  {
    "objectID": "posts/2020-10-21-github-ssh-behind-proxy.html#from-git-ssh-to-git-https",
    "href": "posts/2020-10-21-github-ssh-behind-proxy.html#from-git-ssh-to-git-https",
    "title": "Use git with github (ssh) behind corporate proxy",
    "section": "from git ssh to git https",
    "text": "from git ssh to git https\n$ git remote set-url origin https://githib.com/castorfou/mit_600.2x.git\n$ git remote -v\norigin  https://github.com/castorfou/mit_600.2x.git (fetch)\norigin  https://github.com/castorfou/mit_600.2x.git (push)\nThis allows to fetch and pull updates\ngit fetch\ngit pull\nLastly to setup passwordless access to github"
  },
  {
    "objectID": "posts/2020-10-21-github-ssh-behind-proxy.html#github-token-to-access-passwordless-using-https",
    "href": "posts/2020-10-21-github-ssh-behind-proxy.html#github-token-to-access-passwordless-using-https",
    "title": "Use git with github (ssh) behind corporate proxy",
    "section": "Github token to access passwordless using https",
    "text": "Github token to access passwordless using https\nFrom https://clarusway.com/passwordless-usage-of-private-git-repositories/\nTo Generate token in github: - (profile > Settings > Developer settings > Personal access tokens) - Generate new token - Select repo section\nIntegrate into git config: - copy token into .git/config remote url ( from url = https://github.com/castorfou/guillaume_blog.git to url = https://mytoken@github.com/castorfou/guillaume_blog.git)"
  },
  {
    "objectID": "posts/2020-10-21-open-jupyter-from-http-link.html",
    "href": "posts/2020-10-21-open-jupyter-from-http-link.html",
    "title": "Open Jupyter Notebook with http launch instead of redirect file",
    "section": "",
    "text": "Solution\nAs given in https://stackoverflow.com/questions/57679894/how-to-change-jupyter-launch-from-file-to-url,\nupdate jupyter config file to change #c.NotebookApp.use_redirect_file = True to c.NotebookApp.use_redirect_file = False\n\n\n\n\n\nalt text"
  },
  {
    "objectID": "posts/2020-12-02-push-big-files-to-github.html",
    "href": "posts/2020-12-02-push-big-files-to-github.html",
    "title": "Push large files to github: git-lfs",
    "section": "",
    "text": "Solution: git-lfs\nAs explained in https://github.com/git-lfs/git-lfs/wiki/Tutorial, there is (always) a way to do it properly.\nFirst it is a matter of installing git-lfs:\nsudo apt-get install git-lfs\nThen to setup git lfs\ngit lfs install\nAnd then to “migrate” big files to lfs:\ngit lfs migrate import --include=\"*.mp4\"\ngit lfs migrate import --include=\"*.h5\"\nAnd now to git push\n(base) explore@explore-ThinkPad-P53:~/git/guillaume/deeplearning_specialization$ git push\nUploading LFS objects: 100% (25/25), 954 MB | 37 MB/s, done.                                                                 \nEnumerating objects: 311, done.\nCounting objects: 100% (311/311), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (273/273), done.\nWriting objects: 100% (276/276), 60.49 MiB | 5.59 MiB/s, done.\nTotal 276 (delta 18), reused 0 (delta 0)\nremote: Resolving deltas: 100% (18/18), completed with 16 local objects.\nTo github.com:castorfou/deeplearning_specialization.git\n   d0d2dc2..004fa09  master -> master"
  },
  {
    "objectID": "posts/2020-12-08-share-github-repo-on-2-pc.html",
    "href": "posts/2020-12-08-share-github-repo-on-2-pc.html",
    "title": "Repo with 2 remote-urls",
    "section": "",
    "text": "from existing local repo connected to gitlab\nAdd the new remote url to github, name it github\n(base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git remote add github https://github.com/castorfou/data-scientist-skills.git\nOr this is possible to use ssh protocol: git remote add origin git@github.com:castorfou/data-scientist-skills.git\nPush repo to this new remote url: git push -u github\n(base) guillaume@LL11LPC0PQARQ:~/git/data-scientist-skills$ git push -u github\nUsername for 'https://github.com': castorfou\nPassword for 'https://castorfou@github.com':\nCounting objects: 4736, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (3171/3171), done.\nWriting objects: 100% (4736/4736), 630.97 MiB | 9.08 MiB/s, done.\nTotal 4736 (delta 1549), reused 4538 (delta 1458)\nremote: Resolving deltas: 100% (1549/1549), done.\nTo https://github.com/castorfou/data-scientist-skills.git\n * [new branch]      master -> master\nBranch 'master' set up to track remote branch 'master' from 'github'.\n\n\nfor new local repo\nClone the new repo: git clone git@github.com:castorfou/data-scientist-skills.git\nAnd I want to have same names for same remotes: git remote rename origin github\nSo now it is quite easy to update from different remote repo:\ncat refresh_from_github.sh \n#!/bin/bash\ngit fetch github\ngit pull"
  },
  {
    "objectID": "posts/2020-12-13-from-cron-to-anacron.html",
    "href": "posts/2020-12-13-from-cron-to-anacron.html",
    "title": "From cron to anacron",
    "section": "",
    "text": "I run generate_plots.sh daily at 9:30 AM. However what happens if my PC is off at that time, will have to wait another uptime at 9:30 AM.\nSolution is to move from cron to anacron.\nFrom https://www.putorius.net/cron-vs-anacron.html:\n\n\n\n\n\nalt text"
  },
  {
    "objectID": "posts/2020-12-13-from-cron-to-anacron.html#anacron-folders",
    "href": "posts/2020-12-13-from-cron-to-anacron.html#anacron-folders",
    "title": "From cron to anacron",
    "section": ".anacron folders",
    "text": ".anacron folders\nCreate a .anacron folder in your home directory and in it two subfolders, etc and spool\n\n!mkdir -p ~/.anacron/{etc,spool}"
  },
  {
    "objectID": "posts/2020-12-13-from-cron-to-anacron.html#anacrontab",
    "href": "posts/2020-12-13-from-cron-to-anacron.html#anacrontab",
    "title": "From cron to anacron",
    "section": "anacrontab",
    "text": "anacrontab\nCreate a new file ~/.anacron/etc/anacrontab with the following content:\n# ~/.anacron/etc/anacrontab: configuration file for anacron\n\n# See anacron(8) and anacrontab(5) for details.\n\nSHELL=/bin/bash\nPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/home/explore/miniconda3/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\n# period  delay  job-identifier  command\n1         10     squeezebox         ~/git/guillaume/squeezebox/generate_plots.sh"
  },
  {
    "objectID": "posts/2020-12-13-from-cron-to-anacron.html#start-anacron",
    "href": "posts/2020-12-13-from-cron-to-anacron.html#start-anacron",
    "title": "From cron to anacron",
    "section": "start anacron",
    "text": "start anacron\nAdd the following line to your crontab using crontab -e:\n@hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool\nAnd remove squeezebox entry from crontab.\n\n!crontab -l\n\n# NVIDIA SDK Manager updater\n# NVIDIA SDK Manager updater\n0 12 */7 * * /bin/bash /home/explore/.nvsdkm/.updater/updater.sh\n#30 9 * * * ~/git/guillaume/squeezebox/generate_plots.sh\n@hourly /usr/sbin/anacron -s -t $HOME/.anacron/etc/anacrontab -S $HOME/.anacron/spool"
  },
  {
    "objectID": "posts/2021-01-05-jupyter-export-notebook-as-py.html",
    "href": "posts/2021-01-05-jupyter-export-notebook-as-py.html",
    "title": "Auto export python code from jupyter notebooks",
    "section": "",
    "text": "jupyter_notebook_config.py\nHere is the code:\n# Based off of https://github.com/jupyter/notebook/blob/master/docs/source/extending/savehooks.rst\n\nimport io\nimport os\nfrom notebook.utils import to_api_path\n\n_script_exporter = None\n_html_exporter = None\n\ndef script_post_save(model, os_path, contents_manager, **kwargs):\n    \"\"\"convert notebooks to Python script after save with nbconvert\n\n    replaces `ipython notebook --script`\n    \"\"\"\n    from nbconvert.exporters.script import ScriptExporter\n    from nbconvert.exporters.html import HTMLExporter\n\n    if model['type'] != 'notebook':\n        return\n\n    global _script_exporter\n    if _script_exporter is None:\n        _script_exporter = ScriptExporter(parent=contents_manager)\n    log = contents_manager.log\n\n    global _html_exporter\n    if _html_exporter is None:\n        _html_exporter = HTMLExporter(parent=contents_manager)\n    log = contents_manager.log\n\n    # save .py file\n    base, ext = os.path.splitext(os_path)\n    script, resources = _script_exporter.from_filename(os_path)\n    # si le sous rep eports_py existe, on ecrit dedans, sinon on ecrit à la racine\n    sous_rep=''\n    repertoire=os.path.dirname(base)\n    if os.path.exists(repertoire+'/exports_py'):\n        sous_rep='/exports_py'\n    basename = os.path.basename(base)\n    script_fname = repertoire+ sous_rep+'/'+basename+resources.get('output_extension', '.txt')\n    log.info(\"base: {}, basename: {}, sous_rep: {}, repertoire: {}\".format(base, basename, sous_rep, repertoire))\n    log.info(\"script_fname: {}\".format(script_fname))\n    #script_fname = base + resources.get('output_extension', '.txt')\n    log.info(\"Saving script /%s\", to_api_path(script_fname, contents_manager.root_dir))\n    with io.open(script_fname, 'w', encoding='utf-8') as f:\n        f.write(script)\n\n\"\"\"\n    # save html\n    base, ext = os.path.splitext(os_path)\n    script, resources = _html_exporter.from_filename(os_path)\n    script_fname = base + resources.get('output_extension', '.txt')\n    log.info(\"Saving html /%s\", to_api_path(script_fname, contents_manager.root_dir))\n    with io.open(script_fname, 'w', encoding='utf-8') as f:\n        f.write(script)\n        \n\"\"\"\n\nc.FileContentsManager.post_save_hook = script_post_save\nIn this version, if a subfolder exports_py exists, .py version will be exported in it. Oherwise it will be exported in the notebook folder.\nMaybe in a later version it would be good to export only of this subfolder exists. (for example I don’t need these py files when creating such a blog entry, even if my .gitignore won’t publish .py files)\nAnd to remove the creation of Untitled.txt files when notebooks are just being created (and not yet named).\n\n\ndeployment\nJust save/merge this jupyter_notebook_config.py file (download) to your jupyter home directory.\nAccording to Config file and command line options in jupyter documentation, it is located at ~/.jupyter\nAnd in windows it is at C:\\Users\\<yourID>\\.jupyter\nThis will be valid for all your conda environments.\n\n\ntest\n~/.jupyter$ cp ~/git/guillaume/blog/files/jupyter_notebook_config.py .\nRestart Jupyter notebook server and click save on any notebook:\n\n\n\n\n\nvoila"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html",
    "href": "posts/2021-01-07-datacamp.html",
    "title": "About my datacamp learning process",
    "section": "",
    "text": "I started learning with Datacamp in March 2019. This is a great resource and I recommend all datascience newcomers to give it a shot.\nWhat I like are the consistent courses content. There is an overall logic between all courses. And content is just incredible: more than 300 interactive courses. OK maybe you won’t find all of them super useful but at least you can pick what is of interest for you. Following my learning process it takes me about 8 hours to complete a course.\n\n\n\nDatacamp courses\n\n\nCareer tracks are a smart way to help you build a 1st tour in your datascience journey. I followed python programmer (old version), data scientist with python (old version) and machine learning scientist with python tracks. Mileage may vary but it is about 20 courses per track. Updated versions of tracks are now online and this is a mix between courses, projects and skills assessments. I have tested one project but it is a little bit too basic for me.\n\n\n\nDatacamp Career tracks\n\n\nThere is a nice and smooth progress tracking system, and as in a game you earn XP for each achivement.\n\n\n\nDatacamp Progress"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#starting-a-project",
    "href": "posts/2021-01-07-datacamp.html#starting-a-project",
    "title": "About my datacamp learning process",
    "section": "Starting a project",
    "text": "Starting a project\nAs an example I will use\n\nwhich is a project from the new Data Scientist career track and which is in my ITP:"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#git-repo---data-scientist-skills",
    "href": "posts/2021-01-07-datacamp.html#git-repo---data-scientist-skills",
    "title": "About my datacamp learning process",
    "section": "Git repo - data-scientist-skills",
    "text": "Git repo - data-scientist-skills\nIn my data-scientist-skills github repo, I have 2 folders: * Other datacamp courses - where I keep lectures (pdf slides) from datacamp courses * python-sandbox - where I keep notebooks and data from datacamp exercises\n\ncreation of Data Manipulation with pandas folder under Other datacamp courses\ncreation of data-manipulation-with-pandas folder under python-sandbox\ncopy of python-sandbox/_1project-template/ into python-sandbox/data-manipulation-with-pandas"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#datacamp-project-template",
    "href": "posts/2021-01-07-datacamp.html#datacamp-project-template",
    "title": "About my datacamp learning process",
    "section": "Datacamp project template",
    "text": "Datacamp project template\nIn this project template, \n\ndata_from_datacamp will store all data needed to launch datacamp exercises\nexports_py will contain exports of notebooks in txt/py format (usefull to search on code patterns)\nstart_env.sh start_env.bat to launch jupyter notebook from the right conda env\ndownloadfromFileIO.py to download data files from my local notebooks (using in the background file.io)\nuploadfromdatacamp.py to upload data files from datacamp\nuploadfromdatacamp_examples.py some examples to transfer dataframes, dataseries, lists, …"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#projects-structure",
    "href": "posts/2021-01-07-datacamp.html#projects-structure",
    "title": "About my datacamp learning process",
    "section": "Projects structure",
    "text": "Projects structure\nAfter initialisation, I have the following structure and content:\n\nOn your left lectures (one per chapter) and final certificate.\nOn your right notebooks."
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#notebooks-for-exercises",
    "href": "posts/2021-01-07-datacamp.html#notebooks-for-exercises",
    "title": "About my datacamp learning process",
    "section": "Notebooks for exercises",
    "text": "Notebooks for exercises\nJust run the jupyter notebook environment by calling start_env.sh.\nGet the chapter title:\n\nAnd name the notebook accordingly:\n\nThen enter interactive instructions. I copy paste instructions using copy selection as markdown firefox add-on.\n\nHere in this example, if I want to follow instructions locally I need to have homelessness dataframe.\nI can use the following code from uploadfromdatacamp_examples.py\n\n###################\n##### Dataframe\n###################\n\n#upload and download\n\nfrom downloadfromFileIO import saveFromFileIO\n\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\nuploadToFileIO(homelessness)\n\"\"\"\n\ntobedownloaded=\"\"\"\n{pandas.core.frame.DataFrame: {'homelessness.csv': 'https://file.io/vTM1t2ehXds4'}}\n\"\"\"\nprefixToc='1.1'\nprefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n\n#initialisation\n\nimport pandas as pd\nhomelessness = pd.read_csv(prefix+'homelessness.csv',index_col=0)\n\nBefore executing this cell, I have to copy/paste/execute uploadfromdatacamp.py content on datacamp server. And call\nuploadToFileIO(homelessness)\nThen get the results last line\nIn [2]:\nuploadToFileIO(homelessness)\n \n{\"success\":true,\"key\":\"vTM1t2ehXds4\",\"link\":\"https://file.io/vTM1t2ehXds4\",\"expiry\":\"14 days\"}\n{pandas.core.frame.DataFrame: {'homelessness.csv': 'https://file.io/vTM1t2ehXds4'}}\nand copy it in tobedownloaded variable.\nUpdate prefixTOC to the good value (exercise 1.1 is the 1st one in first chapter) which is used as a prefix in data files. And update local variable name and csv file.\nRun the cell\nHere is the result\nTéléchargements à lancer\n{'pandas.core.frame.DataFrame': {'homelessness.csv': 'https://file.io/vTM1t2ehXds4'}}\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  2528    0  2528    0     0   4870      0 --:--:-- --:--:-- --:--:--  4870\nAnd homelessness is available to be used.\nFiles downloaded are in data_from_datacamp folder.\n\nAnd running again the cell won’t download file from file.io, but will read the cached file. (delete file to force download)\nFull content of this notebook example at the bottom"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#keep-content-in-git",
    "href": "posts/2021-01-07-datacamp.html#keep-content-in-git",
    "title": "About my datacamp learning process",
    "section": "keep content in git",
    "text": "keep content in git\n\n~/git/guillaume/data-scientist-skills$ git add .\n\n~/git/guillaume/data-scientist-skills$ git commit -m 'start of data manipulation in pandas course'\n[master c8696ce] start of data manipulation in pandas course\n 45 files changed, 9010 insertions(+)\n create mode 100644 Other datacamp courses/Data Manipulation with pandas/chapter1.pdf\n create mode 100644 python-sandbox/data-manipulation-with-pandas/.ipynb_checkpoints/chapter1 - Transforming Data-checkpoint.ipynb\n create mode 100644 python-sandbox/data-manipulation-with-pandas/__pycache__/downloadfromFileIO.cpython-37.pyc\n create mode 100644 python-sandbox/data-manipulation-with-pandas/chapter1 - Transforming Data.ipynb\n create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/.empty_dir.txt\n create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_3277903540843719836.lock\n create mode 100644 python-sandbox/data-manipulation-with-pandas/data_from_datacamp/chapter1 - Transforming Data-Exercise1.1_homelessness.csv\n create mode 100644 python-sandbox/data-manipulation-with-pandas/downloadfromFileIO.py\n create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/.empty_dir.txt\n create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.py\n create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/Untitled.txt\n create mode 100644 python-sandbox/data-manipulation-with-pandas/exports_py/chapter1 - Transforming Data.py\n create mode 100644 python-sandbox/data-manipulation-with-pandas/start_env.bat\n create mode 100755 python-sandbox/data-manipulation-with-pandas/start_env.sh\n create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp.py\n create mode 100644 python-sandbox/data-manipulation-with-pandas/uploadfromdatacamp_examples.py\n\n~/git/guillaume/data-scientist-skills$ git push\nEnumerating objects: 43, done.\nCounting objects: 100% (43/43), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (38/38), done.\nWriting objects: 100% (40/40), 5.75 MiB | 3.85 MiB/s, done.\nTotal 40 (delta 8), reused 1 (delta 0)\nremote: Resolving deltas: 100% (8/8), completed with 3 local objects.\nTo github.com:castorfou/data-scientist-skills.git\n   89f60e5..c8696ce  master -> master"
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#update-progress-in-itp",
    "href": "posts/2021-01-07-datacamp.html#update-progress-in-itp",
    "title": "About my datacamp learning process",
    "section": "Update progress in ITP",
    "text": "Update progress in ITP\nDatacamp is giving instant progress\n\nSo I regularly report this progress (here 0.18/4=5%) in ITP."
  },
  {
    "objectID": "posts/2021-01-07-datacamp.html#keep-certificates",
    "href": "posts/2021-01-07-datacamp.html#keep-certificates",
    "title": "About my datacamp learning process",
    "section": "keep certificates",
    "text": "keep certificates\nI download and keep certificates with lectures."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html",
    "href": "posts/2021-01-12-nbdev_tutorial.html",
    "title": "Hello nbdev",
    "section": "",
    "text": "Everything is under nbdev website.\n3 resources worth to be mentioning: * nbdev tutorial video on youtube; 1 year old but seems still valid * nbdev tutorial page * nbdev github repo\nWhat I plan to do is to watch the video part, and keep note of my progress in this blog entry."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#repo-creation",
    "href": "posts/2021-01-12-nbdev_tutorial.html#repo-creation",
    "title": "Hello nbdev",
    "section": "repo creation",
    "text": "repo creation\nAs suggested by Jeremy, I start by creating a github repo named hello_nbdev from a nbdev template.\nIt is just about clicking this link: https://github.com/fastai/nbdev_template/generate. If I am logged in github it will show the proper page."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#github-pages",
    "href": "posts/2021-01-12-nbdev_tutorial.html#github-pages",
    "title": "Hello nbdev",
    "section": "github pages",
    "text": "github pages\nDocumentation will be hosted at github (can be hosted anywhere but github seems a straightforward option) and to do that we have to setup github pages:\n\nSettings > Options > Github pages > Source > Master (branch) > /docs (folder) > Save\n\nAnd when done \nNow we can insert this doc url as our repo website setting:\n\nrepo home > <> code > about (edit repo details) > Website"
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#edit-settings.ini",
    "href": "posts/2021-01-12-nbdev_tutorial.html#edit-settings.ini",
    "title": "Hello nbdev",
    "section": "Edit settings.ini",
    "text": "Edit settings.ini\nEverything is in this file.\nJust edit directly from github.\nlib_name = nbdev_template\n# For Enterprise Git add variable repo_name and company name\n# repo_name = analytics\n# company_name = nike\n\nuser = fastai\n# description = A description of your project\n# keywords = some keywords\n# author = Your Name\n# author_email = email@example.com\n# copyright = Your Name or Company Name\nto\nlib_name = hello_nbdev\nuser = castorfou\ndescription = A tutorial walkthrough with nbdev\nkeywords = fastai nbdev tutorial\nauthor = Guillaume Ramelet\nauthor_email = guillaume.ramelet@gmail.com\ncopyright = Guillaume R.\nand commit changes"
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#clone-repo",
    "href": "posts/2021-01-12-nbdev_tutorial.html#clone-repo",
    "title": "Hello nbdev",
    "section": "Clone repo",
    "text": "Clone repo\n~/git/guillaume$ git clone git@github.com:castorfou/hello_nbdev.git\nCloning into 'hello_nbdev'...\nremote: Enumerating objects: 106, done.\nremote: Counting objects: 100% (106/106), done.\nremote: Compressing objects: 100% (94/94), done.\nremote: Total 106 (delta 7), reused 81 (delta 4), pack-reused 0\nReceiving objects: 100% (106/106), 1.02 MiB | 2.45 MiB/s, done.\nResolving deltas: 100% (7/7), done."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#setup-nbdev-python-environment",
    "href": "posts/2021-01-12-nbdev_tutorial.html#setup-nbdev-python-environment",
    "title": "Hello nbdev",
    "section": "Setup nbdev python environment",
    "text": "Setup nbdev python environment\nIt is not specifically mentionned in the video. For this walkthrough I will use my existing fastai environment.\n~/git/guillaume$ conda activate fastai\n~/git/guillaume$ nbdev_\nnbdev_build_docs         nbdev_diff_nbs           nbdev_test_nbs\nnbdev_build_lib          nbdev_fix_merge          nbdev_trust_nbs\nnbdev_bump_version       nbdev_install_git_hooks  nbdev_update_lib\nnbdev_clean_nbs          nbdev_nb2md              nbdev_upgrade\nnbdev_conda_package      nbdev_new                \nnbdev_detach             nbdev_read_nbs\nAs expected nbdev is already integrated in it.\nOtherwise my guess is that I have to run conda install -c fastai nbdev under my python env."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#install-git-hooks",
    "href": "posts/2021-01-12-nbdev_tutorial.html#install-git-hooks",
    "title": "Hello nbdev",
    "section": "Install git hooks",
    "text": "Install git hooks\n(fastai) ~/git/guillaume/hello_nbdev$ nbdev_install_git_hooks \nExecuting: git config --local include.path ../.gitconfig\nSuccess: hooks are installed and repo's .gitconfig is now trusted\n\ndeal with conflicts\nIf needed in case of conflict, Jeremy explains one can call nbdev_fix_merge filename.ipynb and it will use the standard conflict marker to help you identify and fix the conflict."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#open-00_core.ipynb",
    "href": "posts/2021-01-12-nbdev_tutorial.html#open-00_core.ipynb",
    "title": "Hello nbdev",
    "section": "Open 00_core.ipynb",
    "text": "Open 00_core.ipynb\n\ncreate lib (we start with a core module)\nJust following Jeremy’s instructions. * Create say_hello function * Use it (example) * Test it (assert)\n\n\nbuild_lib\nWe can call nbdev_build_lib from anywhere in the repo.\n(fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_lib \nConverted 00_core.ipynb.\nConverted index.ipynb.\nand it creates files, under hello_nbdev\nhello_nbdev$ ls hello_nbdev/\ncore.py  __init__.py  _nbdev.py  __pycache__\n\n\nModule Documentation\nThere are 2 levels of documentation. Documentation for your library that will be in index.ipynb and documentation for your modules that will be directly created from your code/notebooks 00_core.ipynb, etc\nAnd to generate this documentation it will be just a matter of calling nbdev_build_docs."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#library-documentation-into-index.ipynb",
    "href": "posts/2021-01-12-nbdev_tutorial.html#library-documentation-into-index.ipynb",
    "title": "Hello nbdev",
    "section": "Library documentation into index.ipynb",
    "text": "Library documentation into index.ipynb\n\ncreate doc\nDocumentation (what will be puclished) is in index.ipynb.\nThis is an actual documentation. Documentation won’t be written in markdown. It will be executed as code and rendered as such. How great is that.\nTo make it happen we have to import our lib just freshly generated.\nAnd now we can use all the part of our lib to explain how it works and why it is great.\n\nsay_hello(\"Guillaume\")\n\n'Hello Guillaume!'\n\n\n\n\nbuild_docs\nWe have to call nbdev_build_docs from our repo root.\n(fastai) ~/git/guillaume/hello_nbdev$ nbdev_build_docs \nconverting: /home/explore/git/guillaume/hello_nbdev/00_core.ipynb\nconverting /home/explore/git/guillaume/hello_nbdev/index.ipynb to README.md\n\n\ncommit to publish docs\nHere is the list of files to be pushed:\ngit status\n\nChanges to be committed:\n    modified:   00_core.ipynb\n    new file:   00_core.py\n    new file:   Makefile\n    modified:   README.md\n    new file:   docs/_config.yml\n    modified:   docs/_data/sidebars/home_sidebar.yml\n    new file:   docs/_data/topnav.yml\n    new file:   docs/core.html\n    new file:   docs/index.html\n    modified:   docs/sidebar.json\n    new file:   hello_nbdev/__init__.py\n    new file:   hello_nbdev/_nbdev.py\n    new file:   hello_nbdev/core.py\n    modified:   index.ipynb\n    new file:   index.py\n\ninit.py\nJust add from .core import * to __init__.py\n\n!cat /home/explore/git/guillaume/hello_nbdev/hello_nbdev/__init__.py\n\n__version__ = \"0.0.1\"\nfrom .core import *\n\n\nSo that we can easily use hello_nbdev without mentioning core module\n\n\ncommit and push\n(fastai) ~/git/guillaume/hello_nbdev$ git commit -m 'initial commit'\n[master 3484db7] initial commit\n 15 files changed, 520 insertions(+), 31 deletions(-)\n create mode 100644 00_core.py\n create mode 100644 Makefile\n create mode 100644 docs/_config.yml\n create mode 100644 docs/_data/topnav.yml\n create mode 100644 docs/core.html\n create mode 100644 docs/index.html\n create mode 100644 hello_nbdev/__init__.py\n create mode 100644 hello_nbdev/_nbdev.py\n create mode 100644 hello_nbdev/core.py\n create mode 100644 index.py\n(fastai) ~/git/guillaume/hello_nbdev$ git push\nEnumerating objects: 30, done.\nCounting objects: 100% (30/30), done.\nDelta compression using up to 12 threads\nCompressing objects: 100% (19/19), done.\nWriting objects: 100% (21/21), 4.87 KiB | 2.44 MiB/s, done.\nTotal 21 (delta 7), reused 0 (delta 0)\nremote: Resolving deltas: 100% (7/7), completed with 5 local objects.\nremote: \nremote: GitHub found 1 vulnerability on castorfou/hello_nbdev's default branch (1 low). To find out more, visit:\nremote:      https://github.com/castorfou/hello_nbdev/security/dependabot/docs/Gemfile.lock/nokogiri/open\nremote: \nTo github.com:castorfou/hello_nbdev.git\n   3aec9f4..3484db7  master -> master\n\n\n\nAnd documentation is ready\nhttps://castorfou.github.io/hello_nbdev/"
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#classes",
    "href": "posts/2021-01-12-nbdev_tutorial.html#classes",
    "title": "Hello nbdev",
    "section": "Classes",
    "text": "Classes\nFollowing tutorial, we can create class HelloSayer and document our methods by calling show_doc(HelloSayer.say).\nWe can decide to add entries into index.ipynb if this is something worth having at the library level."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#autoreload",
    "href": "posts/2021-01-12-nbdev_tutorial.html#autoreload",
    "title": "Hello nbdev",
    "section": "autoreload",
    "text": "autoreload\nBy adding these lines\n\n%load_ext autoreload\n%autoreload 2\n\nyour notebook automatically reads in the new modules as soon as the python file changes"
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#launch-nbdev-scripts-directly-from-jupyter",
    "href": "posts/2021-01-12-nbdev_tutorial.html#launch-nbdev-scripts-directly-from-jupyter",
    "title": "Hello nbdev",
    "section": "launch nbdev scripts directly from jupyter",
    "text": "launch nbdev scripts directly from jupyter\nMake it your last cell\n\nfrom nbdev.export import notebook2script; notebook2script()\n\nConverted 00_core.ipynb.\nConverted index.ipynb."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#run-tests-in-parallel",
    "href": "posts/2021-01-12-nbdev_tutorial.html#run-tests-in-parallel",
    "title": "Hello nbdev",
    "section": "run tests in parallel",
    "text": "run tests in parallel\nJust run nbdev_test_nbs\nIf your notebook starts with _, it will be excluded from the test list."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#installation-and-setup",
    "href": "posts/2021-01-12-nbdev_tutorial.html#installation-and-setup",
    "title": "Hello nbdev",
    "section": "Installation and setup",
    "text": "Installation and setup\nFrom https://jekyllrb.com/docs/installation/ubuntu/,\nsudo apt-get install ruby-full build-essential zlib1g-dev\nInstall variables to use gem:\necho '# Install Ruby Gems to ~/gems' >> ~/.bashrc\necho 'export GEM_HOME=\"$HOME/gems\"' >> ~/.bashrc\necho 'export PATH=\"$HOME/gems/bin:$PATH\"' >> ~/.bashrc\nsource ~/.bashrc\nInstall Jekyll and Builder:\ngem install jekyll bundler"
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#setup-our-lib-to-use-jekyll",
    "href": "posts/2021-01-12-nbdev_tutorial.html#setup-our-lib-to-use-jekyll",
    "title": "Hello nbdev",
    "section": "Setup our lib to use Jekyll",
    "text": "Setup our lib to use Jekyll\nFrom our docs folder, launch bundle install\n(fastai) ~/git/guillaume/hello_nbdev/docs$ bundle install\nFetching gem metadata from https://rubygems.org/.........\nUsing concurrent-ruby 1.1.7\n....\nBundle complete! 4 Gemfile dependencies, 90 gems now installed.\nUse `bundle info [gemname]` to see where a bundled gem is installed."
  },
  {
    "objectID": "posts/2021-01-12-nbdev_tutorial.html#use-it",
    "href": "posts/2021-01-12-nbdev_tutorial.html#use-it",
    "title": "Hello nbdev",
    "section": "Use it",
    "text": "Use it\nFrom repo root, launch make docs_serve\n(fastai) ~/git/guillaume/hello_nbdev$ make docs_serve\ncd docs && bundle exec jekyll serve\nConfiguration file: /home/explore/git/guillaume/hello_nbdev/docs/_config.yml\n            Source: /home/explore/git/guillaume/hello_nbdev/docs\n       Destination: /home/explore/git/guillaume/hello_nbdev/docs/_site\n Incremental build: disabled. Enable with --incremental\n      Generating... \n   GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data.\n                    done in 0.098 seconds.\n/home/explore/gems/gems/pathutil-0.16.2/lib/pathutil.rb:502: warning: Using the last argument as keyword parameters is deprecated\n Auto-regeneration: enabled for '/home/explore/git/guillaume/hello_nbdev/docs'\n    Server address: http://127.0.0.1:4000/hello_nbdev//\n  Server running... press ctrl-c to stop.\nIt is available locally at http://127.0.0.1:4000/hello_nbdev/"
  },
  {
    "objectID": "posts/2021-01-13-installing-python-packages-from-jupyter.html",
    "href": "posts/2021-01-13-installing-python-packages-from-jupyter.html",
    "title": "How To Install Packages from the Jupyter Notebook",
    "section": "",
    "text": "This is directly from https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/.\nHere are my own experimentations following this article detailed explanations."
  },
  {
    "objectID": "posts/2021-01-13-installing-python-packages-from-jupyter.html#how-your-operating-system-locates-executables",
    "href": "posts/2021-01-13-installing-python-packages-from-jupyter.html#how-your-operating-system-locates-executables",
    "title": "How To Install Packages from the Jupyter Notebook",
    "section": "How your operating system locates executables",
    "text": "How your operating system locates executables\n\n!echo $PATH\n\n/home/explore/gems/bin:/home/explore/miniconda3/envs/pytorch/bin:/home/explore/miniconda3/condabin:/home/explore/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n\n\n\n!type python\n\npython is /home/explore/miniconda3/envs/pytorch/bin/python\n\n\nYou can optionally add the -a tag to see all available versions of the command in your current shell environment; for example:\n\n!type -a python\n\npython is /home/explore/miniconda3/envs/pytorch/bin/python\npython is /usr/bin/python\n\n\n\n!type -a conda\n\nconda is /home/explore/miniconda3/condabin/conda"
  },
  {
    "objectID": "posts/2021-01-13-installing-python-packages-from-jupyter.html#how-python-locates-packages",
    "href": "posts/2021-01-13-installing-python-packages-from-jupyter.html#how-python-locates-packages",
    "title": "How To Install Packages from the Jupyter Notebook",
    "section": "How Python locates packages",
    "text": "How Python locates packages\n\nimport sys\nsys.path\n\n['/home/explore/git/guillaume/blog/_notebooks',\n '/home/explore/miniconda3/envs/pytorch/lib/python38.zip',\n '/home/explore/miniconda3/envs/pytorch/lib/python3.8',\n '/home/explore/miniconda3/envs/pytorch/lib/python3.8/lib-dynload',\n '',\n '/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages',\n '/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages/IPython/extensions',\n '/home/explore/.ipython']\n\n\nBy default, the first place Python looks for a module is an empty path, meaning the current working directory. If the module is not found there, it goes down the list of locations until the module is found. You can find out which location has been used using the __path__ attribute of an imported module:\n\nimport numpy\nnumpy.__path__\n\n['/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages/numpy']\n\n\nby printing the sys.path variables for each of the available python executables in my path, using Jupyter’s delightful ability to mix Python and bash commands in a single code block:\n\npaths = !type -a python\nfor path in set(paths):\n    path = path.split()[-1]\n    print(path)\n    !{path} -c \"import sys; print(sys.path)\"\n    print()\n\n/usr/bin/python\n['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/explore/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/local/lib/python2.7/dist-packages/PyCapture2-0.0.0-py2.7-linux-x86_64.egg', '/usr/lib/python2.7/dist-packages']\n\n/home/explore/miniconda3/envs/pytorch/bin/python\n['', '/home/explore/miniconda3/envs/pytorch/lib/python38.zip', '/home/explore/miniconda3/envs/pytorch/lib/python3.8', '/home/explore/miniconda3/envs/pytorch/lib/python3.8/lib-dynload', '/home/explore/miniconda3/envs/pytorch/lib/python3.8/site-packages']\n\n\n\npip install will install in the Python in the same path:\n\n!type pip\n\npip is /home/explore/miniconda3/envs/pytorch/bin/pip\n\n\nconda install will install in the active conda envt\n\n!conda env list\n\n# conda environments:\n#\nbase                     /home/explore/miniconda3\nd059                     /home/explore/miniconda3/envs/d059\ndatacamp                 /home/explore/miniconda3/envs/datacamp\ndeeplearning_specialization     /home/explore/miniconda3/envs/deeplearning_specialization\ndeeplearning_specialization_keras     /home/explore/miniconda3/envs/deeplearning_specialization_keras\ndeeplearning_specialization_tf1     /home/explore/miniconda3/envs/deeplearning_specialization_tf1\ndrl_handson              /home/explore/miniconda3/envs/drl_handson\nfastai                   /home/explore/miniconda3/envs/fastai\ngan                      /home/explore/miniconda3/envs/gan\ngan_tensorflow           /home/explore/miniconda3/envs/gan_tensorflow\nmit_6002x                /home/explore/miniconda3/envs/mit_6002x\npytorch               *  /home/explore/miniconda3/envs/pytorch\nsqueezebox               /home/explore/miniconda3/envs/squeezebox\n\n\n\nThe reason both pip and conda default to the conda pytorch environment is that this is the Python environment I used to launch the notebook."
  },
  {
    "objectID": "posts/2021-01-13-installing-python-packages-from-jupyter.html#how-jupyter-executes-code-jupyter-kernels",
    "href": "posts/2021-01-13-installing-python-packages-from-jupyter.html#how-jupyter-executes-code-jupyter-kernels",
    "title": "How To Install Packages from the Jupyter Notebook",
    "section": "How Jupyter executes code: Jupyter Kernels",
    "text": "How Jupyter executes code: Jupyter Kernels\n\n!jupyter kernelspec list\n\nAvailable kernels:\n  python2    /home/explore/.local/share/jupyter/kernels/python2\n  python3    /home/explore/miniconda3/envs/pytorch/share/jupyter/kernels/python3\n\n\n\n!cat /home/explore/miniconda3/envs/pytorch/share/jupyter/kernels/python3/kernel.json\n\n{\n \"argv\": [\n  \"/home/explore/miniconda3/envs/pytorch/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Python 3\",\n \"language\": \"python\"\n}\n\n\nIf you’d like to create a new kernel, you can do so using the jupyter ipykernel command; for example, I created the above kernels for my primary conda environments using the following as a template:\n$ source activate myenv\n$ python -m ipykernel install --user --name myenv --display-name \"Python (myenv)\""
  },
  {
    "objectID": "posts/2021-01-13-matplotlib-cheatsheet.html",
    "href": "posts/2021-01-13-matplotlib-cheatsheet.html",
    "title": "matplotlib cheatsheet",
    "section": "",
    "text": "matplotlib cheatsheet in pdf\npdf lecture in github\n\n\n\n\n# Introducing the pyplot interface\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nplt.show()\n\n# Adding data to axes\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-TAVG-NORMAL\"])\nplt.show()\n\n\n\n\n# Adding markers\nax.plot(seattle_weather[\"MONTH\"],\nseattle_weather[\"MLY-PRCP-NORMAL\"],\nmarker=\"o\")\nplt.show()\n\n# Choosing markers\nax.plot(seattle_weather[\"MONTH\"],\nseattle_weather[\"MLY-PRCP-NORMAL\"],\nmarker=\"v\")\nplt.show()\nmarkers\n# Setting the linestyle\nfig, ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"],\nseattle_weather[\"MLY-TAVG-NORMAL\"],\nmarker=\"v\", linestyle=\"--\")\nplt.show()\nline style\n# Eliminating lines with linestyle\nfig, ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"],\nseattle_weather[\"MLY-TAVG-NORMAL\"],\nmarker=\"v\", linestyle=\"None\")\nplt.show()\n\n# Choosing color\nfig, ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"],\nseattle_weather[\"MLY-TAVG-NORMAL\"],\nmarker=\"v\", linestyle=\"--\", color=\"r\")\nplt.show()\n\n# Customizing the axes labels\nax.set_xlabel(\"Time (months)\")\nplt.show()\n\n# Setting the y axis label\nax.set_xlabel(\"Time (months)\")\nax.set_ylabel(\"Average temperature (Fahrenheit degrees)\")\nplt.show()\n\n# Adding a title\nax.set_title(\"Weather in Seattle\")\nplt.show()\n\n\n\n# Small multiples with plt.subplots\nfig, ax = plt.subplots(3, 2)\nplt.show()\n\n# Adding data to subplots\nax.shape\n(3, 2)\nax[0, 0].plot(seattle_weather[\"MONTH\"],seattle_weather[\"MLY-PRCP-NORMAL\"],color='b')\nplt.show()\n\n# Subplots with data\nfig, ax = plt.subplots(2, 1)\nax[0].plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-PRCP-NORMAL\"],color='b')\nax[0].plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-PRCP-25PCTL\"],linestyle='--', color='b')\nax[0].plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-PRCP-75PCTL\"],linestyle='--', color='b')\nax[1].plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-PRCP-NORMAL\"],color='r')\nax[1].plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-PRCP-25PCTL\"],linestyle='--', color='r')\nax[1].plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-PRCP-75PCTL\"],linestyle='--', color='r')\nax[0].set_ylabel(\"Precipitation (inches)\")\nax[1].set_ylabel(\"Precipitation (inches)\")\nax[1].set_xlabel(\"Time (months)\")\nplt.show()\n\n# Sharing the y-axis range\nfig, ax = plt.subplots(2, 1, sharey=True)\n\n\n\n\n\n\n\n# DateTimeIndex\nclimate_change.index\nDatetimeIndex(['1958-03-06', '1958-04-06', '1958-05-06', '1958-06-06',\n     dtype='datetime64[ns]', name='date', length=706, freq=None)\n               \n               \n# Plotting time-series data\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot(climate_change.index, climate_change['co2'])\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm)')\nplt.show()\n\n# Zooming in on a decade\nsixties = climate_change[\"1960-01-01\":\"1969-12-31\"]\nfig, ax = plt.subplots()\nax.plot(sixties.index, sixties['co2'])\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm)')\nplt.show()\n\n# Zooming in on one year\nsixty_nine = climate_change[\"1969-01-01\":\"1969-12-31\"]\nfig, ax = plt.subplots()\nax.plot(sixty_nine.index, sixty_nine['co2'])\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm)')\nplt.show()\n\n\n\n# Plotting two time-series together\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nax.plot(climate_change.index, climate_change[\"co2\"])\nax.plot(climate_change.index, climate_change[\"relative_temp\"])\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm) / Relative temperature')\nplt.show()\n\n# Using twin axes\nfig, ax = plt.subplots()\nax.plot(climate_change.index, climate_change[\"co2\"])\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm)')\nax2 = ax.twinx()\nax2.plot(climate_change.index, climate_change[\"relative_temp\"])\nax2.set_ylabel('Relative temperature (Celsius)')\nplt.show()\n\n# Separating variables by color\nfig, ax = plt.subplots()\nax.plot(climate_change.index, climate_change[\"co2\"], color='blue')\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm)', color='blue')\nax2 = ax.twinx()\nax2.plot(climate_change.index, climate_change[\"relative_temp\"],\ncolor='red')\nax2.set_ylabel('Relative temperature (Celsius)', color='red')\nplt.show()\n\n# Coloring the ticks\nfig, ax = plt.subplots()\nax.plot(climate_change.index, climate_change[\"co2\"],\ncolor='blue')\nax.set_xlabel('Time')\nax.set_ylabel('CO2 (ppm)', color='blue')\nax.tick_params('y', colors='blue')\nax2 = ax.twinx()\nax2.plot(climate_change.index,\nclimate_change[\"relative_temp\"],\ncolor='red')\nax2.set_ylabel('Relative temperature (Celsius)',\ncolor='red')\nax2.tick_params('y', colors='red')\nplt.show()\n\n# A function that plots time-series\ndef plot_timeseries(axes, x, y, color, xlabel, ylabel):\n    axes.plot(x, y, color=color)\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel, color=color)\n    axes.tick_params('y', colors=color)\n# Using our function\nfig, ax = plt.subplots()\nplot_timeseries(ax, climate_change.index, climate_change['co2'],'blue', 'Time', 'CO2 (ppm)')\nax2 = ax.twinx()\nplot_timeseries(ax, climate_change.index,climate_change['relative_temp'],'red', 'Time', 'Relative temperature (Celsius)')\nplt.show()\n\n\n\n# Annotation\nfig, ax = plt.subplots()\nplot_timeseries(ax, climate_change.index, climate_change['co2'],\n'blue', 'Time', 'CO2 (ppm)')\nax2 = ax.twinx()\nplot_timeseries(ax2, climate_change.index,\nclimate_change['relative_temp'],\n'red', 'Time', 'Relative temperature (Celsius)')\nax2.annotate(\">1 degree\",\nxy=[pd.TimeStamp(\"2015-10-06\"), 1])\nplt.show()\n\n# Positioning the text\nax2.annotate(\">1 degree\",\nxy=(pd.Timestamp('2015-10-06'), 1),\nxytext=(pd.Timestamp('2008-10-06'), -0.2))\n\n# Adding arrows to annotation\nax2.annotate(\">1 degree\",\nxy=(pd.Timestamp('2015-10-06'), 1),\nxytext=(pd.Timestamp('2008-10-06'), -0.2),\narrowprops={})\n\n# Customizing arrow properties\nax2.annotate(\">1 degree\",\nxy=(pd.Timestamp('2015-10-06'), 1),\nxytext=(pd.Timestamp('2008-10-06'), -0.2),\narrowprops={\"arrowstyle\":\"->\", \"color\":\"gray\"})\nCustomizing annotations\n\n\n\n\n\n\n\n# Olympic medals: visualizing the data\nmedals = pd.read_csv('medals_by_country_2016.csv', index_col=0)\nfig, ax = plt.subplots()\nax.bar(medals.index, medals[\"Gold\"])\nplt.show()\n\n# Interlude: rotate the tick labels\nfig, ax = plt.subplots()\nax.bar(medals.index, medals[\"Gold\"])\nax.set_xticklabels(medals.index, rotation=90)\nax.set_ylabel(\"Number of medals\")\nplt.show()\n\n# Olympic medals: visualizing the other medals : stacked bar chart\nfig, ax = plt.subplots\nax.bar(medals.index, medals[\"Gold\"])\nax.bar(medals.index, medals[\"Silver\"], bottom=medals[\"Gold\"])\nax.set_xticklabels(medals.index, rotation=90)\nax.set_ylabel(\"Number of medals\")\nplt.show()\n\n# Olympic medals: visualizing all three\nfig, ax = plt.subplots\nax.bar(medals.index, medals[\"Gold\"])\nax.bar(medals.index, medals[\"Silver\"], bottom=medals[\"Gold\"])\nax.bar(medals.index, medals[\"Bronze\"],\nbottom=medals[\"Gold\"] + medals[\"Silver\"])\nax.set_xticklabels(medals.index, rotation=90)\nax.set_ylabel(\"Number of medals\")\nplt.show()\n\n# Adding a legend\nfig, ax = plt.subplots\nax.bar(medals.index, medals[\"Gold\"], label=\"Gold\")\nax.bar(medals.index, medals[\"Silver\"], bottom=medals[\"Gold\"],\nlabel=\"Silver\")\nax.bar(medals.index, medals[\"Bronze\"],\nbottom=medals[\"Gold\"] + medals[\"Silver\"],\nlabel=\"Bronze\")\nax.set_xticklabels(medals.index, rotation=90)\nax.set_ylabel(\"Number of medals\")\nax.legend()\nplt.show()\n\n\n\n\n# Introducing histograms\nfig, ax = plt.subplots()\nax.hist(mens_rowing[\"Height\"])\nax.hist(mens_gymnastic[\"Height\"])\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"# of observations\")\nplt.show()\n\n# Labels are needed\nax.hist(mens_rowing[\"Height\"], label=\"Rowing\")\nax.hist(mens_gymnastic[\"Height\"], label=\"Gymnastics\")\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"# of observations\")\nax.legend()\nplt.show()\n\n# Customizing histograms: setting the number of bins\nax.hist(mens_rowing[\"Height\"], label=\"Rowing\", bins=5)\nax.hist(mens_gymnastic[\"Height\"], label=\"Gymnastics\", bins=5)\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"# of observations\")\nax.legend()\nplt.show()\n\n# Customizing histograms: setting bin boundaries\nax.hist(mens_rowing[\"Height\"], label=\"Rowing\",\nbins=[150, 160, 170, 180, 190, 200, 210])\nax.hist(mens_gymnastic[\"Height\"], label=\"Gymnastics\",\nbins=[150, 160, 170, 180, 190, 200, 210])\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"# of observations\")\nax.legend()\nplt.show()\n\n# Customizing histograms: transparency\nax.hist(mens_rowing[\"Height\"], label=\"Rowing\",\nbins=[150, 160, 170, 180, 190, 200, 210],\nhisttype=\"step\")\nax.hist(mens_gymnastic[\"Height\"], label=\"Gymnastics\",\nbins=[150, 160, 170, 180, 190, 200, 210],\nhisttype=\"step\")\nax.set_xlabel(\"Height (cm)\")\nax.set_ylabel(\"# of observations\")\nax.legend()\nplt.show()\n\n\n\n# Adding error bars to bar charts\nfig, ax = plt.subplots()\nax.bar(\"Rowing\",mens_rowing[\"Height\"].mean(),\nyerr=mens_rowing[\"Height\"].std())\nax.bar(\"Gymnastics\",mens_gymnastics[\"Height\"].mean(),\nyerr=mens_gymnastics[\"Height\"].std())\nax.set_ylabel(\"Height (cm)\")\nplt.show()\n\n# Adding error bars to plots\nfig, ax = plt.subplots()\nax.errorbar(seattle_weather[\"MONTH\"],\nseattle_weather[\"MLY-TAVG-NORMAL\"],\nyerr=seattle_weather[\"MLY-TAVG-STDDEV\"])\n\nax.errorbar(austin_weather[\"MONTH\"],\naustin_weather[\"MLY-TAVG-NORMAL\"],\nyerr=austin_weather[\"MLY-TAVG-STDDEV\"])\n\nax.set_ylabel(\"Temperature (Fahrenheit)\")\nplt.show()\n\n# Adding boxplots\nfig, ax = plt.subplots()\nax.boxplot([mens_rowing[\"Height\"],\nmens_gymnastics[\"Height\"]])\nax.set_xticklabels([\"Rowing\", \"Gymnastics\"])\nax.set_ylabel(\"Height (cm)\")\nplt.show()\n\n\n\n\n# Introducing scatter plots\nfig, ax = plt.subplots()\nax.scatter(climate_change[\"co2\"], climate_change[\"relative_temp\"])\nax.set_xlabel(\"CO2 (ppm)\")\nax.set_ylabel(\"Relative temperature (Celsius)\")\nplt.show()\n\n# Customizing scatter plots\neighties = climate_change[\"1980-01-01\":\"1989-12-31\"]\nnineties = climate_change[\"1990-01-01\":\"1999-12-31\"]\nfig, ax = plt.subplots()\nax.scatter(eighties[\"co2\"], eighties[\"relative_temp\"],\ncolor=\"red\", label=\"eighties\")\nax.scatter(nineties[\"co2\"], nineties[\"relative_temp\"],\ncolor=\"blue\", label=\"nineties\")\nax.legend()\nax.set_xlabel(\"CO2 (ppm)\")\nax.set_ylabel(\"Relative temperature (Celsius)\")\nplt.show()\n\n# Encoding a third variable by color\nfig, ax = plt.subplots()\nax.scatter(climate_change[\"co2\"], climate_change[\"relative_temp\"],\nc=climate_change.index)\nax.set_xlabel(\"CO2 (ppm)\")\nax.set_ylabel(\"Relative temperature (Celsius)\")\nplt.show()\n\n\n\n\n\n\n\n# Choosing a style\nplt.style.use(\"ggplot\")\nfig, ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-TAVG-NORMAL\"\nax.plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-TAVG-NORMAL\"])\nax.set_xlabel(\"Time (months)\")\nax.set_ylabel(\"Average temperature (Fahrenheit degrees)\")\nplt.show()\n                                                  \n# Back to the default\nplt.style.use(\"default\")                                                  \navailable styles\n# The \"bmh\" style\nplt.style.use(\"bmh\")\nfig, ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-TAVG-NORMAL\"\nax.plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-TAVG-NORMAL\"])\nax.set_xlabel(\"Time (months)\")\nax.set_ylabel(\"Average temperature (Fahrenheit degrees)\")\nplt.show()\n                                                  \n# Seaborn styles\nplt.style.use(\"seaborn-colorblind\")\nfig, ax = plt.subplots()\nax.plot(seattle_weather[\"MONTH\"], seattle_weather[\"MLY-TAVG-NORMAL\"\nax.plot(austin_weather[\"MONTH\"], austin_weather[\"MLY-TAVG-NORMAL\"])\nax.set_xlabel(\"Time (months)\")\nax.set_ylabel(\"Average temperature (Fahrenheit degrees)\")\nplt.show()                                                  \n\n\n\n\n# Saving the figure to file\nfig, ax = plt.subplots()\nax.bar(medals.index, medals[\"Gold\"])\nax.set_xticklabels(medals.index, rotation=90)\nax.set_ylabel(\"Number of medals\")\nfig.savefig(\"gold_medals.png\")\n\n# Different file formats\nfig.savefig(\"gold_medals.jpg\")\nfig.savefig(\"gold_medals.jpg\", quality=50)\nfig.savefig(\"gold_medals.svg\")\n\n# Resolution\nfig.savefig(\"gold_medals.png\", dpi=300)\n\n# Size\nfig.set_size_inches([5, 3])\n\n# Another aspect ratio\nfig.set_size_inches([3, 5])\n\n\n\n\n# Getting unique values of a column\nsports = summer_2016_medals[\"Sport\"].unique()\n\n# Bar-chart of heights for all sports\nfig, ax = plt.subplots()\nfor sport in sports:\nsport_df = summer_2016_medals[summer_2016_medals[\"Sport\"] == spor\nax.bar(sport, sport_df[\"Height\"].mean(),\nyerr=sport_df[\"Height\"].std())\nax.set_ylabel(\"Height (cm)\")\nax.set_xticklabels(sports, rotation=90)\nplt.show()"
  },
  {
    "objectID": "posts/2021-01-13-pandas-cheatsheet.html",
    "href": "posts/2021-01-13-pandas-cheatsheet.html",
    "title": "pandas cheatsheet",
    "section": "",
    "text": "# Exploring a DataFrame: .head()\ndogs.head()\n\n# Exploring a DataFrame: .info()\ndogs.info()\n\n# Exploring a DataFrame: .shape\ndogs.shape\n\n# Exploring a DataFrame: .describe()\ndogs.describe()\n\n# Components of a DataFrame: .values\ndogs.values\n\n# Components of a DataFrame: .columns and .index\ndogs.columns\n\ndogs.index\n\n\n\n#Sorting by multiple variables\ndogs.sort_values([\"weight_kg\", \"height_cm\"], ascending=[True, False])\n\n#Subsetting based on dates\ndogs[dogs[\"date_of_birth\"] > \"2015-01-01\"]\n\n#Subsetting based on multiple conditions\nis_lab = dogs[\"breed\"] == \"Labrador\"\nis_brown = dogs[\"color\"] == \"Brown\"\ndogs[is_lab & is_brown]\ndogs[ (dogs[\"breed\"] == \"Labrador\") & (dogs[\"color\"] == \"Brown\") ]\n\n#Subsetting using .isin()\nis_black_or_brown = dogs[\"color\"].isin([\"Black\", \"Brown\"])\ndogs[is_black_or_brown]\n\n\n\n# Adding a new column\ndogs[\"height_m\"] = dogs[\"height_cm\"] / 100\n\n\n\n\n\n\n#Summarizing numerical data\ndogs[\"height_cm\"].mean()\n\n.median() , .mode()\n.min() , .max()\n.var() , .std()\n.sum()\n.quantile()\n\n#The .agg() method\ndef pct30(column):\nreturn column.quantile(0.3)\ndogs[\"weight_kg\"].agg(pct30)\n\n#Multiple summaries\ndef pct40(column):\nreturn column.quantile(0.4)\ndogs[\"weight_kg\"].agg([pct30, pct40])\n\n#Cumulative sum\ndogs[\"weight_kg\"].cumsum()\n\n#Cumulative statistics\n.cummax()\n.cummin()\n.cumprod()\n\n\n\n#Dropping duplicate names\nvet_visits.drop_duplicates(subset=\"name\")\n\n#Dropping duplicate pairs\nunique_dogs = vet_visits.drop_duplicates(subset=[\"name\", \"breed\"])\n\n#Counting\nunique_dogs[\"breed\"].value_counts(sort=True)\n\n\n\n\n#Summaries by group\ndogs[dogs[\"color\"] == \"Black\"][\"weight_kg\"].mean()\ndogs[dogs[\"color\"] == \"Brown\"][\"weight_kg\"].mean()\n\n#Grouped summaries\ndogs.groupby(\"color\")[\"weight_kg\"].mean()\n\n#Multiple grouped summaries\ndogs.groupby(\"color\")[\"weight_kg\"].agg([min, max, sum])\n\n#Grouping by multiple variables\ndogs.groupby([\"color\", \"breed\"])[\"weight_kg\"].mean()\n\n#Many groups, many summaries\ndogs.groupby([\"color\", \"breed\"])[[\"weight_kg\", \"height_cm\"]].mean()\n\n\n\n\n#pivot table\ndogs.pivot_table(values=\"weight_kg\",index=\"color\")\n\n#Different statistics\nimport numpy as np\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", aggfunc=np.median)\n\n#Multiple statistics\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", aggfunc=[np.mean, np.median])\n\n#Pivot on two variables\ndogs.groupby([\"color\", \"breed\"])[\"weight_kg\"].mean()\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", columns=\"breed\")\n\n#Filling missing values in pivot tables\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", columns=\"breed\", fill_value=0)\n\n# Summing with pivot tables\ndogs.pivot_table(values=\"weight_kg\", index=\"color\", columns=\"breed\",\nfill_value=0, margins=True)\n\n\n\n\n\n\n\n# Setting a column as the index\ndogs_ind = dogs.set_index(\"name\")\n\n# Removing an index\ndogs_ind.reset_index()\n\n# Dropping an index\ndogs_ind.reset_index(drop=True)\n\n# Indexes make subsetting simpler\ndogs[dogs[\"name\"].isin([\"Bella\", \"Stella\"])]\n# versus\ndogs_ind.loc[[\"Bella\", \"Stella\"]]\n\n# Multi-level indexes a.k.a. hierarchical indexes\ndogs_ind3 = dogs.set_index([\"breed\", \"color\"])\n\n# Subset the outer level with a list\ndogs_ind3.loc[[\"Labrador\", \"Chihuahua\"]]\n\n# Subset inner levels with a list of tuples\ndogs_ind3.loc[[(\"Labrador\", \"Brown\"), (\"Chihuahua\", \"Tan\")]]\n\n# Sorting by index values\ndogs_ind3.sort_index()\n\n# Controlling sort_index\ndogs_ind3.sort_index(level=[\"color\", \"breed\"], ascending=[True, False])\n\n\n\n\n# Sort the index before you slice\ndogs_srt = dogs.set_index([\"breed\", \"color\"]).sort_index()\n\n# Slicing the outer index level\ndogs_srt.loc[\"Chow Chow\":\"Poodle\"]\n\n# Slicing the inner index levels correctly\ndogs_srt.loc[(\"Labrador\", \"Brown\"):(\"Schnauzer\", \"Grey\")]\n\n# Slicing columns\ndogs_srt.loc[:, \"name\":\"height_cm\"]\n\n# Slice twice\ndogs_srt.loc[\n(\"Labrador\", \"Brown\"):(\"Schnauzer\", \"Grey\"),\n\"name\":\"height_cm\"]\n\n# Dog days\ndogs = dogs.set_index(\"date_of_birth\").sort_index()\n\n# Slicing by dates\n# Get dogs with date_of_birth between 2014-08-25 and 2016-09-16\ndogs.loc[\"2014-08-25\":\"2016-09-16\"]\n\n# Slicing by partial dates\n# Get dogs with date_of_birth between 2014-01-01 and 2016-12-31\ndogs.loc[\"2014\":\"2016\"]\n\n# Subsetting by row/column number\nprint(dogs.iloc[2:5, 1:4])\n\n\n\n\n# Pivoting the dog pack\ndogs_height_by_breed_vs_color = dog_pack.pivot_table(\n\"height_cm\", index=\"breed\", columns=\"color\")\n\n# The axis argument\ndogs_height_by_breed_vs_color.mean(axis=\"index\")\n\n# Calculating summary stats across columns\ndogs_height_by_breed_vs_color.mean(axis=\"columns\")\n\n\n\n\n\n\n\n# Histograms\nimport matplotlib.pyplot as plt\ndog_pack[\"height_cm\"].hist(bins=20)\n\n# Bar plots\navg_weight_by_breed = dog_pack.groupby(\"breed\")[\"weight_kg\"].mean()\navg_weight_by_breed.plot(kind=\"bar\", title=\"Mean Weight by Dog Breed\")\n\n# Line plots\nsully.head()\nsully.plot(x=\"date\", y=\"weight_kg\", kind=\"line\")\n\n# Rotating axis labels\nsully.plot(x=\"date\", y=\"weight_kg\", kind=\"line\", rot=45)\n\n# Scatter plots\ndog_pack.plot(x=\"height_cm\", y=\"weight_kg\", kind=\"scatter\")\n\n# Layering plots\ndog_pack[dog_pack[\"sex\"]==\"F\"][\"height_cm\"].hist()\ndog_pack[dog_pack[\"sex\"]==\"M\"][\"height_cm\"].hist()\n\n# Add a legend\nplt.legend([\"F\", \"M\"])\n\n# Transparency\ndog_pack[dog_pack[\"sex\"]==\"F\"][\"height_cm\"].hist(alpha=0.7)\ndog_pack[dog_pack[\"sex\"]==\"M\"][\"height_cm\"].hist(alpha=0.7)\nplt.legend([\"F\", \"M\"])\n\n\n\n\n# Detecting missing values\ndogs.isna()\n\n# Detecting any missing values\ndogs.isna().any()\n\n# Counting missing values\ndogs.isna().sum()\n\n# Plotting missing values\nimport matplotlib.pyplot as plt\ndogs.isna().sum().plot(kind=\"bar\")\nplt.show()\n\n# Removing rows containing missing values\ndogs.dropna()\n\n# Replacing missing values\ndogs.fillna(0)\n\n\n\n\n# CSV to DataFrame\nimport pandas as pd\nnew_dogs = pd.read_csv(\"new_dogs.csv\")\n\n# DataFrame to CSV\nnew_dogs.to_csv(\"new_dogs_with_bmi.csv\")\n\n# CSV to dataframe parsing dates, and having date as index\nclimate_change = pd.read_csv(prefix+'climate_change.csv', parse_dates=['date'], index_col='date')"
  },
  {
    "objectID": "posts/2021-01-13-pandas-cheatsheet.html#data-merging-basics",
    "href": "posts/2021-01-13-pandas-cheatsheet.html#data-merging-basics",
    "title": "pandas cheatsheet",
    "section": "Data merging basics",
    "text": "Data merging basics\n\nInner join\n# Inner join\nwards_census = wards.merge(census, on='ward')\n\n# Suffixes\nwards_census = wards.merge(census, on='ward', suffixes=('_ward','_cen'))\n\n\nOne-to-many relationships\n\n# One-to-many example\nward_licenses = wards.merge(licenses, on='ward', suffixes=('_ward','_lic'))\n\n\nMerging multiple DataFrames\n# Single merge\ngrants.merge(licenses, on=['address','zip'])\n\n# Merging multiple tables\ngrants_licenses_ward = grants.merge(licenses, on=['address','zip']) \\\n.merge(wards, on='ward', suffixes=('_bus','_ward'))\n\n\n# Plot Results\nimport matplotlib.pyplot as plt\ngrant_licenses_ward.groupby('ward').agg('sum').plot(kind='bar', y='grant')"
  },
  {
    "objectID": "posts/2021-01-13-pandas-cheatsheet.html#merging-tables-with-different-join-types",
    "href": "posts/2021-01-13-pandas-cheatsheet.html#merging-tables-with-different-join-types",
    "title": "pandas cheatsheet",
    "section": "Merging Tables With Different Join Types",
    "text": "Merging Tables With Different Join Types\n\nLeft join\n\n# Merge with left join\nmovies_taglines = movies.merge(taglines, on='id', how='left')\n\n\nOther joins\n\n# Merge with right join\ntv_movies = movies.merge(tv_genre, how='right',\nleft_on='id', right_on='movie_id')\n\n# Merge with outer join\nfamily_comedy = family.merge(comedy, on='movie_id', how='outer',\nsuffixes=('_fam', '_com'))\n\n\nMerging a table to itself\n\n# Merging a table to itself\noriginal_sequels = sequels.merge(sequels, left_on='sequel', right_on='id',\nsuffixes=('_org','_seq'))\n\n\nMerging on indexes\n\n# Setting an index\nmovies = pd.read_csv('tmdb_movies.csv', index_col=['id'])\n\n# Merging on index\nmovies_taglines = movies.merge(taglines, on='id', how='left')\n\n# MultiIndex merge\nsamuel_casts = samuel.merge(casts, on=['movie_id','cast_id'])\n\n# Index merge with left_on and right_on\nmovies_genres = movies.merge(movie_to_genres, left_on='id', left_index=True,\nright_on='movie_id', right_index=True)"
  },
  {
    "objectID": "posts/2021-01-13-pandas-cheatsheet.html#advanced-merging-and-concatenating",
    "href": "posts/2021-01-13-pandas-cheatsheet.html#advanced-merging-and-concatenating",
    "title": "pandas cheatsheet",
    "section": "Advanced Merging and Concatenating",
    "text": "Advanced Merging and Concatenating\n\nFiltering joins\n\n###########\n# semi-join\n\n# Step 1 - semi-join\ngenres_tracks = genres.merge(top_tracks, on='gid')\n\n# Step 2 - semi-join\ngenres['gid'].isin(genres_tracks['gid'])\n\n# Step 3 - semi-join\ngenres_tracks = genres.merge(top_tracks, on='gid')\ntop_genres = genres[genres['gid'].isin(genres_tracks['gid'])]\n\n###########\n# anti-join\n\n# Step 1 - anti-join\ngenres_tracks = genres.merge(top_tracks, on='gid', how='left', indicator=True)\n\n# Step 2 - anti-join\ngid_list = genres_tracks.loc[genres_tracks['_merge'] == 'left_only', 'gid']\n\n# Step 3 - anti-join\ngenres_tracks = genres.merge(top_tracks, on='gid', how='left', indicator=True)\ngid_list = genres_tracks.loc[genres_tracks['_merge'] == 'left_only','gid']\nnon_top_genres = genres[genres['gid'].isin(gid_list)]\n\n\nConcatenate DataFrames together vertically\n# Basic concatenation\npd.concat([inv_jan, inv_feb, inv_mar])\n\n# Ignoring the index\npd.concat([inv_jan, inv_feb, inv_mar],\nignore_index=True)\n\n# Setting labels to original tables\npd.concat([inv_jan, inv_feb, inv_mar],\nignore_index=False,\nkeys=['jan','feb','mar'])\n\n# Concatenate tables with different column names\npd.concat([inv_jan, inv_feb],\nsort=True)\n\n# Concatenate tables with different column names\npd.concat([inv_jan, inv_feb],\njoin='inner')\n\n# Append the tables\ninv_jan.append([inv_feb, inv_mar],\nignore_index=True, \nsort=True)\n\n\nVerifying integrity\n\n# Validating merges\n.merge(validate=None) :\nChecks if merge is of specified type\n'one_to_one'\n'one_to_many'\n'many_to_one'\n'many_to_many'\n\n# Merge validate: one_to_one\ntracks.merge(specs, on='tid',\nvalidate='one_to_one')\n\n# Merge validate: one_to_many\nalbums.merge(tracks, on='aid',\nvalidate='one_to_many')\n\n# Verifying concatenations\n.concat(verify_integrity=False) :\nCheck whether the new concatenated index contains duplicates\nDefault value is False"
  },
  {
    "objectID": "posts/2021-01-13-pandas-cheatsheet.html#merging-ordered-and-time-series-data",
    "href": "posts/2021-01-13-pandas-cheatsheet.html#merging-ordered-and-time-series-data",
    "title": "pandas cheatsheet",
    "section": "Merging Ordered and Time-Series Data",
    "text": "Merging Ordered and Time-Series Data\n\nUsing merge_ordered()\n# Merging stock data\nimport pandas as pd\npd.merge_ordered(appl, mcd, on='date', suffixes=('_aapl','_mcd'))\n\n# Forward fill example\npd.merge_ordered(appl, mcd, on='date',\nsuffixes=('_aapl','_mcd'),\nfill_method='ffill')\n\n\nUsing merge_asof()\n\n# merge_asof() example\npd.merge_asof(visa, ibm, on='date_time',\nsuffixes=('_visa','_ibm'))\n\n# merge_asof() example with direction\npd.merge_asof(visa, ibm, on=['date_time'],\nsuffixes=('_visa','_ibm'),\ndirection='forward')\n\n\nSelecting data with .query()\n\n# Querying on a single condition\nstocks.query('nike >= 90')\n\n# Querying on a multiple conditions, \"and\", \"or\"\nstocks.query('nike > 90 and disney < 140')\nstocks.query('nike > 96 or disney < 98')\n\n# Using .query() to select text\nstocks_long.query('stock==\"disney\" or (stock==\"nike\" and close < 90)')\n\n\nReshaping data with .melt()\n\n# Example of .melt()\nsocial_fin_tall = social_fin.melt(id_vars=['financial','company'])\n\n# Melting with value_vars\nsocial_fin_tall = social_fin.melt(id_vars=['financial','company'],\nvalue_vars=['2018','2017'])\n\n# Melting with column names\nsocial_fin_tall = social_fin.melt(id_vars=['financial','company'],\nvalue_vars=['2018','2017'],\nvar_name=['year'], value_name='dollars')"
  },
  {
    "objectID": "posts/2021-01-14-java-installation-on-ubuntu-20.04.html",
    "href": "posts/2021-01-14-java-installation-on-ubuntu-20.04.html",
    "title": "Java installation on Ubuntu 20.04",
    "section": "",
    "text": "Current configuration\n\n!java --version\n\nopenjdk 11.0.9.1 2020-11-04\nOpenJDK Runtime Environment (build 11.0.9.1+1-Ubuntu-0ubuntu1.20.04)\nOpenJDK 64-Bit Server VM (build 11.0.9.1+1-Ubuntu-0ubuntu1.20.04, mixed mode, sharing)\n\n\n\n\nDownload Oracle JDK 11\nFrom https://launchpad.net/~linuxuprising/+archive/ubuntu/java/+packages, I can identify the focal version:\noracle-java11-installer-local - 11.0.9-1~linuxuprising0    (changes file)  logix2  2020-10-22  Published   Focal   Java\nI download the given version from Oracle website: https://www.oracle.com/java/technologies/javase-jdk11-downloads.html. Java SE Development Kit 11.0.9 Linux x64 Compressed Archive\nAnd yes you have to login with an oracle account to download it.\n\n\nInstallation via linuxuprising/java\nsudo add-apt-repository ppa:linuxuprising/java\nsudo apt update\nsudo mkdir -p /var/cache/oracle-jdk11-installer-local/\nsudo cp ~/Downloads/jdk-11.0.9_linux-x64_bin.tar.gz /var/cache/oracle-jdk11-installer-local/\nsudo apt install oracle-java11-installer-local\nAfter accepting the license agreement, installation is running\n\n\nCheck\n$ sudo update-alternatives --config java\nThere are 2 choices for the alternative java (providing /usr/bin/java).\n\n  Selection    Path                                         Priority   Status\n------------------------------------------------------------\n  0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java   1111      auto mode\n  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java   1111      manual mode\n* 2            /usr/lib/jvm/java-11-oracle/bin/java          1091      manual mode\n\n\nEnvironment variable\nEnter /usr/lib/jvm/java-11-oracle as your JAVA_HOME variable in /etc/environment\n$ cat /etc/environment\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\nJAVA_HOME=\"/usr/lib/jvm/java-11-oracle\"\n$ source /etc/environment\n$ echo $JAVA_HOME\n/usr/lib/jvm/java-11-oracle"
  },
  {
    "objectID": "posts/2021-01-19-seaborn-cheatsheet.html",
    "href": "posts/2021-01-19-seaborn-cheatsheet.html",
    "title": "seaborn cheatsheet",
    "section": "",
    "text": "pdf lectures in github\n\n\n\n\n# Getting started\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Example 1: Scatter plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nheight = [62, 64, 69, 75, 66, 68, 65, 71, 76, 73]\nweight = [120, 136, 148, 175, 137, 165, 154, 172, 200, 187]\nsns.scatterplot(x=height, y=weight)\nplt.show()\n\n# Example 2: Create a count plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ngender = [\"Female\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\"]\nsns.countplot(x=gender)\nplt.show()\n\n\n\n\n# Using DataFrames with countplot()\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv(\"masculinity.csv\")\nsns.countplot(x=\"how_masculine\", data=df)\nplt.show()\n\n\n\n# Tips dataset\nimport pandas as pd\nimport seaborn as sns\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n# A basic scatter plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips)\nplt.show()\n\n# A scatter plot with hue\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips, hue=\"smoker\")\nplt.show()\n\n# Setting hue order\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips, hue=\"smoker\", hue_order=[\"Yes\",\"No\"])\nplt.show()\n\n# Specifying hue colors\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nhue_colors = {\"Yes\": \"black\", \"No\": \"red\"}\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips, hue=\"smoker\", palette=hue_colors)\nplt.show()\n\n# Using HTML hex color codes with hue\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nhue_colors = {\"Yes\": \"#808080\", \"No\": \"#00FF00\"}\nsns.scatterplot(x=\"total_bill\", y=\"tip\", data=tips, hue=\"smoker\", palette=hue_colors)\nplt.show()\n\n# Using hue with count plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(x=\"smoker\", data=tips, hue=\"sex\")\nplt.show()\n\n\n\n\n\n\n\n# Using relplot()\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"scatter\")\nplt.show()\n\n# Subplots in columns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"scatter\", col=\"smoker\")\nplt.show()\n\n# Subplots in rows\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"scatter\", row=\"smoker\")\nplt.show()\n\n# Subplots in rows and columns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\", y=\"tip\", data=tips, kind=\"scatter\", col=\"smoker\", row=\"time\")\nplt.show()\n\n# Wrapping columns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\",y=\"tip\",data=tips,kind=\"scatter\",col=\"day\",col_wrap=2)\nplt.show()\n\n# Ordering columns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\",y=\"tip\",data=tips,kind=\"scatter\",col=\"day\",col_wrap=2,col_order=[\"Thur\",\"Fri\",\"Sat\",\"Sun\"])\nplt.show()\n\n\n\n\n# Subgroups with point size\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\",y=\"tip\",data=tips,kind=\"scatter\",size=\"size\")\nplt.show()\n\n# Point size and hue\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\",y=\"tip\",data=tips,kind=\"scatter\",size=\"size\",hue=\"size\")\nplt.show()\n\n# Subgroups with point style\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.relplot(x=\"total_bill\",y=\"tip\",data=tips,kind=\"scatter\",hue=\"smoker\",style=\"smoker\")\nplt.show()\n\n# Changing point transparency\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Set alpha to be between 0 and 1\nsns.relplot(x=\"total_bill\",y=\"tip\",data=tips,kind=\"scatter\",alpha=0.4)\nplt.show()\n\n\n\n# Line plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2_mean\",data=air_df_mean,kind=\"line\")\nplt.show()\n\n# Subgroups by location\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2_mean\",data=air_df_loc_mean,kind=\"line\",style=\"location\",hue=\"location\")\nplt.show()\n\n# Adding markers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2_mean\",data=air_df_loc_mean,kind=\"line\",style=\"location\",hue=\"location\",markers=True)\nplt.show()\n\n# Turning off line style\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2_mean\",data=air_df_loc_mean,kind=\"line\",style=\"location\",hue=\"location\",markers=True,dashes=False)\nplt.show()\n\n# Multiple observations per x-value\n# Line plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2\",data=air_df,kind=\"line\")\nplt.show()\n\n# Replacing confidence interval with standard deviation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2\",data=air_df,kind=\"line\",ci=\"sd\")\nplt.show()\n\n# Turning off confidence interval\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.relplot(x=\"hour\", y=\"NO_2\",data=air_df,kind=\"line\",ci=None)\nplt.show()\n\n\n\n\n\n\n\n# countplot() vs. catplot()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"how_masculine\",data=masculinity_data,kind=\"count\")\nplt.show()\n\n# Changing the order\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncategory_order = [\"No answer\",\"Not at all\",\"Not very\",\"Somewhat\",\"Very\"]\nsns.catplot(x=\"how_masculine\",data=masculinity_data,kind=\"count\",order=category_order)\nplt.show()\n\n# Bar plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"day\",y=\"total_bill\",data=tips,kind=\"bar\")\nplt.show()\n\n# Turning off confidence intervals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"day\",y=\"total_bill\",data=tips,kind=\"bar\",ci=None)\nplt.show()\n\n# Changing the orientation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"total_bill\",y=\"day\",data=tips,kind=\"bar\")\nplt.show()\n\n\n\n# How to create a box plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ng = sns.catplot(x=\"time\",y=\"total_bill\",data=tips,kind=\"box\")\nplt.show()\n\n# Change the order of categories\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ng = sns.catplot(x=\"time\",y=\"total_bill\",data=tips,kind=\"box\",order=[\"Dinner\",\"Lunch\"])\nplt.show()\n\n# Omitting the outliers using `sym`\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ng = sns.catplot(x=\"time\",y=\"total_bill\",data=tips,kind=\"box\",sym=\"\")\nplt.show()\n\n# Changing the whiskers using `whis`\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ng = sns.catplot(x=\"time\",y=\"total_bill\",data=tips,kind=\"box\",whis=[0, 100])\nplt.show()\n\n\n\n# Creating a point plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"age\",y=\"masculinity_important\",data=masculinity_data,hue=\"feel_masculine\",kind=\"point\")\nplt.show()\n\n# Disconnecting the points\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"age\",y=\"masculinity_important\",data=masculinity_data,hue=\"feel_masculine\",kind=\"point\",join=False)\nplt.show()\n\n# Displaying the median\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy import median\nsns.catplot(x=\"smoker\",y=\"total_bill\",data=tips,kind=\"point\",estimator=median)\nplt.show()\n\n# Customizing the confidence intervals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"smoker\",y=\"total_bill\",data=tips,kind=\"point\",capsize=0.2)\nplt.show()\n\n# Turning off confidence intervals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.catplot(x=\"smoker\",y=\"total_bill\",data=tips,kind=\"point\",ci=None)\nplt.show()\n\n\n\n\n\n\n\n# Figure style: \"whitegrid\"\nsns.set_style(\"whitegrid\")\nsns.catplot(x=\"age\",y=\"masculinity_important\",data=masculinity_data,hue=\"feel_masculine\",kind=\"point\")\nplt.show()\n\n# Other styles:\nsns.set_style(\"ticks\")\nsns.set_style(\"dark\")\nsns.set_style(\"darkgrid\")\n\n# Example (diverging palette)\nsns.set_palette(\"RdBu\")\ncategory_order = [\"No answer\",\"Not at all\",\"Not very\",\"Somewhat\",\"Very\"]\nsns.catplot(x=\"how_masculine\",data=masculinity_data,kind=\"count\",order=category_order)\nplt.show()\n\n# Custom palettes\ncustom_palette = [\"red\", \"green\", \"orange\", \"blue\",\"yellow\", \"purple\"]\nsns.set_palette(custom_palette)\n\n# Custom palettes\ncustom_palette = ['#FBB4AE', '#B3CDE3', '#CCEBC5','#DECBE4', '#FED9A6', '#FFFFCC','#E5D8BD', '#FDDAEC', '#F2F2F2']\nsns.set_palette(custom_palette)\n\n# Larger context: \"talk\"\n#Smallest to largest: \"paper\", \"notebook\", \"talk\", \"poster\"\nsns.set_context(\"talk\")\n\n\n\n\n# Adding a title to FacetGrid\ng = sns.catplot(x=\"Region\",y=\"Birthrate\",data=gdp_data,kind=\"box\")\ng.fig.suptitle(\"New Title\")\nplt.show()\n\n# Adjusting height of title in FacetGrid\ng = sns.catplot(x=\"Region\",y=\"Birthrate\",data=gdp_data,kind=\"box\")\ng.fig.suptitle(\"New Title\",y=1.03)\nplt.show()\n\n\n\n# Adding a title to AxesSubplot\ng = sns.boxplot(x=\"Region\",y=\"Birthrate\",data=gdp_data)\ng.set_title(\"New Title\",y=1.03)\n\n# Titles for subplots\ng = sns.catplot(x=\"Region\",y=\"Birthrate\",data=gdp_data,kind=\"box\",col=\"Group\")\ng.fig.suptitle(\"New Title\",y=1.03)\ng.set_titles(\"This is {col_name}\")\n\n# Adding axis labels\ng = sns.catplot(x=\"Region\",y=\"Birthrate\",data=gdp_data,kind=\"box\")\ng.set(xlabel=\"New X Label\",ylabel=\"New Y Label\")\nplt.show()\n\n# Rotating x-axis tick labels\ng = sns.catplot(x=\"Region\",y=\"Birthrate\",data=gdp_data,kind=\"box\")\nplt.xticks(rotation=90)\nplt.show()"
  },
  {
    "objectID": "posts/2021-01-19-seaborn-cheatsheet.html#seaborn-introduction",
    "href": "posts/2021-01-19-seaborn-cheatsheet.html#seaborn-introduction",
    "title": "seaborn cheatsheet",
    "section": "Seaborn Introduction",
    "text": "Seaborn Introduction\n\nIntroduction to Seaborn\n# Seaborn distplot \nimport seaborn as sns\nsns.distplot(df['alcohol'])\n\n\nUsing the distribution plot\n# Creating a histogram\nsns.distplot(df['alcohol'], kde=False, bins=10)\n\n# Alternative data distributions\nsns.distplot(df['alcohol'], hist=False, rug=True)\n\n# Further Customizations\nsns.distplot(df['alcohol'], hist=False,rug=True, kde_kws={'shade':True})\n\n\nRegression Plots in Seaborn\n# Introduction to regplot\nsns.regplot(x=\"alcohol\", y=\"pH\", data=df)\n\n# lmplot faceting\nsns.lmplot(x=\"quality\", y=\"alcohol\",data=df, hue=\"type\") \nsns.lmplot(x=\"quality\", y=\"alcohol\",data=df, col=\"type\")"
  },
  {
    "objectID": "posts/2021-01-19-seaborn-cheatsheet.html#customizing-seaborn-plots-1",
    "href": "posts/2021-01-19-seaborn-cheatsheet.html#customizing-seaborn-plots-1",
    "title": "seaborn cheatsheet",
    "section": "Customizing Seaborn Plots",
    "text": "Customizing Seaborn Plots\n\nUsing Seaborn Styles\n# Setting Styles\n# Seaborn has default configurations that can be applied with sns.set()\n# These styles can override matplotlib and pandas plots as well\nsns.set()\n\n# Theme examples with sns.set_style()\nfor style in ['white','dark','whitegrid','darkgrid','ticks']:\n    sns.set_style(style)\n    sns.distplot(df['Tuition'])\n    plt.show()\n\n# Removing axes with despine()\nsns.set_style('white')\nsns.distplot(df['Tuition'])\nsns.despine(left=True)\n\n\nColors in Seaborn\n# Defining a color for a plot\nsns.set(color_codes=True)\nsns.distplot(df['Tuition'], color='g')\n\n# Palettes\nfor p in sns.palettes.SEABORN_PALETTES:\n    sns.set_palette(p)\n    sns.distplot(df['Tuition'])\n    \n# Displaying Palettes\nfor p in sns.palettes.SEABORN_PALETTES:\n    sns.set_palette(p)\n    sns.palplot(sns.color_palette())\n    plt.show()\n    \n# Defining Custom Palettes\n# Circular colors = when the data is not ordered \nsns.palplot(sns.color_palette(\"Paired\", 12))\n\n# Sequential colors = when the data has a consistent range from high to low\nsns.palplot(sns.color_palette(\"Blues\", 12))\n\n# Diverging colors = when both the low and high values are interesting\nsns.palplot(sns.color_palette(\"BrBG\", 12))\n\n\nCustomizing with matplotlib\n\n# Matplotlib Axes\nfig, ax = plt.subplots()\nsns.distplot(df['Tuition'], ax=ax)\nax.set(xlabel=\"Tuition 2013-14\")\n\n# Further Customizations\nfig, ax = plt.subplots()\nsns.distplot(df['Tuition'], ax=ax)\nax.set(xlabel=\"Tuition 2013-14\",ylabel=\"Distribution\", xlim=(0, 50000),title=\"2013-14 Tuition and Fees Distribution\")\n\n# Combining Plots\nfig, (ax0, ax1) = plt.subplots(nrows=1,ncols=2, sharey=True, figsize=(7,4))\nsns.distplot(df['Tuition'], ax=ax0)\nsns.distplot(df.query('State == \"MN\"')['Tuition'], ax=ax1)\nax1.set(xlabel=\"Tuition (MN)\", xlim=(0, 70000))\nax1.axvline(x=20000, label='My Budget', linestyle='--')\nax1.legend()"
  },
  {
    "objectID": "posts/2021-01-19-seaborn-cheatsheet.html#additional-plot-types",
    "href": "posts/2021-01-19-seaborn-cheatsheet.html#additional-plot-types",
    "title": "seaborn cheatsheet",
    "section": "Additional Plot Types",
    "text": "Additional Plot Types\n\nCategorical Plot Types\n# Plots of each observation - stripplot\nsns.stripplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\",\njitter=True)\n\n# Plots of each observation - swarmplot\nsns.swarmplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\")\n\n# Abstract representations - boxplot\nsns.boxplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\")\n\n# Abstract representation - violinplot\nsns.violinplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\")\n\n# Abstract representation - lvplot\nsns.lvplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\")\n\n# Statistical estimates - barplot\nsns.barplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\",\nhue=\"Region\")\n\n# Statistical estimates - pointplot\nsns.pointplot(data=df, y=\"DRG Definition\",\nx=\"Average Covered Charges\",\nhue=\"Region\")\n\n# Statistical estimates - countplot\nsns.countplot(data=df, y=\"DRG_Code\", hue=\"Region\")\n\n\nRegression Plots\n# Plotting with regplot()\nsns.regplot(data=df, x='temp',\ny='total_rentals', marker='+')\n\n# Evaluating regression with residplot()\nsns.residplot(data=df, x='temp', y='total_rentals')\n\n# Polynomial regression\nsns.regplot(data=df, x='temp',\ny='total_rentals', order=2)\n\n# residplot with polynomial regression\nsns.residplot(data=df, x='temp',\ny='total_rentals', order=2)\n\n# Categorical values\nsns.regplot(data=df, x='mnth', y='total_rentals',\nx_jitter=.1, order=2)\n\n# Estimators\nsns.regplot(data=df, x='mnth', y='total_rentals',\nx_estimator=np.mean, order=2)\n\n# Binning the data\nsns.regplot(data=df,x='temp',y='total_rentals',\nx_bins=4)\n\n\nMatrix plots\n\n# Getting data in the right format\npd.crosstab(df[\"mnth\"], df[\"weekday\"],\nvalues=df[\"total_rentals\"],aggfunc='mean').round(0)\n\n# Build a heatmap\nsns.heatmap(pd.crosstab(df[\"mnth\"], df[\"weekday\"],\nvalues=df[\"total_rentals\"], aggfunc='mean')\n)\n\n# Customize a heatmap\nsns.heatmap(df_crosstab, annot=True, fmt=\"d\",\ncmap=\"YlGnBu\", cbar=False, linewidths=.5)\n\n# Centering a heatmap\nsns.heatmap(df_crosstab, annot=True, fmt=\"d\",\ncmap=\"YlGnBu\", cbar=True,\ncenter=df_crosstab.loc[9, 6])\n\n# Plotting a correlation matrix\nsns.heatmap(df.corr())"
  },
  {
    "objectID": "posts/2021-01-19-seaborn-cheatsheet.html#creating-plots-on-data-aware-grids",
    "href": "posts/2021-01-19-seaborn-cheatsheet.html#creating-plots-on-data-aware-grids",
    "title": "seaborn cheatsheet",
    "section": "Creating Plots on Data Aware Grids",
    "text": "Creating Plots on Data Aware Grids\n\nUsing FacetGrid, factorplot and lmplot\n# FacetGrid Categorical Example\ng = sns.FacetGrid(df, col=\"HIGHDEG\")\ng.map(sns.boxplot, 'Tuition',\norder=['1', '2', '3', '4'])\n\n# factorplot()\nsns.factorplot(x=\"Tuition\", data=df,\ncol=\"HIGHDEG\", kind='box')\n\n# FacetGrid for regression\n# FacetGrid() can also be used for sca er or regression plots\ng = sns.FacetGrid(df, col=\"HIGHDEG\")\ng.map(plt.scatter, 'Tuition', 'SAT_AVG_ALL')\n\n\n# lmplot\n# lmplot plots sca er and regression plots on a FacetGrid\nsns.lmplot(data=df, x=\"Tuition\", y=\"SAT_AVG_ALL\",\ncol=\"HIGHDEG\", fit_reg=False)\n\n# lmplot with regression\nsns.lmplot(data=df, x=\"Tuition\", y=\"SAT_AVG_ALL\",\ncol=\"HIGHDEG\", row='REGION')\n\n\nUsing PairGrid and pairplot\n# Creating a PairGrid\ng = sns.PairGrid(df, vars=[\"Fair_Mrkt_Rent\", \"Median_Income\"])\ng = g.map(plt.scatter)\n\n# Customizing the PairGrid diagonals\ng = sns.PairGrid(df, vars=[\"Fair_Mrkt_Rent\", \"Median_Income\"])\ng = g.map_diag(plt.hist)\ng = g.map_offdiag(plt.scatter)\n\n# Pairplot\nsns.pairplot(df, vars=[\"Fair_Mrkt_Rent\", \"Median_Income\"], kind='reg', diag_kind='hist')\n\n# Customizing a pairplot\nsns.pairplot(df.query('BEDRMS < 3'),vars=[\"Fair_Mrkt_Rent\",\"Median_Income\", \"UTILITY\"],hue='BEDRMS', palette='husl', plot_kws={'alpha': 0.5})\n\n\nUsing JointGrid and jointplot\n# Basic JointGrid\ng = sns.JointGrid(data=df, x=\"Tuition\",y=\"ADM_RATE_ALL\")\ng.plot(sns.regplot, sns.distplot)\n\n# Advanced JointGrid\ng = sns.JointGrid(data=df, x=\"Tuition\",y=\"ADM_RATE_ALL\")\ng = g.plot_joint(sns.kdeplot)\ng = g.plot_marginals(sns.kdeplot, shade=True)\ng = g.annotate(stats.pearsonr)\n\n# jointplot()\nsns.jointplot(data=df, x=\"Tuition\",y=\"ADM_RATE_ALL\", kind='hex')\n\n# Customizing a jointplot\ng = (sns.jointplot(x=\"Tuition\",\n                   y=\"ADM_RATE_ALL\", kind='scatter',\n                   xlim=(0, 25000),\n                   marginal_kws=dict(bins=15,rug=True),\n                   data=df.query('UG < 2500 & Ownership == \"Public\"'))\n     .plot_joint(sns.kdeplot))"
  },
  {
    "objectID": "posts/2021-01-21-aristotle-and-deep-learning.html",
    "href": "posts/2021-01-21-aristotle-and-deep-learning.html",
    "title": "Aristotle and Deep learning",
    "section": "",
    "text": "This is a massive book from 2005 in its 6th edition. I don’t think it has been updated since that. And the author starts a writing of AI history.\nI have been intrigued by the use of the opening sentence from Aristotle in the Metaphysics: “All men by nature desire to know…”. I remembered that sentence (without knowing it was from Aristotle), and I jumped to The Metaphysics _ Aristotle’s wikipedia page (the French one). There is a nice presentation of The MetaPhysics and some extracts that I have found quite interesting. One of them following “All men by nature desire to know” is detailing what is art and science; and for art: one need to be able to recognize similar cases and be able to generalize to an (more) universal rule.\nI cannot not see a link with what is happening in what we do on a daily basis in AI and deep learning. I had been surprised by Jeremy Howard’s curriculum (if I am not wrong he has a major) in Philosophy, and I better understand why he is so good in what he does.\nShould have studied Philosophy and ancient Greek!\nWould love to know your thoughts about that. (and if anyone can ask Jeremy’s without directly @ him)\nHere is a more detailed analysis of Aristotle thought: (again from wikipedia, not my own ;))\nBy nature, all animals are sentient; but sensation is not yet sufficient to produce knowledge: indeed, remarks Aristotle, sensation engenders memory or not. But animals endowed with memory are the most intelligent and the best able to learn. However, man “lives on art and reasoning.” To learn, you have to feel, remember, but man has the capacity to draw experience from these simple images and from a multitude of experimental notions emerges a single judgment that is universal in all similar cases: it is what constitutes art: “Science and art arise for men through experience” 10. Art therefore presupposes: the ability to recognize similar cases and the ability to apply a universal rule to these cases.\n\nOf experience and art, which is more perfect? In practical life, experience seems superior to art, because it is knowledge of the particular, of the individual: sensations, the foundation of knowledge of the particular, are not science and do not teach us the why ( διότι). Art, for its part, knows the universal and goes beyond individual things, it is to art that knowledge and the faculty of understanding belong: men of art know the why and the cause. The wisest are wise not by practical skill, but by theory (λόγος) and knowledge of the causes. This explains the superiority of the architect over the maneuver.\n\nThe sign of this knowledge is that it can be taught; now men of art can teach. However, among the arts some relate to the necessities of life and others come from “leisure” which is knowledge sought for itself, as in mathematics. And through these appears the highest knowledge, wisdom, which has for its object the first causes and the first principles of what-is; therefore the theoretical sciences are superior to the practical sciences.\nFrom observations (rows of data) we can recognize similar cases (patterns or embeddings) and identify universal rules (models?)"
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html",
    "title": "Reinforcement learning readings",
    "section": "",
    "text": "from https://www.youtube.com/watch?v=Obek04C8L5E&feature=youtu.be\nat 26’ idea that you can tackle over-optimism models by using ensemble models. See paper at 2018 Model-Ensemble Trust-Region Policy Optimization"
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-algorithms-an-intuitive-overview",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-algorithms-an-intuitive-overview",
    "title": "Reinforcement learning readings",
    "section": "1/26/21 - Reinforcement Learning algorithms — an intuitive overview",
    "text": "1/26/21 - Reinforcement Learning algorithms — an intuitive overview\nfrom https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc\n\ngive an overview of various RL models. Model-based vs model-free.\nAnd papers and codes."
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-partie-1-introduction-in-french",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-partie-1-introduction-in-french",
    "title": "Reinforcement learning readings",
    "section": "1/26/21 - Reinforcement learning, partie 1 : introduction (in French)",
    "text": "1/26/21 - Reinforcement learning, partie 1 : introduction (in French)\nThere is a reference to an introduction paper: from Sutton, Richard S., and Andrew G. Barto « Reinforcement learning : an introduction. » (2011). (I have an updated version from 2015)\nThere is a reference to a blog article [2] Steeve Huang. “Introduction to Various Reinforcement Learning Algorithms. Part I” (Q-Learning, SARSA, DQN, DDPG)”. (2018)\nAnd the paper for OpenAI Gym [3] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba. “OpenAI Gym”. (2016)"
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-an-introduction---i-tabular-solution-methods",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-an-introduction---i-tabular-solution-methods",
    "title": "Reinforcement learning readings",
    "section": "1/27/21 - Reinforcement learning : an introduction - I tabular solution methods",
    "text": "1/27/21 - Reinforcement learning : an introduction - I tabular solution methods\nas a ref. from Reinforcement learning, partie 1 : introduction (in French)\nI like this summary about RL\n\nReinforcement learning is a computational approach to understanding and automating goal-directed learning and decision-making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without relying on exemplary supervision or complete models of the environment. In our opinion, reinforcement learning is the first field to seriously address the computational issues that arise when learning from interaction with an environment in order to achieve long-term goals. Reinforcement learning uses a formal framework defining the interaction between a learning agent and its environment in terms of states, actions, and rewards. This framework is intended to be a simple way of representing essential features of the artificial intelligence problem. These features include a sense of cause and effect, a sense of uncertainty and nondeterminism, and the existence of explicit goals.\n\nThere is some history about RL. Bellman equation and dynamic programming are at the beginning of RL.\nI read about HJB equation from Huyên PHAM (from a French Math magazine). It is funny to see why dynamic programming has been named that way, and how to deal with management.\n\nThe class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming (Bellman, 1957a). Bellman (1957b) also introduced the discrete stochastic version of the optimal control problem known as Markovian decision processes (MDPs), and Ronald Howard (1960) devised the policy iteration method for MDPs. All of these are essential elements underlying the theory and algorithms of modern reinforcement learning.\n\nAll the vocabulary around RL is coming from dynamic programming and MDP.\nMarkov decision process - Wikipedia\n\n\n\nInteresting to read that the famous cart pole experiment (learning to balance a pole hinged to a movable cart) came from Michie and Chambers in 1968, 53 years ago! (and derived from tic-tac-toe experiment)\nI don’t understand the subtlety behind the move from “learning with a teacher” to “learning with a critic” following the modified Least-Mean-Square (LMS) algorithm; Widrow and Hoff (1973)\nAnd some explanations about temporal-difference. I have just understood that a convergence effort happened (in 1989) by Chris Watkin who brought together temporal-difference and optimal control by developing Q-learning.\nAfter this introduction, here is the content:\n1st part is about finite markov decision processes—and its main ideas including Bellman equations and value functions.\n2nd part is about describing three fundamental classes of methods for solving finite Markov decision problems: dynamic programming, Monte Carlo methods, and temporal-difference learning. Each class of methods has its strengths and weaknesses. Dynamic programming methods are well developed mathematically, but require a complete and accurate model of the environment. Monte Carlo methods don’t require a model and are conceptually simple, but are not suited for step-by-step incremental computation. Finally, temporal-difference methods require no model and are fully incremental, but are more complex to analyze.\n3rd part is about combining these methods to offer a complete and unified solution to the tabular reinforcement learning problem.\nWe can think of terms agent, environment, and action as engineers’ terms controller, controlled system (or plant), and control signal.\n\n\n\nThe agent–environment interaction in reinforcement learning.\n\n\nExplanation about agent vs environment. Often not the same as physical boundaries of a robot: this boundary represents the limit of the agent’s absolute control, not of its knowledge. Many different agents can be operated at once.\nThe agent’s goal is to maximize the total mount of reward it receives.\nI should re-read the full chapter3 because a lot of concepts coming from MDP is exposed, and their links to RL. At the end I should be able to answer most of end-of-chapter exercises. Have clearer view about how to define what are my agents/environment in my case; how to define actions (low-level definition (e.g. V in level1 electrical grid vs high level decision)); everything related to q* and Q-learning.\n\ndynamic programming (DP) (chap4 - 103-126)\nWhat is key here is to have an exact way to describe your environment. Which is not always feasible. And we need computer power to go through all states, compute value function. There is a balance between policy evaluation and policy improvement but this is not crystal clear to me. And I don’t understand asynchronous DP. I haven’t developed enough intuitions behind DP, and I am unable to answer exercises. I understand though that reinforcement learning can solve some problems by approximating part of it (evaluation, environment, …)\n\n\nmonte carlo (MC) methods (chap5 - 127-156)\nfirst-visit vs every-visit methods. First-visit has been widely studied. Blackjack example. Explanation of Monte Carlo ES (exploratory starts); and how to avoid this unlikely assumption thanks to on-policy or off-policy methods (on-policy estimate the value of a policy while using it for control. In off-policy methods these two functions are separated (behavior and target)).\nOne issue with MC methods is to ensure sufficient exploration. One approach is to start with a random state-action pair, could work with simulated episodes but unlikely to learn from real experience.\nMC methods do not bootstrap (i.e. they don’t update their value estimates based on other value estimates) (TODO learn more about bootstrapping)\n\n\ntemporal-difference (TD) learning (chap6 - 157-180)\nTD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap, or said differently they learn a guess from a guess).\nIf you consider optimization as a 2 phases approach: prediction problem (ie policy evaluation) and control problem (ie optimal policy), DP, TD, MC differences are at the prediction problem. On control problem they use variations of generalized policy iteration (GPI).\nTD methods combine the sampling of Monte Carlo with the bootstrapping of DP.\nExample based on Driving Home. In TD you update prediction at each step, not waiting for the final return as in MC.\n\n\neligibility traces (chap7 - 181-208)\nTD(\\[\\lambda\\]) is a way to integrate MC and TD.\nIf one wants to use TD methods because of their other advantages, but the task is at least partially non-Markov, then the use of an eligibility trace method is indicated. Eligibility traces are the first line of defense against both long-delayed rewards and non-Markov tasks.\nI am not sure to understand the effect of bootstrap.\n\n\nPlanning and Learning with Tabular Methods (chap8 - 209-220-236)\nplanning = require a model (dynamic programming, heuristic search)\nlearning = can be used without a model (MC, TD)\nThe difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment."
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html#from-a-deep-reinforcement-learning-based-multi-criteria-decision-support-system-for-optimizing-textile-chemical-process",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html#from-a-deep-reinforcement-learning-based-multi-criteria-decision-support-system-for-optimizing-textile-chemical-process",
    "title": "Reinforcement learning readings",
    "section": "2/18/21 - from A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Optimizing Textile Chemical Process",
    "text": "2/18/21 - from A Deep Reinforcement Learning Based Multi-Criteria Decision Support System for Optimizing Textile Chemical Process\nThis is a more practical paper and should help to figure out what could be our own implementation.\nOverall MDP (markov decision process) structure is quite interesting with 3 blocks:\n\nRF (random forest) models (one per objective)\nAHP (analytic hierarchy process) which is a MCDM (Multiple criteria decision-making) method\nDQN which is the reinforcement learning part to approximate the Q function\n\n\nthere are interesting references.\n[2] K. Suzuki, ARTIFICIAL NEURAL NETWORKS - INDUSTRIAL AND CONTROL ENGINEERING APPLICATIONS. 2011.\n\nIt is nearly impossible to upgrade the textile chemical manufacturing processes directly by only following the cases from other industries without considering the detailed characteristics of this sector and specific investigations in the applicable advanced technologies. To this end, the construction of accurate models for simulating manufacturing processes using intelligent techniques is rather necessary[2]\n\n[4]A. Ghosh, P. Mal, and A. Majumdar, Advanced Optimization and Decision-Making Techniques in Textile Manufacturing.2019.\n\n[..] Therefore, production decision-makers cannot effectively control the processes in order to obtain desired product functionalities [4]\n\n[53] T.L. Saaty, “What is the analytic hierarchy process?” Mathematical models for decision support, Springer, 1988, pp.109 121.\n\nThe AHP is a MCDM method introduced by Saaty [53]\n\n[54]R. S. Sutton and A. G. Barto, Introduction to reinforcement learning, vol. 135. MIT press Cambridge, 1998.\n\nThe Markov property indicates that the state transitions are only dependent on the current state and current action is taken, but independent to all prior states and actions[54].\n\n[66] Z. Chourabi, F.Khedher, A. Babay and M. Cheikhrouhou, “Multi-criteria decision making in workforce choice using AHP, WSM and WPM”, J.Text.Inst., 2018\n\nHowever, it is worth remarking that certain features of this framework may hinder the massive promotion and application of it. The AHP has been successfully implemented in MCDM problems [41], [66]"
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html#the-complete-reinforcement-learning-dictionary",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html#the-complete-reinforcement-learning-dictionary",
    "title": "Reinforcement learning readings",
    "section": "2/18/21 - The Complete Reinforcement Learning Dictionary",
    "text": "2/18/21 - The Complete Reinforcement Learning Dictionary\nrecommandations:\n\n\nIf you’re looking for a quick, 10-minutes crash course into RL with code examples, checkout my Qrash Course series: Introduction to RL and Q-Learning and Policy Gradients and Actor-Critics.\nI you’re into something deeper, and would like to learn and code several different RL algorithms and gain more intuition, I can recommend this series by Thomas Simonini and this series by Arthur Juliani.\nIf you’re ready to master RL, I will direct you to the “bible” of Reinforcement Learning — “Reinforcement Learning, an introduction” by Richard Sutton and Andrew Barto. The second edition (from 2018) is available for free (legally) as a PDF file."
  },
  {
    "objectID": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-an-introduction---ii-approximate-solution-methods",
    "href": "posts/2021-01-26-reinforcement-learning-readings.html#reinforcement-learning-an-introduction---ii-approximate-solution-methods",
    "title": "Reinforcement learning readings",
    "section": "3/5/21 - Reinforcement learning : an introduction - II Approximate Solution Methods",
    "text": "3/5/21 - Reinforcement learning : an introduction - II Approximate Solution Methods\nThis is the 2nd part of the book.\nOn-policy Approximation of Action Values\nAs mentioned in introduction of part II, what is developed in part I (our estimates of value functions are represented as a table with one entry for each state or for each state–action pair) is instructive, but of course it is limited to tasks with small numbers of states and actions.\nHow can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset?\nThis is a generalization issue (or function approximation) one could consider as an instance of supervised learning, where we use the s->v of each backup as a training example, and then interpret the approximate function produced as an estimated value function.\nBertsekas and Tsitsiklis (1996) present the state of the art in function approximation in reinforcement learning.\nPolicy approximation\nActor-Critic: The policy structure is known as the actor, because it is used to select actions, and the estimated value function is known as the critic, because it criticizes the actions made by the actor.\n(3/12/21) end of book. Chapter 14 - Applications and case studies.\nI like this statement:\n\nApplications of reinforcement learning are still far from routine and typically require as much art as science. Making applications easier and more straightforward is one of the goals of current research in reinforcement learning.\n\nTD backgammon (1995). It uses a neural net (1 hidden layer, from 40 to 80 units) to approximate the predicted probability of winning v(s) for a given state. In later version, some domain features were used but still using self-play TD learning method. (I don’t know specifics for these domain features). And last versions give an interest to opponent reactions (possible dice rolls and moves)\nSamuel’s Checkers Player (~1960). (Checkers c’est le jeu de dames). It is based on minimax procedure to find the best move from current position. 1st learning used was rote learning (storing position(s value). 2nd learning used alpha-beta (linked to minimax procedure) and hierarchical lookup tables instead of linear function approximation.\nAcrobot (1993). Use of Sarsa(\\[\\lambda\\]). Interesting to see that an exploration step can spoil a whole sequence of good actions. This is why greedy policy is used (\\[\\epsilon\\]=0).\nElevator dispatching (1996). With a reward being the negative of the sum of the squared waiting times of all waiting passengers. (squared to push the system to avoid big waiting times). We use an extension of Q-learning to semi-Markov decision problems. For function approximation, a nonlinear neural network trained by back-propagation was used to represent the action-value function.\nDynamic Channel Allocation (1997). The channel assignment problem can be formulated as a semi-Markov decision process much as the elevator dispatching problem was in the previous section.\nJob-Shop Scheduling (1996). Zhang and Dietterich’s job-shop scheduling system is the first successful instance of which we are aware in which reinforcement learning was applied in plan-space, that is, in which states are complete plans (job-shop schedules in this case), and actions are plan modifications. This is a more abstract application of reinforcement learning than we are used to thinking about.\nChapter 15 - Prospects\n\nThis is a map to distinguish where to use different techniques. And considerations of a 3rd dimension regarding function approximation, or on/off-policy.\nAnd then opening to non markov case such as the theory of partially observable MDPs (POMDPs). (StarCraft!)\nReferences\n25 pages of references! Woawww."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "",
    "text": "From http://introtodeeplearning.com/\nI keep all content (lectures, notebooks) in github\nThis is done with google contribution, and therefore all examples are in tensorflow. I will try to adapt notebooks in PyTorch."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#intro-to-deep-learning---lecture-1",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#intro-to-deep-learning---lecture-1",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "2/5/21 - Intro to Deep Learning - lecture 1",
    "text": "2/5/21 - Intro to Deep Learning - lecture 1\nLecturer: Alexander Amini\nIntro is just jaw-dropping!\n2020 intro was top.\n2021 intro is just awesome.\nIt is a standard overview of simple deep learning concepts: Perceptron, multi-perceptron, dense layers, loss, gradient-descent, backprop, SGD, regularization, dropout, early stoppping"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-sequence-modeling---lecture-2",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-sequence-modeling---lecture-2",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "2/15/21 - Deep Sequence Modeling - lecture 2",
    "text": "2/15/21 - Deep Sequence Modeling - lecture 2\nNew lecturer: Ava Soleimany\nNice introduction to sequence modeling with Many-to-One, One-to-Many, Many-to-Many.\nRNN and implementation in TensorFlow. And NLP examples: next word problem. (and NLP concepts such as Vocabulary, Indexing, Embedding)\nAnd what we need for sequence modeling:\n\nhandle variable-length sequences\ntrack long-term dependencies\nmaintain information about order\nshare parameters across the sequence\n\nBackpropagation through time and problem of exploding/vanishing gradients.\nAgainst exploding: gradient clipping. Against vanishing: 3 ways explained - activation functions, weight init, network arch.\nGated cell: to control what information is passed through. Ex: LSTM Long Short Term Memory. They support something closed to Forget Store Update Output. Ava explains graphically which part of LSTM cells is providing which function.\nAnd then examples: Music generation (to generate 4th movement of last symphony from Schubert!), sentiment classification, machine translation (with Attention mechanisms which provide learnable memory access to solve Not long memory), trajectory prediction, environmental modeling."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#intro-to-tensorflow-music-generation---software-lab-1",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#intro-to-tensorflow-music-generation---software-lab-1",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "2/16/21 - Intro to TensorFlow; Music Generation - software lab 1",
    "text": "2/16/21 - Intro to TensorFlow; Music Generation - software lab 1\nAs an exercise I have completed labs in TensorFlow and adapted them in PyTorch.\nWith LSTM, I ran into this error: UnknownError: Fail to find the dnn implementation. [Op:CudnnRNN]\nWhich is solved by calling tf.config.experimental.set_memory_growth.\nimport tensorflow as tf \ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\nMusic lab is nice to play with. I am not sure I would be able to convert to PyTorch. It would require time!"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-computer-vision---lecture-3",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-computer-vision---lecture-3",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "2/22/21 - Deep Computer Vision - lecture 3",
    "text": "2/22/21 - Deep Computer Vision - lecture 3\nI have never been a big fan of computer vision.\nI like the idea developed by Alexander Amini about hierarchy of features. (low level: edges, spots; mid level: eyes, noses)\nAnd how he explains limitation of FC layers for visual detection, and introduction of spatial structure (feature extraction with convolutions)\nSome nice examples of hand-engineered convolution filters for different needs: sharpen, edge detect, strong edge detect.\nThen classic explanations of CNN with convolution, max pooling.\nI like the way classification problems are broken down between feature learning (convolution+relu, pooling, repeated several times) and classification (flatten, FC, softmax) which is a task learning part.\nThe second part (task learning part) can be anything: classification, object detection, segmentation, probabilistic control, …\n\nNice explanation of R-CNN to learn region proposals.\nIntroduction to Software lab2: de-biaising facial recognition systems."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-generative-modeling---lecture-4",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-generative-modeling---lecture-4",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/1/21 - Deep Generative Modeling - lecture 4",
    "text": "3/1/21 - Deep Generative Modeling - lecture 4\nFrom pattern discovered from data (underlying structure of the data), generate examples following these patterns.\nAutoencoder: foundational generative model which builds up latent variable representation by self-encoding the input. To train such network, we create a decoder to go from latent variable to generated output, and then compare input to generated output.\n\nVariational autoencoder (vae): with vae we try to encode inputs as distributions defined by mean \\[\\mu\\] and variance \\[\\sigma\\]. And we want to achieve continuity and completeness:\n\ncontinuity: points that are close in latent space –> similar content after decoding\ncompleteness: sampling from latent space –> ‘meaningful’ content after decoding\n\nRegularization is pushing to get these properties.\n\nAnd the learning process is about minimizing reconstruction loss + a regularization term:\n\nAva is then explaining the smart trick to allow backpropagation to happen. Indeed by introducing stochastic term in the sampling layer, we are breaking the backpropagation logic.\nWe are moving z from a normal distribution to \\[\\mu\\]+\\[\\sigma\\].\\[\\epsilon\\] where \\[\\epsilon\\] follow a normal distribution of mean 0, std 1.\nExplanation then of space disentanglement via \\[\\beta\\]-VAEs. It allows latent variables to be independent.\n\nAnd then some introduction about **GANs* (Generative Adversarial Network) which are a way to make a generative model by having 2 neural networks (generator and discriminator) compete with each other.\nAnd share some recent advances on GAN such as StyleGAN(2), conditional GAN, CycleGAN. CycleGAN is famous for turning horses in zebras, but it can be used to transform speech as well (used in the synthesis of Obama’s voice)"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#de-biasing-facial-recognition-systems---software-lab-2",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#de-biasing-facial-recognition-systems---software-lab-2",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/1/21 - De-biasing Facial Recognition Systems - Software Lab 2",
    "text": "3/1/21 - De-biasing Facial Recognition Systems - Software Lab 2\nPart 1 MNIST\nstarts with FC layers. With some overfitting but a good accuracy of 96%.\nthen move to a CNN architecture. I ran into gpu issues. Accuracy is now 99%.\nI didn’t manage to make the last part working. (using tape.gradient)\nPart 2 Debiasing\nFit a CNN model to classify faces based on celebA dataset. And see the bias effect by predicting on Fitzpatrick scale skin type classification system.\nUse VAE to learn latent structure.\n\n\n\nThe concept of a VAE\n\n\nTo then debias using DB-VAE model.\n\n\n\nDB-VAE\n\n\nThere is a lack of progressive unit tests to validate each step. Cannot go to the end.\nWould be interested to see how to apply to non computer vision problems."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-reinforcement-learning---lecture-5",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#deep-reinforcement-learning---lecture-5",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/8/21 - Deep Reinforcement Learning - lecture 5",
    "text": "3/8/21 - Deep Reinforcement Learning - lecture 5\nQ-function captures the expected total future reward an agent in state s can receive by executing a certain action a.\nDistinction between Value Learning (learn Q function) and Policy Learning (find directly \\[\\pi\\](s)).\n\nValue Learning or DQN\n\n\nThe key thing is about handling of continuous actions.\nLet’s see how to do it with policy learning:\nPolicy learning or Policy Gradient (PG)\n\n\nAlexanders ends the lecture by discussing about Deepmind progress:\n\nalphaGo - 2016: with a pretrain in supervised mode then standard DRL\nalphaGo Zero - 2017: standard DRL without pretraining\nalphaZero - 2018: standard DRL without pretraining and applied to several games (Go, Chess, Shogi)\nMuZero - 2020: learns the rules of the game by itself, create unknown dynamics"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#limitations-and-new-frontiers---lecture-6",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#limitations-and-new-frontiers---lecture-6",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/15/21 - Limitations and New Frontiers - lecture 6",
    "text": "3/15/21 - Limitations and New Frontiers - lecture 6\nUniversal Approximation Theorem: A feedforward network with a single layer is sufficient to approximate, to an arbitrary precision, any continuous function.\nAva emphasizes importance of training data (e.g. for generalization) and mentions a paper called “Understanding Deep Neural Networks Requires Rethinking Generalization”.\nSome fail examples with dogs colorization (BW -> colors) creating pink zone under the mouth.\nAnd another one with Tesla autopilot. It motivates working on uncertainty in Deep Learning.\n\nwe need uncertainty metrics to assess the noise inherent to the data: aleatoric uncertainty\nwe need uncertainty metrics to assess the network’s confidence in its predictions: epistemic uncertainty\n\nAva cites an example of a real 3D printed turtle designed to fool a classifier from turtle to rifle.\nNew frontier: Encoding Structure into Deep Learning.\nCNN is a nice way to extract features from an image. But not all kind of data can express features in an euclidean way. Graphs is used as a structure for representing data in a lot of cases.\nIt drives us to Graph Convolutional Networks (GCNs). The graph convolutional operator is going to associate weights with each of the edges and apply the weights across the graph and then the kernel is going to be moved to the next node in the graph extracting information about its local connectivity. That local information is going to be aggregated and the NN is going to then learn a function that encodes that local information into a higher level representation.\nNew frontier: Automated Machine Learning & AI.\nUsing a neural architecture search algorithm. At each step the model samples a brand new network. For each layer, defines number of fileters, filet height, width, stride height, width, nbr of fileters, etc. Update RNN controller based on the accuracy of the child network after training.\nFrom autoML to autoAI: an automated complete pipeline for designing and deploying ML and AI models."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#pixels-to-control-learning---software-lab-3",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#pixels-to-control-learning---software-lab-3",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/15/21 - Pixels-to-Control Learning - Software Lab 3",
    "text": "3/15/21 - Pixels-to-Control Learning - Software Lab 3\nThis is about reinforcement learning.\n\n\n\nalt text\n\n\nWe install (apt) xvfb and python-opengl.\nAnd will learn with cartpole and pong.\nStill this issue\n\nUnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[node sequential_8/conv2d_4/Conv2D (defined at :19) ]] [Op:__inference_distributed_function_2442603]\n\nSolved by running\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        # Memory growth must be set before GPUs have been initialized\n        print(e)\nI couldn’t go through the training of Pong agent due to GPU limitation?\n2021-03-15 10:54:19.479775: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at conv_grad_input_ops.cc:1254 : Resource exhausted: OOM when allocating tensor with shape[3944,48,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfcbash"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#evidential-deep-learning-and-uncertainty---lecture-7",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#evidential-deep-learning-and-uncertainty---lecture-7",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/22/21 - Evidential Deep Learning and Uncertainty - lecture 7",
    "text": "3/22/21 - Evidential Deep Learning and Uncertainty - lecture 7"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#bias-and-fairness---lecture-8",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#bias-and-fairness---lecture-8",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "3/29/21 - Bias and Fairness - lecture 8",
    "text": "3/29/21 - Bias and Fairness - lecture 8\nThis starts as a standard lecture about bias.\nI like emphasis about bias that could stand in all stages of AI life cycle:\n\ndata (obviously)\nmodel\ntraining and deployment\nevaluation\ninterpretation\n\nGood explanation about biases due to class imbalance. It develops my intuition about it.\nBalanced batches can be the answer.\nExample weighting is another option using inverse frequency as a weight.\n\nAdversarial learning to mitiage Bias.\nApplication in NLP to complete analogies. He is to she, as doctor is to ?\nSame thing with Learned Latent Structure. (can be used to create fair and representative dataset)"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#learning-for-information-extraction---lecture-9.",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#learning-for-information-extraction---lecture-9.",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "4/15/21 - Learning for Information Extraction - lecture 9.",
    "text": "4/15/21 - Learning for Information Extraction - lecture 9.\nDeep CPCFG for Information Extraction\nLecturer: Nigel Duffy and Freddy Chua, Ernst & Young AI Labs\nFocus is about document intelligence (extract info from business documents)\ne.g. extract information from semi-structured documents such as tax forms (souvenirs ;))"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#taming-dataset-bias---lecture-10",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#taming-dataset-bias---lecture-10",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "4/27/21 - Taming Dataset Bias - lecture 10",
    "text": "4/27/21 - Taming Dataset Bias - lecture 10\nvideo\ndataset bias and training shift\n(from one city to another (summer vs winter), from simulated to real control, from one culture to another)\nCan fix with more data …(can be very expensive if we want to address all combinations) or use unlabeled data ?\n\nAdversarial approach to fool a domain discriminator. (domain discriminator trained to distinguished source and target domains)\nAnother approach is pixel alignment."
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#towards-ai-for-3d-content-creation---lecture-11",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#towards-ai-for-3d-content-creation---lecture-11",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "4/30/21 - Towards AI for 3D Content Creation - lecture 11",
    "text": "4/30/21 - Towards AI for 3D Content Creation - lecture 11\nvideo\n\nSanja Fidler; Professor U. of Toronto and Head of AI at NVIDIA"
  },
  {
    "objectID": "posts/2021-02-05-learning-MIT-6.S191-2021.html#ai-in-healthcare---lecture-12",
    "href": "posts/2021-02-05-learning-MIT-6.S191-2021.html#ai-in-healthcare---lecture-12",
    "title": "Learning: MIT 6.S191 Introduction to Deep Learning - 2021",
    "section": "4/30/21 - AI in Healthcare - lecture 12",
    "text": "4/30/21 - AI in Healthcare - lecture 12\nvideo\n\nKatherine Chou; Director of Research and Innovations, Google"
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "",
    "text": "Un exposé en 8 cours au collège de France de Stéphane Mallat sur les représentations parcimonieuses - 2021.\nCela donne envie d’aller voir ses autres cours:\nA peu près 16 vidéos de 1h30 par cours. Et des notes de cours en pdf."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#le-triangle-régularité-approximation-parcimonie-lecture-1",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#le-triangle-régularité-approximation-parcimonie-lecture-1",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "2/15/21 - Le triangle « Régularité, Approximation, Parcimonie » (lecture 1)",
    "text": "2/15/21 - Le triangle « Régularité, Approximation, Parcimonie » (lecture 1)\nC’est l’introduction du cours. J’apprécie les références historiques et philosphiques partant du rasoir d’Ockam. C’est le principe d’économie ou de parcimonie: le beau, le vrai viendrait du simple.\nLa 1ere fois que j’entends une référence précise sur l’opposition entre biais (erreur sur modèle) et variance (erreur sur données ou mesures)\nEt une invitation à consulter une méthodologie d’analyse de données par Pierre Courtiol en utilisant Kaggle. L’idée d’une approche simple linéaire pour bien comprendre quelles étapes successives à emprunter pour améliorer son approche. Me semble assez orthogonal à ce que peut proposer Jeremy Howard: commencer tôt, overfitting n’est pas un probleme, pas de early stopping, etc."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#approximations-linéaires-et-analyse-de-fourier-lecture-2",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#approximations-linéaires-et-analyse-de-fourier-lecture-2",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "2/10/21 - Approximations linéaires et analyse de Fourier (lecture 2)",
    "text": "2/10/21 - Approximations linéaires et analyse de Fourier (lecture 2)\nJ’ai commencé par ce cours conseillé par Rémi mon pote enseignant chercheur en math. C’est un peu le grand écart avec des méthodes d’enseignement anglo-saxonnes mais ça fait du bien. C’est finalement plus proche de ce que j’ai connu dans ma formation initiale.\nS.Mallat présente les équivalences (sous certaines conditions) entre\n\nRégularité\nApproximation en basse dimension\net représentation parcimonieuse\n\ndans le cadre des approximations linéaires. Il parle des 2 mondes: traitement du signal et analyse de la donnée. Je suis moins intéressé par le 1er monde, mais j’apprécie la piqure de rappel. Je ne me rappelais pas du tout l’importance de l’analyse de Fourier et la construction des bases de L[0,1] par exemple.\nEt il revient sur les singularités, beaucoup d’informations sont portées par les singularités (par exemple les frontières dans une image)\nJe crois bien que je vais me faire toute la session, et sans doute les autres années."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#grande-dimension-et-composantes-principales-lecture-3",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#grande-dimension-et-composantes-principales-lecture-3",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "2/23/21 - Grande dimension et composantes principales (lecture 3)",
    "text": "2/23/21 - Grande dimension et composantes principales (lecture 3)\nDans ce cadre linéaire grande dimension, quelle meilleure base - approche PCA et base Karhunen-Loeve.\nQuid quand on passe en non linéaire.\nRéseau neurone à 1 couche cachée, théoreme de representation universel.\nRetour sur les bases de L²[0,1] qui sont les bases de Fourier en variables complexes.\nPour un passage en dimension q, on remplace n par (n1, …, nq) et la multiplication n*u par le produit scalaire <n, u>.\nEn travaillant sur les équivalences du triangle, il montre pourquoi on est très limité en approximation lineaire quand la dimension augmente.\nEn approximation lineaire, il suffit de prendre les 1ers vecteurs (se limiter à une dimension q) (en base de fourier par exemple) pour avoir une assez bonne approximation. Dans des signaux plus perturbés (avec des singularités) on perd plus d’énergie: il faudrait échantilloner plus fin dans ces zones de singularités et si on dispose d’une base orthonormée il s’agirait non plus de prendre les q 1ers vecteurs mais de prendre ceux d’intéret."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#approximations-non-linéaires-et-réseaux-de-neurones-lecture-4",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#approximations-non-linéaires-et-réseaux-de-neurones-lecture-4",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "3/2/21 - Approximations non linéaires et réseaux de neurones (lecture 4)",
    "text": "3/2/21 - Approximations non linéaires et réseaux de neurones (lecture 4)\nLe triangle (approximation basse dimensions, représentation parcimonieuse, régularité) d’un point de vue non linéaire.\nIci plutôt qu’approximer un signal en prenant les M 1ers coefficients de Fourier (basses dimensions), on va prendre M coefficients mais dépendamment de x. C’est ici qu’on introduit la non-linéarité. L’erreur est alors la queue de distribution des coefficients ordonnés. On veut que l’énergie des plus petits coefficients soit négligeable.\nPas facile d’obtenir cet ordre, on cherche une façon de limiter les coefficients non ordonnés nous donnant une représentation parcimonieuse. En utilisant la nome l\\(\\alpha\\) avec \\[\\alpha\\] petit (inférieur à 2 et proche de 0), on introduit cette décroissance mais cette fois-ci sur les coefficients non ordonnés.\nIntéressant d’avoir des normes convexes, et dans ce cas on ne peut prendre que \\[\\alpha\\]=1. C’est pour ça qu’on voit apparaître partout les normes l1 dans les algorithmes d’apprentissage (norme convexe garantissant une forme de sparsité).\nOn passe aux réseaux de neurones à 1 couche cachée. Et on va basculer dans les notations de x(u) à f(x)., avec x \\[\\epsilon\\] [0, 1]d.\n\nIci on projette f dans l’espace engendré par ces vecteurs { \\[\\rho\\](x.wm+bm) }n<=M.\nOn peut facilement calculer l’erreur quadratique comme l’intégrale sur les x \\[\\epsilon\\] [0, 1]d de la norme l² ( f(x)-ftilde(x) ) et il y a un belle démonstration qui est le théorème d’approximation universelle (démontrée entre 1988 et 1992) qui montre que l’erreur tend vers 0 quand M tend vers l’infini.\nLa démonstration avec \\[\\rho\\] = eia revient à une décomposition d’en Fourier. Et pour d’autres non régularité comme reLu ou sigmoid, il s’agit d’un changement de base.\nEt là on arrive à la malédiction de la dimensionnalité car quand d est grand (disons 1M), les coefficients baissent à une faible vitesse. Que faut-il faire pour battre cette malédiction?\nBaron en 1993 introduit une hypothèse de regularité qui permet de borner l’erreur par un terme qui ne dépend pas de la dimension. C’est donc gagné sauf que l’hypothèse de régularité n’est généralement pas valide dans les cas qui nous intéressent.\nStéphane Mallat, de façon brillante mais est-ce étonnant, explique pourquoi l’approche des mathématiciens est une impasse et pourquoi ce qu’on cherche à faire se ramène à un problème bayésien. Car les problèmes qui nous intéressent (par exemple la classification d’objets, ne va solliciter qu’un minuscule espace (même si de grande dimension) parmi toutes les images possibles). On va donc chercher à caractériser x pour chaque y (classe). (revoir vidéo entre 49’ et 1h03)\nL’enjeu est de caractériser le support qui est beaucoup plus concentré que [0,1]d.\nDonc on va retravailler sur les approximations non linéaires de x, le signal lui-même (et non plus f), et d’essayer de comprendre pourquoi on peut faire beaucoup mieux que la transformée de Fourier et quelle genre de bases vont nous permettre de faire bcp mieux. Une des applications va être la compression, qui va nous amener à étudier la théorie de l’information et la théorie de l’information c’est exactement la théorie probabiliste qui explique ces phénomènes de concentration et les mesure avec l’entropie.\nIntroduction des bases d’ondelettes qui vont permettre de représenter les singularités locales. Les ondelettes sont à la fois localisées (paramètre v) et dilatées (paramètre s). Il faudra à partir de ces ondelettes construire des bases orthogonales pour arriver à des approximations basses dimensions (et garder les grands coefficients)\nOn introduit la notion de régularité locale exprimée avec lipchitz \\[\\alpha\\]. Avec \\[\\alpha\\] <1 pour exprimer les singularités."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#ondelettes-et-échantillonnage-lecture-5",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#ondelettes-et-échantillonnage-lecture-5",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "3/9/21 - Ondelettes et échantillonnage (lecture 5)",
    "text": "3/9/21 - Ondelettes et échantillonnage (lecture 5)\nOn était resté sur une représentation de signaux qui ne présentent pas de régularité uniforme mais qui présentent des singularités que nous voulons capter, ces singularités étant porteuses d’informations importantes (par exemple les contours dans une image). Ces singularités n’étant pas très nombreuses, on peut toujours parler de régularité locale.\nOn va donc utiliser des ondelettes pour décomposer ces signaux, d’où la notion de représentation parcimonieuse, exprimée sur la base d’ondelettes orthonormales. Et enfin en en sélectionnant un petit nombre nous revenons sur nos approximations en basse dimension.\nLe produit scalaire du signal x(u) par l’ondelette \\[\\psi\\]v,s revient à un produit de convolution de x par l’ondelette conjuguée. Ca veut dire que sur les points de singularités les produits scalaires vont être maximisés.\nStéphane Mallat passe un long moment pour nous amener à la construction de ces bases d’ondelettes orthonormales. Il part des bases de Haar puis de Shannon et arrive à une construction plus récente par Yves Meyer en 1986."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#multi-résolutions-lecture-6",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#multi-résolutions-lecture-6",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "3/16/21 - Multi-résolutions (lecture 6)",
    "text": "3/16/21 - Multi-résolutions (lecture 6)\nOn a vu la dernière fois qu’on pouvait construire une base d’ondelette le long des indices de dilatations en 2j.\nOn va voir maintenant qu’on peut translater les ondelettes par des facteurs 2j.n.\nDonc quand j est grand, les échelles sont de plus en plus grande. Et j petit va amener un échantillonnage de plus en plus fin.\n\\[\n\\left\\{ \\Psi_{(j,n)}(u)=\\frac{1}{\\sqrt{2^j}}\\Psi \\left( \\frac{u-2^jn}{2^j} \\right) \\right\\}_{(j, n) \\epsilon \\Z^2}\n\\]\nsont-elles des bases orthonormales. Ensuite on appliquerait les techniques d’approximations consistant à éliminer les petits coefficients.\nLes multi-résolutions sont des espaces linéaires sur lesquels nous allons projeter ces signaux. On va chercher à réduire les dimensions (par ex d’une image) en projetant sur ces espaces emboîtés. Et conserver le maximum d’information.\nUn produit scalaire avec une fonction translatée peut toujours s’écrire comme un produit de convolution (Stéphane Mallat répète souvent cette propriété)\nStéphane Mallat fait ensuite le lien avec les algorithmes en bancs de filtre (cascades de filtrage + échantillonnage).\nDans ces opérations il y a sans arrêt des passages du continu au discret. Par exemple si je prends un signal et que je le projette sur ces espaces je me retrouve avec les coordonnées, qui sont les produits scalaires avec mes \\[\\phi\\]j,n (car base orthogonale), ce qui revient à filtrer et sous échantillonner."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#bases-orthonormales-dondelettes-lecture-7",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#bases-orthonormales-dondelettes-lecture-7",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "3/23/21 - Bases orthonormales d’ondelettes (lecture 7)",
    "text": "3/23/21 - Bases orthonormales d’ondelettes (lecture 7)\nOn repart sur notre triangle. Depuis 2 cours on est sur l’approximation basse dimension.\nStéphane Mallat applique le théorème sur des cas particuliers de la base de Haar, puis de la base de Shannon. Et revient sur la construction d’une base orthonormales avec des ondelettes “optimales”.\n\nQuand on prend le produit scalaire de notre signal f avec les ondelettes, on obtient des résultats presque nuls lorsque le signal est régulier. Et plus on a de moments nuls avec nos ondelettes, plus la régularité est ignorée (l’approximation par projection sur un espace vectorielle des monômes à l’ordre n).\nOn va cascader les projections aj (et les détails dj), et ça va revenir à cascader les filtres (les coefficients et les ondelettes).\nPour cela on calcule les valeurs des aj et dj en fonction de aj-1. On montre que cela s’obtient en filtrant (respectivement avec les \\[\\overline{h}\\] et \\[\\overline{g}\\]) puis en sous-échantillonnant. En cascadant on obtient une série de filtrages, sous-échantillonnages, filtrages, sous-échantillonnages, , etc.\nLes filtrages sont des convolutions. Si h a un support compact, ça va réduire le temps de calcul.(le nombre d’éléments non nuls correspond à la taille du filtre). Le nombre d’opérations pour passer de aL à aL-1, dL-1 est N*2m (où N: nombre de coefficients de aL et m est le nombre de moments nuls)\nLe nombre d’opérations est linéaire, et la constante correspond à la taille des filtres.\nOn peut inverser cet algorithmes (car base o.n.) et la structure emboîtée va nous donner algorithme de reconstruction. On va sur-échantillonner (augmenter d’un facteur 2 en intercalant des 0) et appliquer les filtres g et h, et sommer pour obtenir le résultat.\nDonc en gardant la base fréquence aJ et tous les détails {dj}, on reconstitue aL. (les signaux sur des grilles de plus en plus fines)\nStéphane Mallat finit sur des exemples en 2 dimensions. En 2 dimensions on aura 3 ondelettes à chaque échelle (1 avec les hautes fréquences dans une direction, 1 avec les hautes fréquences dans l’autre direction, et la dernière avec haute fréquence sur les 2 directions (les coins))."
  },
  {
    "objectID": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#parcimonie-et-compression-dimages-lecture-8",
    "href": "posts/2021-02-10-college-de-france-representations-parcimonieuses.html#parcimonie-et-compression-dimages-lecture-8",
    "title": "Learning: College de France - Representations parcimonieuses",
    "section": "3/30/21 - Parcimonie et compression d’images (lecture 8)",
    "text": "3/30/21 - Parcimonie et compression d’images (lecture 8)\nStéphane Mallat propose un survol de tout le cours pour montrer la logique dans laquelle on a évolué.\nEn reprenant le triangle Régularité - Approximation en basse dimension (au cœur du traitement de donnée) - Représentation parcimonieuse. Les équivalences entre régularité et la construction de représentations parcimonieuses permettent de construire des approximations en basse dimension.\nMais on peut les interpréter différemment :\n\nd’un point de vue linéaire : on peut construire des approximations linéaires qui vont correspondre à des formes de régularité et certains types de représentations parcimonieuses (en particulier dans la base de Fourrier quand on a des invariants par translation)\nen prenant un point de vue non linéaire : qui consiste non pas à faire des projections dans des espaces linéaires mais plutôt des projections dans des unions d’espaces linéaires obtenus en sélectionnant de façon libre dans une base orthogonale les plans les plus représentatifs.\n\nIl reprend en détail ce qu’on a vu en repartant de la théorie développée par Fourier (1822 ça ne date pas d’hier). Et reprend les réseaux de neurones à 1 couche cachée.\n\\[\nf_M(x)=\\displaystyle\\sum_{m}w(m)\\rho(\\langle{x,w_m}\\rangle+b_m)\n\\]\nL’entrée est x en dimension d, dans la première couche on calcule des produits scalaires avec les vecteurs \\(v_m\\) qui sont les colonnes d’un opérateur linéaire \\(W_1\\) et ces M produits scalaires vont être regroupés avec un relu (ou toute autre non-régularité) et un biais, et dans la dernière couche on fait une combinaison linéaire pour construire l’approximation. M est le nombre d’éléments dans la couche cachée, peut-on bien approximer f(x) à partir de cette construction ?\nCes réseaux, en prenant comme non-régularité un cosinus, nous font retomber sur des séries de Fourier.\n\\[\nf_M(x)=\\displaystyle\\sum_{\\| v_m \\|<R}w(m) \\cos (\\langle{x,w_m}\\rangle+b_m)\n\\]\nFaire une décomposition avec un réseau de neurone à 1 couche cachée est très similaire à décomposer la fonction dans une base de Fourier. Prendre un relu consisterait à faire un changement de base entre le relu et le cosinus.\nSi on veut approximer une fonction uniformément régulière, il va falloir garder les basses fréquences. Mais \\(x\\) n’est pas en dimension 1 mais en dimension \\(d\\). Les fréquences qu’il va falloir prendre ici sont dans \\(\\Z^d\\), il va falloir garder toutes les fréquences dans une boule de rayon plus petit que \\(R\\). Mais quand on est en dimension \\(q\\), le nombre d’éléments dans une boule plus petit que \\(R\\) va croître comme \\(R^q\\). Donc il va falloir garder énormément d’éléments.\nOn a la possibilité d’approximer n’importe quelle fonction dans \\(L^2\\) avec une erreur qui va décroître vers 0 quand le nombre de termes \\(M\\) tend vers \\(\\infty\\) parce qu’on a une base orthogonale et donc n’importe quelle fonction peut être représentée à partir de la base\n\\[\nf \\in L^2 \\implies \\lim\\limits_{M \\to \\infty}\\| f-f_M \\|=0\n\\]\nC’est le théorème d’approximation universelle.\nPar contre si on a une régularité on peut préciser la vitesse de décroissance de l’erreur et en particulier si ma fonction est \\(\\alpha\\) dérivée dans un espace de Sobolev de degré \\(\\alpha\\), l’erreur va décroître d’autant plus vite que la régularité est grande, parce que les coefficients de Fourier vont décroître, et la vitesse de décroissance dépend de \\(\\alpha/d\\).\n\\[\nf \\in H^\\alpha \\implies \\|f-f_M\\| = o(M^{-\\alpha/d})\n\\]\nC’est la malédiction de la dimensionnalité.\nUne autre approche consiste à reprendre ce cercle d’un point de vue non-linéaire. Au lieu de toujours prendre les mêmes coefficients pour approximer les fonctions qui m’intéressent, je vais adapter les coefficients à la fonction. C’est l’esprit des approximations non-linéaires.\nSi je considère les vecteurs de Fourier, et ses coefficients ont une norme \\(L^p\\) qui converge, pour un \\(p<2\\). Alors on a vu que les coefficients vont décroître à une vitesse qui dépend de \\(p\\). Ca veut dire qu’il y a quelques grands coefficients et beaucoup de petits. Si on choisit les grands coefficients alors on va avoir une erreur qui décroît comme \\(-2/(p+1)\\), l’erreur décroît lorsque \\(M\\) augmente, indépendamment de la dimension.\n\\[\nSparse \\quad Fourier \\quad coefficients: Barron \\quad p<2\n\\\\\n\\displaystyle\\sum_{v \\in \\Z^d} |\\langle{f(x), F_v(x)}\\rangle|^p < \\infty \\implies \\|f-f_M\\|=o(M^{-2/p+1})\n\\]\nno curse. Mais résultat tautologique. Pourquoi cette fonction serait approximable avec quelques coefficients de Fourier. Ça n’explique en rien pourquoi on peut améliorer fortement ce résultat en augmentant le nombre de couche. C’est simple mais ça n’explique pas les performances des réseaux de neurones profonds.\nD’où l’approche par ondelettes.\nEt la nécessité de construire des bases orthogonales d’ondelettes à décroissance rapide. Travaux de Yves Meyer. (en essayant de démontrer que ça n’était pas possible il a réussi à en construire ;)) Et S.Mallat a amélioré cette approche en se basant sur des approches de multi résolutions avec des espaces imbriqués.\nOn peut construire ces ondelettes en cascadant des filtres à différentes échelles (passe bas et passe bande à différentes échelles).\nI.Daubechies a montré qu’on peut construire des ondelettes à support compact.\nY.Meyer a montré ce que ça donnait en dimension 2 (et c’est généralisable en dimension q) avec 3 ondelettes."
  },
  {
    "objectID": "posts/2021-02-16-conda-and-jupyter-tips.html",
    "href": "posts/2021-02-16-conda-and-jupyter-tips.html",
    "title": "Conda and jupyter tips",
    "section": "",
    "text": "I manage all my python environments with conda from miniconda.\n\n\nHowever I don’t have a strong process to keep track of my environment specifications. Usually I manually create an env.txt file under my projects. Keeping all commands I have used to create that environment.\n\n!cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/env\\ \\ mit_6S191.txt\n\nenv_name: mit_6S191\nlibraries: python 3.7, tensorflow 2\n\n\nInstallation commands:\nconda create -n mit_6S191 python=3.7\nconda activate mit_6S191\n\nconda install tensorflow tensorflow-gpu\nconda install -c conda-forge jupyter_contrib_nbextensions\nconda install matplotlib numpy opencv\nconda install -c pytorch torchvision\nconda install nb_conda\n\n\nWhat happens if I add packages in that environment. Or want to use that environment in another project. I have to remember the link between env name and project name.\nThat is not robust.\n\n\n\nKeeping a yml file could be a solution to keep track of environment specifications. It doesn’t answer to my last concern though (linking env name and project name)\nBut there is a limitation linked with channels.\n\n!conda env export --from-history\n\nname: fastai\nchannels:\n  - defaults\ndependencies:\n  - python=3.8\n  - fastai\n  - jupyter\n  - jupyter_contrib_nbextensions\n  - fastbook\nprefix: /home/explore/miniconda3/envs/fastai\n\n\nIn that example, fastai package should come from fastai channel but conda doesn’t keep that information.\nUsing\nconda install -n my_env rdkit::rdkit\ncould be an option.\n\n\n\nSince conda keeps active environment in env variable CONDA_DEFAULT_ENV, we can automatically create up-to-date yml file.\n\n!echo $CONDA_DEFAULT_ENV\n\nfastai\n\n\n\n!conda env export --from-history > ~/temp/env_`echo $CONDA_DEFAULT_ENV`.yml\n!ls ~/temp/env_`echo $CONDA_DEFAULT_ENV`.yml\n\n/home/explore/temp/env_fastai.yml\n\n\nBut for it to be usable, I will have to install package using the <channel>::<package> way.\n\n!cat /home/explore/git/guillaume/mit_6S191_Intro_to_deep_learning/create_yml.sh\n\n#!/bin/bash\nconda env export --from-history > env_`echo $CONDA_DEFAULT_ENV`.yml"
  },
  {
    "objectID": "posts/2021-02-16-conda-and-jupyter-tips.html#jupyter-extensions",
    "href": "posts/2021-02-16-conda-and-jupyter-tips.html#jupyter-extensions",
    "title": "Conda and jupyter tips",
    "section": "Jupyter extensions",
    "text": "Jupyter extensions\nI have already explained how to install jupyter extensions and the one I use. update jupyter to include extensions"
  },
  {
    "objectID": "posts/2021-02-16-conda-and-jupyter-tips.html#nb_conda",
    "href": "posts/2021-02-16-conda-and-jupyter-tips.html#nb_conda",
    "title": "Conda and jupyter tips",
    "section": "nb_conda",
    "text": "nb_conda\nThis is usefull to switch from environment to another without having to stop/restart jupyter."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "",
    "text": "A course by Thomas Simonini\nSyllabus (from 2018)\nCourse introduction (from 2020)\nEverything available in github\nI appreciate the effort to update examples, and some 2018 implementations became obsolete. Historical Atari VC2600 games are now Starcraft 2 or minecraft, and news series on building AI for video games in Unity and Unreal Engine.."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-1---an-introduction-to-deep-reinforcement-learning",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-1---an-introduction-to-deep-reinforcement-learning",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(2/19/21) - Chapter 1 - An Introduction to Deep Reinforcement Learning?",
    "text": "(2/19/21) - Chapter 1 - An Introduction to Deep Reinforcement Learning?\nPrevious version from 2018: What is Deep Reinforcement Learning? is quite interesting. With 3 parts:\n\nWhat Reinforcement Learning is, and how rewards are the central idea\nThe three approaches of Reinforcement Learning\nWhat the “Deep” in Deep Reinforcement Learning means\n\n\nRewards, long-term future reward, discount rate.\n\nEpisodic (starting and ending point) vs Continuous (e.g. stock trading) tasks.\nWay of learning: Monte Carlo (MC: rewards collected at the end of an episode) vs Temporal Difference (TD: estimate rewards at each step)\n\nExploration/Exploitation trade off. Will see later different ways to handle that trade-off.\n\n\nThree approaches to Reinforcement Learning\nThese are value-based, policy-based, and model-based.\n\nValue Based\nIn value-based RL, the goal is to optimize the value function V(s).\nThe value function is a function that tells us the maximum expected future reward the agent will get at each state.\n\n\n\nPolicy Based\nIn policy-based RL, we want to directly optimize the policy function π(s) without using a value function.\nThe policy is what defines the agent behavior at a given time.\nWe have two types of policy:\n\nDeterministic: a policy at a given state will always return the same action.\nStochastic: output a distribution probability over actions.\n\n\n\n\nModel Based\nIn model-based RL, we model the environment. This means we create a model of the behavior of the environment. Not addressed in this course.\n\n\n\nDeep Reinforcement Learning\nIn Q-learning, we keep a table of actions to take for each state (based on reward). This can be huge.\nDeep Learning allows to approximate this Q function.\n\nUpdated version from 2020 (and video version)\nThis is a good starting point, well explained.\nReinforcement Learning is just a computational approach of learning from action.\nA formal definition\n\nReinforcement learning is a framework for solving control tasks (also called decision problems) by building agents that learn from the environment by interacting with it through trial and error and receiving rewards (positive or negative) as unique feedback.\n\nSome explanations about observations (partial description) vs states (fully observed envt). Only differs in implementation, all theoretical background stays the same.\nAction space where we can distinguish discrete (e.g. fire, up) actions from continuous (e.g. turn 23deg) ones.\nReward part is the same as the one from 2018. With cheese, mouse, maze example.\nEpisodic and continuous tasks part is the same as the one from 2018.\nExploration/Exploitation trade-off is explained the same way + an additional example taken from berkley - CS 294-112 - Deep Reinforcement Learning course. I want to learn more about this course!\nAbout solving RL problems, it is now presented as 2 main approaches:\n\npolicy-based methods\nvalue-based methods\n\n\nAnd bedore to explain that, nice presentation of what is a policy \\(\\pi\\). Solving RL problem is to find that optimal policy: directly with policy-based method, indirectly (through value function) with value-based method.\nThere is an explanation about different types of policy: deterministic and stochastic.\nAnd that we use deep neural networks to estimate the action to take (policy based) or to estimate the value of a state (value based). Thomas suggests to go further with deep learning with MIT 6.S191, which is the one (version 2021) I follow these days."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-2---part-1---q-learning-lets-create-an-autonomous-taxi",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(2/19/21) - Chapter 2 - part 1 - Q-Learning, let’s create an autonomous Taxi",
    "text": "(2/19/21) - Chapter 2 - part 1 - Q-Learning, let’s create an autonomous Taxi\nAnd in video (I like to read + watch the video at the same time)\nHere in Step 2 we focus on a value-based method: Q-learning. And what is seen in part 1 and 2:\n\n\nValue-based method\nRemember what we mean in value-based method\n\nyou don’t train your policy, you define a simple function such as greedy function to select the best association State-Action, so the best action.\n\n\nBellman equation\neach value as the sum of the expected return, which is a long process. This is equivalent to the sum of immediate reward + the discounted value of the state that follows.\n\n\n\nMonte Carlo vs Temporal Difference\nAnd then an explanation about 2 types of method to learn a policy or a value-function:\n\nMonte Carlo: learning at the end of the episode. With Monte Carlo, we update the value function from a complete episode and so we use the actual accurate discounted return of this episode.\nTD learning: learning at each step. With TD learning, we update the value function from a step, so we replace Gt that we don’t have with an estimated return called TD target. (chich is the immediate reward + the discounted value of the next state)\n\n\nIt was not clear to me that these methods could be used for policy-based approach. It is now!"
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-2---part-2---q-learning-lets-create-an-autonomous-taxi",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(2/24/21) - Chapter 2 - part 2 - Q-Learning, let’s create an autonomous Taxi",
    "text": "(2/24/21) - Chapter 2 - part 2 - Q-Learning, let’s create an autonomous Taxi\nBut the video is not yet available.\nWhat is Q-Learning?\nQ-Learning is an off-policy value-based method that uses a TD approach to train its action-value function:\n\n“Off-policy”: we’ll talk about that at the end of this chapter.\n“Value-based method”: it means that it finds its optimal policy indirectly by training a value-function or action-value function that will tell us what’s the value of each state or each state-action pair.\n“Uses a TD approach”: updates its action-value function at each step.\n\nQ stands for quality (quality of action). After training we’ll get the optimal Q-function.\nWhen choosing an action, we have to balance between exploration and exploitation with \\[\\epsilon\\] - greedy:\n\nBut at beginning Q table is not trained yet so we have to increase exploitation. It is done with some decreasing \\[\\epsilon\\].\n\nThe Q-learning algorithm is a 4-step process:\n\nstep1: Q-Table init\nstep2: Choose action (\\[\\epsilon\\] - greedy strategy)\nstep3: Perform action At and get Rt+1 and St+1\nstep4: Update Q(St, At)\n\n\n\n\nUpdate Q(St, At)\n\n\nWhy it is called off-policy? Because we don’t have the same logic to select action (\\[\\epsilon\\] - greedy) and update Q (greedy).\nWith On-policy: we use the same policy for acting and updating. Sarsa is such an algorithm.\n\nNice and simple manual example with mouse, cheese in a maze. We run Q-learning and make all calculation by hands.\n\n \n\nimplement with numpy+gym this algorithm should be a nice exercise.\n\nThere is an exercise to implement a taxi, within this notebook at colab google. Taxi V3 is an env from opengym."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#back-to-2018---chapter-3---deep-q-learning-with-doom",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#back-to-2018---chapter-3---deep-q-learning-with-doom",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom",
    "text": "(3/3/21) - back to 2018 - Chapter 3 - Deep Q-learning with Doom\nArticle, Notebook, Video\nWe’ll create an agent that learns to play Doom. Doom is a big environment with a gigantic state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient at all.\nThe best idea in this case is to create a neural network that will approximate, given a state, the different Q-values for each action.\n\n\nAddresses pb of temporal limitation: get multiple frames to have sense of motion.\nVideo is nice because it goes from start and follows closely all steps.\nI wil try to implement in my own by creating an environment and running under a clone of Deep_reinforcement_learning_Course Thomas’s repo\nHere at Deep Q learning with Doom.ipynb\nI had to switch to tensorflow-gpu 1.13. Manage some cuda memory issue. But then was able to run it.\nHowever as Thomas says, I should do it step by step on my own."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-4-improvements-in-deep-q-learning-v1",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-4-improvements-in-deep-q-learning-v1",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(3/10/21) - Chapter 4: Improvements in Deep Q Learning V1",
    "text": "(3/10/21) - Chapter 4: Improvements in Deep Q Learning V1\nArticle, Notebook, Video\nfour strategies that improve — dramatically — the training and the results of our DQN agents:\n\nfixed Q-targets\ndouble DQNs\ndueling DQN (aka DDQN)\nPrioritized Experience Replay (aka PER)\n\nfixed Q-targets to avoid chasing a moving target\n\nUsing a separate network with a fixed parameter (let’s call it w-) for estimating the TD target.\nAt every \\[\\Tau\\] step, we copy the parameters from our DQN network to update the target network.\n\n\nImprovements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed…\nImplementation\nImplementing fixed q-targets is pretty straightforward:\n\nFirst, we create two networks (DQNetwork, TargetNetwork)\nThen, we create a function that will take our DQNetwork parameters and copy them to our TargetNetwork\nFinally, during the training, we calculate the TD target using our target network. We update the target network with the DQNetwork every \\[\\Tau\\] step (\\[\\Tau\\] is an hyper-parameter that we define).\n\ndouble DQNs to handle overestimating of Q-values (at the beginning of training, taking the maximum q value (which is noisy) as the best action to take can lead to false positives)\nwe move from this TD target logic\n\nto the use of 2 networks\n\nuse our DQN network to select what is the best action to take for the next state (the action with the highest Q value).\nuse our target network to calculate the target Q value of taking that action at the next state.\n\n\nImplementation\n\nDueling DQN (aka DDQN)\nbased on this paper Dueling Network Architectures for Deep Reinforcement Learning.\nWith DDQN, we want to separate the estimator of these two elements, using two new streams:\n\none that estimates the state value V(s)\none that estimates the advantage for each action A(s,a)\n\n\nand this can be combined with Prioritized experience replay.\nThis is nicely explained in this article. DDQN explanation is clearer than Thomas’.\nThe key here is to deal efficiently with experiences. When treating all samples the same, we are not using the fact that we can learn more from some transitions than from others. Prioritized Experience Replay (PER) is one strategy that tries to leverage this fact by changing the sampling distribution.\nI guess there are several options to manage this prioritization (we would prefer transitions that do not fit well to our current estimate of Q function). And a key aspect is the performance of this selection. One implementation is SumTree.\nI have to see full implementation in the notebook to fully understand the logic.\nAbout the video\nThomas has insisted about the importance to master these architecture (DQN then DDQN, etc) before going further with state of the art architectures (Policy Gradient, PPO…)\nApproach in videos is now different. In previous videos it was about explaining articles. Now it is more turned to implementation details based on notebooks.\nThomas has given a reference to Arthur Juliani who is a senior ML engineer at Unity. I would like to browse though this reference and see what can be done.\nShould follow video and run/update notebook in //."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-5-policy-gradients-v1",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-5-policy-gradients-v1",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(3/17/21) - Chapter 5: Policy Gradients V1",
    "text": "(3/17/21) - Chapter 5: Policy Gradients V1\nArticle, Notebook, Video\nIn policy-based methods, instead of learning a value function that tells us what is the expected sum of rewards given a state and an action, we learn directly the policy function that maps state to action (select actions without using a value function).\n3 main advantages to use Policy Gradients vs Q learning:\n\nconvergence - have better convergence properties\neffective in high dimension, or with continuous actions\nstochastic policy - no need for exploration,/exploitation tradeoff\n\nBut can be longer to train.\nPolicy search\nWe can dfine our policy as the probability distribution of actions (for a given state)\n\nAnd how good is this policy? Measured with J(\\[\\theta\\])\n\nWe must find \\[\\theta\\] to maximize J(\\[\\theta\\]). How?\n2 steps:\n\nMeasure the quality of a π (policy) with a policy score function J(θ)\nUse policy gradient ascent to find the best parameter θ that improves our π.\n\nthe Policy Score function J(θ)\n3 ways (maybe more)\nCalculate the mean of the return from the first time step (G1). This is the cumulative discounted reward for the entire episode.\n\nIn a continuous environment, we can use the average value, because we can’t rely on a specific start state. Each state value is now weighted (because some happen more than others) by the probability of the occurrence of the respected state.\n\nThird, we can use the average reward per time step. The idea here is that we want to get the most reward per time step.\n\nPolicy gradient ascent\nbecause we want to maximize our Policy score function\n\nThe solution will be to use the Policy Gradient Theorem. This provides an analytic expression for the gradient ∇ of J(θ) (performance) with respect to policy θ that does not involve the differentiation of the state distribution. (using likelihood ratio trick)\n\nIt gives\n\nR(\\[\\tau\\]) is like a scalar value score.\nImplementation\nAs with the previous section, this is good to watch the video at the same time.\nAnd now this is the implementation in\ndoom deathmatch notebook\n\nas with Pong, we stack frames to understand dynamic with deque.\nEven with GPU growth setup, I run an error after the 1st epoch.\n==========================================\nEpoch:  1 / 5000\n\nNumber of training episodes: 15\nTotal reward: 7.0\nMean Reward of that batch 0.4666666666666667\nAverage Reward of all training: 0.4666666666666667\nMax reward for a batch so far: 7.0\nResourceExhaustedError: OOM when allocating tensor with shape[5030,32,24,39] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n     [[{{node PGNetwork/train/gradients/PGNetwork/conv2/conv2/Conv2D_grad/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\nI have to reduce batch size (to 1000) to make it work.\nAnd I can monitor gpu memory consumption with watch nvidia-smi\n\nor we can use gpustat -i 2\n[0] Quadro RTX 4000 | 59’C, 34 %, 39 W | 7819 / 7982 MB | explore(6729M) gdm(162M) explore(388M) explore(282M) explore(86M) explore(89M) explore(3M)"
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-6-advantage-actor-critic-a2c-and-asynchronous-advantage-actor-critic-a3c-v1",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1",
    "text": "(3/19/21) - Chapter 6: Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C) V1\nArticle, Notebook, Video\n“hybrid method”: Actor Critic. We’ll using two neural networks:\n\nan Actor that controls how our agent behaves (policy-based)\na Critic that measures how good the action taken is (value-based)\n\n\nActor is using a policy function \\[\n\\pi(s, a, \\theta)\n\\] Critic is using a value function\n\\[\n\\widehat{q}(s,a,w)\n\\] Which means 2 sets of weights to be optimized separately \\[\\theta\\] and w.\n\nWe can use advantage function to stabilize learning:\n\n\nTwo different strategies: Asynchronous or Synchronous\nWe have two different strategies to implement an Actor Critic agent:\n\nA2C (aka Advantage Actor Critic)\nA3C (aka Asynchronous Advantage Actor Critic)\n\nHere we focus on A2C.\n(3/22/21) - Implementation and video\nIt is a little bit confusing. I won’t run it. I would have liked a more pregressive approach and to understand all steps Thomas did to get to that final implementation."
  },
  {
    "objectID": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-7-proximal-policy-optimization-ppo-v1",
    "href": "posts/2021-02-19-Deep Reinforcement Learning Course by Thomas Simonini.html#chapter-7-proximal-policy-optimization-ppo-v1",
    "title": "Practicing: Deep Reinforcement Learning Course by Thomas Simonini",
    "section": "(3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1",
    "text": "(3/24/21) - Chapter 7: Proximal Policy Optimization PPO V1\nArticle, Notebook\nThe central idea of Proximal Policy Optimization is to avoid having too large policy update. (we use a ratio that will tells us the difference between our new and old policy and clip this ratio from 0.8 to 1.2)\nClipped Surrogate Objective Function\n\nWe will penalize changes that lead to a ratio that will away from 1 (in the paper ratio can only vary from 0.8 to 1.2). By doing that we’ll ensure that not having too large policy update because the new policy can’t be too different from the older one.\n2 implementations are known TRPO (Trust Region Policy Optimization) and PPO clip. TRPO being complex and costly, we focus on PPO:\n\nAnd the final loss will be:\n\nNow the implementation\nBy looking at the implementation, I ran into Stable baselines3.\nThis is a major update of Stable Baselines based on pytorch. It seems interesting!\nI like this comment from Stable Baselines3 in the v1.0 blog post:\n\nMotivation\nDeep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often difficult to reproduce. A major challenge is that small implementation details can have a substantial effect on performance – often greater than the difference between algorithms. It is particularly important that implementations used as experimental baselines are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements.\nTo help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the OpenAI Gym interface.\n\nI will create a new blog entry about Stable Baselines3.\nas for previous notebook, I need to purchase Sonic2-3 to make it worked. Not for now maybe later."
  },
  {
    "objectID": "posts/2021-02-26-logbook-Februrary.html",
    "href": "posts/2021-02-26-logbook-Februrary.html",
    "title": "Logbook for February 21",
    "section": "",
    "text": "This is a test. I will try to keep words on a monthly (this page), weekly (per heading), daily basis. Just some short entries with possibly some links to more detailed materials."
  },
  {
    "objectID": "posts/2021-02-26-logbook-Februrary.html#week-8---feb-21",
    "href": "posts/2021-02-26-logbook-Februrary.html#week-8---feb-21",
    "title": "Logbook for February 21",
    "section": "Week 8 - Feb 21",
    "text": "Week 8 - Feb 21\nMonday 2/22\nTo develop knowledge about RL, here is my learning process on a weekly basis.\nMonday MIT 6S191\nTuesday College de France\nWednesday Deep Reinforcement Learning by Thomas Simonini\nFriday RL readings: papers, books, …\nFriday 2/26\nblog fastpages - setup automated upgrade (instructions from _fastpages_docs) v2.1.42\nblog fastpages - display image preview (update of _config.yml)\nRL - understood differences between Q-learning and Sarsa algorithms in end of step2 part2\nRL - Sutton book (p200-220) - eligibility traces, and start of planning vs learning"
  },
  {
    "objectID": "posts/2021-03-01-logbook-March.html",
    "href": "posts/2021-03-01-logbook-March.html",
    "title": "Logbook for March 21",
    "section": "",
    "text": "Monday 3/1\nMIT 6S191 Deep Generative Modeling (lecture 4) - vaes and gans.\nMIT 6S191 De-biasing Facial Recognition Systems (lab 2): CNN, VAE, DB-VAE\nTuesday 3/2\nCollege de France Approximations non linéaires et réseaux de neurones (lecture 4)\nRL Course by David Silver lecture 1 - intro (22’/88’)\nFuture of Manufacturing@MIT - interesting landscape about Manufacturing and AI\nWednesday 3/3\nInterpretable Machine Learning by Christoph Molnar. LIME reading to understand context of local surrogate models. SHAP chapter using Janus data.\nDeep Reinforcement Learning by Thomas Simonini (Chapter 3 v1) on DQN with temporal limitation using LSTM, and experience replay. (replay buffer)\nThursday 3/4\nInterpretable Machine Learning by Christoph Molnar. PDP chapter using Janus data.\nFriday 3/5\nRL - Sutton book (p220-223) - full vs sample backups, trajectory sampling, heuristic search\nRL - Sutton book (p223+) - start of Approximate Solution Methods, why to use NN."
  },
  {
    "objectID": "posts/2021-03-01-logbook-March.html#week-10---mar-21",
    "href": "posts/2021-03-01-logbook-March.html#week-10---mar-21",
    "title": "Logbook for March 21",
    "section": "Week 10 - Mar 21",
    "text": "Week 10 - Mar 21\nMonday 3/8\nMIT 6S191 Deep Reinforcement Learning. Q-learning vs Policy Gradient.\nTuesday 3/9\nCollege de France Ondelettes et échantillonnage (lecture 5)\nRL Course by David Silver Introduction to Reinforcement Learning (lecture 1)\nInstallation of clustergit to detect local (=uncommited or unpushed) changes in repo\nWednesday 3/10\nDeep Reinforcement Learning by Thomas Simonini (Chapter 4 v1) on four strategies to improve DQN (fixed Q-targets, double DQN, dueling DQN (DDQN), Prioritized Experience Replay (PER))\nt-SNE using Janus data.\nRL Course by David Silver Markov Decision Processes (lecture 2)\nFriday 3/12\nRL - Sutton book (p287-352) - Applications and case studies, end of the book\nRL Course by David Silver Planning by Dynamic Programming (lecture 3)"
  },
  {
    "objectID": "posts/2021-03-01-logbook-March.html#week-11---mar-21",
    "href": "posts/2021-03-01-logbook-March.html#week-11---mar-21",
    "title": "Logbook for March 21",
    "section": "Week 11 - Mar 21",
    "text": "Week 11 - Mar 21\nMonday 3/15\nMIT 6S191 Limitations and New Frontiers.\nMIT 6S191 Pixels-to-Control Learning (lab 3): Cartpole and Pong\nRL Course by David Silver Model-Free Prediction (lecture 4)\nTuesday 3/16\nCollege de France Multi-résolutions (lecture 6)\nWednesday 3/17\nDeep Reinforcement Learning by Thomas Simonini (Chapter 5 v1) - Policy Gradient\nThursday 3/18\nDeep Reinforcement Learning by Thomas Simonini (Chapter 5 v1) - Policy Gradient notebooks\nRL Course by David Silver Model-Free Control (lecture 5)\nFriday 3/19\nDeep Reinforcement Learning by Thomas Simonini (Chapter 6 v1) - Advantage Actor Critic (A2C) and Asynchronous Advantage Actor Critic (A3C)\nCollege de France - l’apprentissage profond par Yann Lecunn 2016 - Pourquoi l’apprentissage profond ?"
  },
  {
    "objectID": "posts/2021-03-01-logbook-March.html#week-12---mar-21",
    "href": "posts/2021-03-01-logbook-March.html#week-12---mar-21",
    "title": "Logbook for March 21",
    "section": "Week 12 - Mar 21",
    "text": "Week 12 - Mar 21\nMonday 3/22\nMIT 6S191 Evidential Deep Learning and Uncertainty (lecture 7).\nDeep Reinforcement Learning by Thomas Simonini (v1 Part 5) - Advantage Actor Critic (A2C) - implementation and video\nTuesday 3/23\nCollege de France Bases orthonormales d’ondelettes (lecture 7)\nWednesday 3/24\nDeep Reinforcement Learning by Thomas Simonini (Chapter 7 v1) - Proximal Policy Optimization PPO\nStable baselines 3 - init and 1st tutorial\nThursday 3/25\nsetup headless raspberry pi to bridge wifi (tethering from phone) to ethernet (to wifi-router)\nStable baselines 3 - finalize init and go through documentation\nCreate a patch for a github project (by forking and pulling request)\nFriday 3/26\nStable baselines 3 - Documentation > Examples\nRename my branches named master to main"
  },
  {
    "objectID": "posts/2021-03-01-logbook-March.html#week-13---mar-21",
    "href": "posts/2021-03-01-logbook-March.html#week-13---mar-21",
    "title": "Logbook for March 21",
    "section": "Week 13 - Mar 21",
    "text": "Week 13 - Mar 21\nMonday 3/29\nMIT 6S191 Bias and Fairness (lecture 8).\nWednesday 3/31\nCollege de France Parcimonie et compression d’images (lecture 8)"
  },
  {
    "objectID": "posts/2021-03-03-gpg-linux.html",
    "href": "posts/2021-03-03-gpg-linux.html",
    "title": "Use of gpg under linux",
    "section": "",
    "text": "from best ways to encrypt files on linux"
  },
  {
    "objectID": "posts/2021-03-03-gpg-linux.html#gpg",
    "href": "posts/2021-03-03-gpg-linux.html#gpg",
    "title": "Use of gpg under linux",
    "section": "gpg",
    "text": "gpg\n\nsetup the key\ngpg --gen-key\nand enter a strong passphrase.\n\n\nexport public key\ngpg --armor --output mypubkey.gpg --export <E-mail that you registered>\n\n\nimport from windows box\ngpg --import mypubkey.gpg\n\n\nencrypt files from windows box\ngpg --output test.txt.gpg --encrypt --recipient <Receiver's E-Mail ID> test.txt\n\n\ndecrypt files on linux box\ngpg --output test.txt --decrypt test.txt.gpg"
  },
  {
    "objectID": "posts/2021-03-03-gpg-linux.html#find-gpg-tmpfs",
    "href": "posts/2021-03-03-gpg-linux.html#find-gpg-tmpfs",
    "title": "Use of gpg under linux",
    "section": "find + gpg + tmpfs",
    "text": "find + gpg + tmpfs\nencrypt from Windows\nfind . -name 'df_76*.csv' -exec gpg --output {}.gpg --encrypt --recipient guillaume.ramelet@michelin.com {} \\;\ndecrypt from Linux\nThere should be better ways to do it.\nHere is my process:\n\nBefore starting: call mount_decrypt.sh. It mounts a tmpfs in secured_data/data, and decrypt all gpg files to this directory\n\nAfter work is done: call umount_decrypt.sh\n\ngpg_decrypt.sh\n#!/bin/bash\ngpg_name=\"$1\"\nsrc_name=${gpg_name%.*}\nTARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data\necho \"gpg decrypt $gpg_name -> $src_name\"\ngpg --output $TARGET_DATA/$src_name --decrypt $gpg_name(base)\nmount_decrypt.sh\n#!/bin/bash\nGPG_DEC_CMD=/home/explore/git/guillaume/d059/secured_data/gpg_decrypt.sh\nTARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data\nsudo mount -t tmpfs -o size=1G tmpfs $TARGET_DATA\ncd /media/explore/CHACLEF/janus\nfind . -name 'df_76*.csv.gpg' -exec $GPG_DEC_CMD {} \\;\numount_decrypt.sh\n#!/bin/bash\nTARGET_DATA=/home/explore/git/guillaume/d059/secured_data/data\nsudo umount $TARGET_DATA"
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html",
    "href": "posts/2021-03-09-clustergit.html",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "",
    "text": "Very basic question to help keep my repo clean."
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#installation-clustergit",
    "href": "posts/2021-03-09-clustergit.html#installation-clustergit",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "Installation clustergit",
    "text": "Installation clustergit\nclustergit seems a good candidate\n\ncd ~/Applications\ngit clone git@github.com:mnagel/clustergit.git\n# add export PATH=\"$PATH:$HOME/Applications/clustergit\" to ~.bashrc\nsource ~.bashrc\nor using .local/bin\ncd ~/Applications/\ngit clone git@github.com:castorfou/clustergit.git\ncd ~\nmkdir -p .local/bin\ncd .local/bin/\nln -s ~/Applications/clustergit/clustergit .\nsource .profile"
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#usage-clustergit",
    "href": "posts/2021-03-09-clustergit.html#usage-clustergit",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "Usage clustergit",
    "text": "Usage clustergit\nclustergit status\n$ clustergit \nScanning sub directories of .\n./Deep-Reinforcement-Learning-Hands-On  : Changesn .    (1/17)\n./Deep_reinforcement_learning_Course    : Changes\n./ReinforcementLearning_references      : On branch main, Untracked files\n./blog                                  : Untracked files\n./d059                                  : On branch main, Changes\n./data-scientist-skills                 : Clean\n./deeplearning_specialization           : Clean\n./fastai                                : Changes\n./fastai_experiments                    : Changes\n./fastbook                              : Changes\n./gan_specialization                    : Clean\n./hello_nbdev                           : Clean\n./introduction-reinforcement-learning-david-silver: On branch main, Untracked files\n./mit_600.2x Introduction to Computational Thinking and Data Science: Clean\n./mit_6S191_Intro_to_deep_learning      : On branch main, No Changes\n./pytorch_tutorial                      : On branch main, Changes\n./squeezebox                            : On branch main, No Changes\nDone\nclustergit status (detailed)\n$ clustergit -v\n[...]\n---------------- ./squeezebox -----------------\nrunning  LC_ALL=C git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n\n./squeezebox                            : On branch main, No Changes\n---------------- ./squeezebox -----------------\nDone\nclustergit status (less detailed: hide Clean)\n$ clustergit -H\nScanning sub directories of .\n./d059                                  : On branch main, Changes\n./fastai                                : Changes\n./fastai_experiments                    : Changes\n./fastbook                              : Changes\n./introduction-reinforcement-learning-david-silver: On branch main, Untracked files\n./mit_6S191_Intro_to_deep_learning      : On branch main, No Changes\n./pytorch_tutorial                      : On branch main, Changes\n./squeezebox                            : On branch main, No Changes\nDone\nClean vs On branch main, No Changes\nseems related to branch name. If branch is named master, then clean is displayed.\n(Mar/25 21) I have just changed clustergit to have main as default branch name instead of master (github having set main as the new standard)\nRename everything from master to main\nGit pull, push\nI am not sure I will use it. But allows to recursively launch pull commands to update repos (if no local changes)"
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#rename-branches-from-main-to-master",
    "href": "posts/2021-03-09-clustergit.html#rename-branches-from-main-to-master",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "Rename branches from main to master",
    "text": "Rename branches from main to master\nRenaming a branch from github website.\nRename branch main to master from github website\n\nUpdate local clones\ngit branch -m main master\ngit fetch origin\ngit branch -u origin/master master"
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#rename-branches-from-master-to-main-i-know",
    "href": "posts/2021-03-09-clustergit.html#rename-branches-from-master-to-main-i-know",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "Rename branches from master to main (I know)",
    "text": "Rename branches from master to main (I know)\nRenaming a branch from github website.\nRename branch master to main from github website\n\nUpdate local clones\ngit branch -m master main\ngit fetch origin\ngit branch -u origin/main main"
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#rabbitvcs",
    "href": "posts/2021-03-09-clustergit.html#rabbitvcs",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "RabbitVCS",
    "text": "RabbitVCS\nFrom this page\nInstallation\nsudo apt install rabbitvcs-nautilus\nResult\n\nThese overlay icons are not automatically updated (have to hit Ctrl-F5, it is a cache issue?) Which is not a surprise: number of actions are fired based on file modifications, and here status (commited, pushed) is not at all linked to file modifications. The system doesn’t know that overlay icon should be changed because file was not touched."
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#git-nautilus-icons",
    "href": "posts/2021-03-09-clustergit.html#git-nautilus-icons",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "git-nautilus-icons",
    "text": "git-nautilus-icons\nJust to check if it works better than RabbitVCS regarding overlay icon cache issue.\nNo I didn’t manage to make it work. Back to RabbitVCS."
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#activate-git-with-globalprotect",
    "href": "posts/2021-03-09-clustergit.html#activate-git-with-globalprotect",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "Activate git with GlobalProtect",
    "text": "Activate git with GlobalProtect\nmove from ssh to https, keeping password\n$ git remote -v\norigin  git@github.com:castorfou/guillaume_blog.git (fetch)\norigin  git@github.com:castorfou/guillaume_blog.git (push)\nmove to https://github.com/castorfou/guillaume_blog.git\ngit remote set-url origin https://github.com/castorfou/guillaume_blog.git\n\nMake Git store the username and password and it will never ask for them.\n\ngit config --global credential.helper store\n\nSave the username and password for a session (cache it);\n\ngit config --global credential.helper cache\nand to activate trace\n$ GIT_TRACE_PACKET=1 GIT_TRACE=1 GIT_CURL_VERBOSE=1 git fetch\nwe can enrich certificates with Global Protect CA\n~/anaconda3/ssl$ sudo cp certPG.pem /etc/ssl/certs/"
  },
  {
    "objectID": "posts/2021-03-09-clustergit.html#add-a-ca-certificate-in-ubuntu",
    "href": "posts/2021-03-09-clustergit.html#add-a-ca-certificate-in-ubuntu",
    "title": "Git - How to find all unpushed commits for all projects in a directory?",
    "section": "Add a ca-certificate in ubuntu",
    "text": "Add a ca-certificate in ubuntu\n\nGo to /usr/local/share/ca-certificates/\nCreate a new folder, i.e. sudo mkdir school\nCopy the . crt file into the school folder.\nMake sure the permissions are OK (755 for the folder, 644 for the file)\nRun sudo update-ca-certificates\n\nWe should see effects in /etc/ssl/certs\n/etc/ssl/certs$ ll -tr\n[..]\nlrwxrwxrwx 1 root root     86 mars  24 10:02  cert_M_X5C_sase-net-sslfwd-trust-ca.pem -> /usr/local/share/ca-certificates/globalprotect/cert_M_X5C_sase-net-sslfwd-trust-ca.crt\nlrwxrwxrwx 1 root root     39 mars  24 10:02  0dc7de9e.0 -> cert_M_X5C_sase-net-sslfwd-trust-ca.pem"
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "",
    "text": "This classic 10 part course, taught by Reinforcement Learning (RL) pioneer David Silver, was recorded in 2015 and remains a popular resource for anyone wanting to understand the fundamentals of RL.\nWebsite with 10 lectures: videos and slides\nMy repo with slides"
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-1-introduction-to-reinforcement-learning",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-1-introduction-to-reinforcement-learning",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "3/9/21 - Lecture 1: Introduction to Reinforcement Learning",
    "text": "3/9/21 - Lecture 1: Introduction to Reinforcement Learning\nThis introduction is essentially about giving examples of RL to have a good intuition about this field and to provide definitions or context:\n\nDefinitions: rewards, actions, agent, environment, state (and history)\nMajor components: policy, value function, model\nCategorizing RL agents (taxonomy): value based, policy based, actor critic, model free, model based\nLearning and planning\nPrediction and control\n\nAnd David gives 2 references:\n\nwell known Introduction to Reinforcement Learning, Sutton and Barto, 1998\nAlgorithms for Reinforcement Learning, Szepesvari. Available online.\n\nPolicy \\[\\pi\\](s): essentially a map from state to action. Can be deterministic \\[\\pi\\](s) or stochastic \\[\\pi\\](a|s).\nValue function v\\(\\pi\\)(s): is a prediction of expected future reward.\nModel: it is not the environment itself but useful to predict what the environment will do next. 2 types of models: transitions model and rewards model. Transition model predicts the next state (e.g. based on dynamics). Reward model predicts the next immediate reward.\nA lot of algorithms are model-free and doesn’t require these models. It is a fundamental distinctions in RL.\n\nAnd then David explains 2 fundamental different problems with Learning vs Planning.\nWith Learning, environment is unknown, agent interacts directly with the environment and improves its policy.\nWith Planning, a model of environment is known, and agent “plays” with this model and improves its policy.\nThese 2 problems may be linked where you start to learn from the environment and apply planning then.\n2 examples based on atari games.\nAnother topic is exploration vs exploitation then prediction and control."
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-2-markov-decision-processes",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-2-markov-decision-processes",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "3/10/21 - Lecture 2: Markov Decision Processes",
    "text": "3/10/21 - Lecture 2: Markov Decision Processes\nMarkov decision processes formally describe an environment for reinforcement learning.\nMarkov property: the future is independent of the past given the present.\nMarkov Process (or Markov Chain) is the tuple (S, P)\n\nWe can take sample episodes from this chain. (e.g. C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep)\nWe can formalize the transition matrix from s to s’.\nWhen you add reward you get Markov reward process (S, P, R, \\[\\gamma\\])\nReward here is a function to map for each state the immediate reward.\n\\[\\gamma\\] is the discounted factor, \\[\\epsilon\\] [0,1]. David explains why we could need such discount.\nReturn Gt is the total discounted reward at time-step t for a given sample.\n\nValue function v(s) is really what we care about, it is the long-term value of state s.\n\nBellman Equation for MRPs\nThe value function can be decomposed into two parts: - immediate reward Rt+1 - discounted value of next state \\[\\gamma\\].v (St+1)\n\nWe use that to calculate value function with \\[\\gamma\\] \\(\\neq\\) 0.\nAnd calculating value function can be seen as the resolution of this linear equation:\n\nAnd now we introduce actions and it gives Markov Decision Process\n\nAnd we introduce policy\n\nThen we can define the state-value function v\\(\\pi\\)(s,a) for a given policy \\[\\pi\\]\n\nand action-value function q\\(\\pi\\)(s,a) for a given policy \\[\\pi\\]\n\nAnd impact on Bellman Equation ends like that:\n\nv is giving us how good it is to be in a state. q is giving us how good is it to take an action.\nAnd then we have the Bellman equation expressed with v and q.\nWe don’t care much about a given v\\(\\pi\\), we want to get the best policy. And ultimately to get q* which is the optimal action value function.\n\nThe optimal value function specifies the best possible performance in the MDP. A MDP is “solved” when we know the optimal value function q*.\nWhat we really care about is optimal policy \\[\\pi\\]*. There is a partial ordering about policies. And a theorem saying that for any MDP, there exists at least one optimal policy.\nSo the optimal value function calculation is similar to what we did earlier when we averaged the value of the next state but now we take the max instead of average.\nSo no we can write the Bellman Optimality Equation. Unfortunately this is non-linear.\nThere are many approaches such as iterative ones.\n\nValue Iteration\nPolicy Iteration\nQ-learning\nSarsa"
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-3-planning-by-dynamic-programming",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-3-planning-by-dynamic-programming",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "3/12/21 - Lecture 3: Planning by Dynamic Programming",
    "text": "3/12/21 - Lecture 3: Planning by Dynamic Programming\nWill discuss from the agent side: how to solve these MDP problems.\nDavid starts with general ideas on dynamic programming. (programming in a sense of policy)\nValue function is an important idea for RL because it sotres valuable information that you can later reuse (it embeds solutions). And Bellman equation gives the recursive decomposition.\nPlanning by Dynamic Programming\nWe assume full knowledge of the MDP. Dynamic programming is used for planning in an MDP. With 2 usages:\n\nprediction: given MDP and policy \\[\\pi\\], we predict the value of this policy v\\(\\pi\\).\ncontrol: given MDP, we get optimal value function v* and optimal policy \\(\\pi\\)*.\n\nAnd by full MDP it would mean for an atari game to have access to internal code to calculate everything.\nWe need the 2 aspects to solve MDP: prediction to value policy, and control to get the best one.\nPolicy Evaluation\nProblem: evaluate a given policy π Solution: iterative application of Bellman expectation backup\n(Bellman expectation is used in prediction, Bellman optimality is used in control)\nDavid takes an example with a small grid-world and calculates iteratively (k=0, 1, 2, …) v(s) for a uniform random policy (north, south, east, west with prob 0.25) (left column). And then we follow policy greedily using v function. (right column)\nPolicy Iteration\nIn small grid-world example, just by evaluating the policy and act greedily were sufficient to get the optimal policy. This is not generally the case. In general, need more iterations of evaluation (iterative policy evaluation) / improvement (greedy policy). But this process of policy iteration always converges to π∗\nDavid uses Jack’s Car Rental where it needs 4 steps to get the optimal policy. And explains why acting greedy improves the policy. And if improvement stops, Bellman optimality equation is satisfied, we have our optimal policy.\nSome question then about convergence of v\\(\\pi\\) . Why not update policy at each step of evaluation -> this is value iteration.\nValue Iteration\nProblem: find optimal policy π Solution: iterative application of Bellman optimality backup\nExtensions to dynamic programming\nDP uses full-width backups. It is effective for medium-sized problems. Curse of dimensionality for large problems. Even one backup can be too expensive.\nOne solution is to sample backups.\nAdvantages: Model-free: no advance knowledge of MDP required Breaks the curse of dimensionality through sampling Cost of backup is constant, independent of n = |S|"
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-4-model-free-prediction",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-4-model-free-prediction",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "3/15/21 - Lecture 4: Model-Free Prediction",
    "text": "3/15/21 - Lecture 4: Model-Free Prediction\nModel-Free: no-one gives us the MDP. And we still want to solve it.\n\nMonte-Carlo learning: basically methods which goes all the way to the end of trajectory and estimates value by looking at sample returns.\nTemporal-Difference learning: goes one step ahead and estimates after one step\nTD(\\[\\lambda\\]): unify both approaches\n\nWe give up the assumption giving how the environment works (which is highly unrealistic for interesting problems). We break it down in 2 pieces (as with previous lecture with planning):\n\npolicy evaluation case (this lecture) - how much reward we get from that policy (in model-free envt)\ncontrol (next lecture) - find the optimum value function and then optimum policy\n\nMonte-Carlo Reinforcement Learning\nWe go all the way through the episodes and we take sample returns. So the estimated value function can be the average of all returns. You have to terminate to perform this mean.\nIt means we use the empirical mean return in place of expected return. (by law of large numbers, this average returns will converge to value function as the number of episodes for that state tends to infinity)\nTemporal-Difference Reinforcement Learning\nTD learns from incomplete episodes, by bootstrapping\nDavid takes an example from Sutton about predicting time to commute home, comparing MC and TD.\nTD target (Rt+1+\\[\\gamma\\]Vt+1) is biased estimate of v\\(\\pi\\)(St), but has lower variance than the return Gt.\nDavid compares perf of MC, TD(0), … using Random Walk example and different values of \\[\\alpha\\]."
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-5-model-free-control",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-5-model-free-control",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "3/18/21 - Lecture 5: Model-Free Control",
    "text": "3/18/21 - Lecture 5: Model-Free Control\nDistinction between on-policy (learning by doing the job) and off-policy (following someone else behavior)\non-policy\nIn Monte-Carlo approach, we have 2 issues. First is that we don’t have access to model so we should use Q(s, a) instead of v(s). Second is lack of exploration so we should use \\[\\epsilon\\]-greedy policy.\nWith GLIE (Greedy in the Limit with Infinite Exploration), we can update Q after each episodes.\nWe will now use TD:\nNatural idea: use TD instead of MC in our control loop\n\nApply TD to Q(S, A)\nUse \\[\\epsilon\\]-greedy policy improvement\nUpdate every time-step\n\nThis is SARSA update. Every single time-step we update our diagram.\nA generalisation is n-step Sarsa. n=1 is standard Sarsa. n=\\[\\infty\\] is MC.\nTo get the best of both worlds, we consider Sarsa(\\[\\lambda\\]). We have a forward version\n\nAnd a backward version which allows online experience. Thanks to eligibility traces.\noff-policy\nWhy is this important?\n\nLearn from observing humans or other agents\nRe-use experience generated from old policies π 1 , π 2 , …, π t−1\nLearn about optimal policy while following exploratory policy\nLearn about multiple policies while following one policy\n\nWe can apply it in importance sampling for off-policy. With Monte-Carlo it is however useless due to high variance. It is imperative to to TD.\nWe can apply that to Q-learning. We can use greedy slection on target policy \\[\\pi\\] and \\[\\epsilon\\] greedy on behaviour policy \\[\\mu\\]."
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-6-value-function-approximation",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-6-value-function-approximation",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "4/27/21 - Lecture 6: Value Function Approximation",
    "text": "4/27/21 - Lecture 6: Value Function Approximation\nHow to scale up value function approach.\nValue Function Approximation\nSo far we have represented value function by a lookup table Every state s has an entry V (s) Or every state-action pair s, a has an entry Q(s, a)\nSolution for large MDPs: Estimate value function with function approximation v̂ (s, w) ≈ v π (s) or q̂(s, a, w) ≈ q π (s, a) Generalise from seen states to unseen states Update parameter w using MC or TD learning\nThere are many function approximators, e.g.\n\nLinear combinations of features\nNeural network\nDecision tree\nNearest neighbour\nFourier / wavelet bases\n\nWe focus on differentiable function approximators."
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-7-policy-gradient-methods",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-7-policy-gradient-methods",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "5/4/21 - Lecture 7: Policy Gradient Methods",
    "text": "5/4/21 - Lecture 7: Policy Gradient Methods\n3 methods:\n\nfinite difference\nMC policy gradient\nActor-Critic Policy Gradient\n\nadvantages of policy based RL vs value based RL:\n\nconvergence (w/o oscillation that one can see in value based)\neffective in continuous action spaces (in some cases taking the max (of q value) can be quite expensive)\npolicy based RL can learn stochastic policies which can be beneficial in some cases (e.g. rock scissor paper) (usually where you don’t fall into MDP with perfect states representation but we get partially observed environments)\n\nSome examples of policy: softmax policy and gaussian policy.\nOne-step MDP: terminating after 1 time-step. No sequence. In that case we have \\[\nJ(\\theta)=\\mathbb{E}_{\\pi_\\theta}[r]=\\sum_{s \\in \\mathcal{S}}d(s)\\sum_{a \\in \\mathcal{A}}\\pi_\\theta(s, a)\\mathcal{R}_{s,a}\n\\\\\nand\\\\\n\\nabla_\\theta J(\\theta) = \\sum_{s \\in \\mathcal{S}}d(s)\\sum_{a \\in \\mathcal{A}}\\pi_\\theta(s, a)\\nabla_\\theta \\log\\pi_\\theta(s, a)\\mathcal{R}_{s,a}\n\\\\\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log\\pi_\\theta(s, a)r]\n\\] Generalization is to replace instantaneous reward r with long-term value \\(Q_\\pi(s,a)\\)\n1st algorithm is Monte-Carlo policy gradient (REINFORCE) - tend to be slow, very high variance\nWhere we sample \\(Q_\\pi(s,a)\\) in \\(v_t\\) and regularly update \\(\\theta\\).\nReducing variance using a critic.\nWe use a critic to estimate the action-value function, \\(Q_w (s, a) ≈ Q_{π_θ} (s, a)\\) Actor-critic algorithms maintain two sets of parameters:\n\nCritic Updates action-value function parameters w\nActor Updates policy parameters θ, in direction suggested by critic\n\nActor-critic algorithms follow an approximate policy gradient \\[\n\\nabla_\\theta J(\\theta) \\approx \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log\\pi_\\theta(s, a)Q_w(s, a)]\n\\\\\n\\Delta\\theta=\\alpha\\nabla_\\theta\\log\\pi_\\theta(s, a)Q_w(s, a)\n\\] Critic will use a policy evaluation (several options seen so far: monte-carlo policy evaluation, TD, TD(\\(\\lambda\\)))"
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-8-integrating-learning-and-planning",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-8-integrating-learning-and-planning",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "6/21/21 - Lecture 8: Integrating Learning and Planning",
    "text": "6/21/21 - Lecture 8: Integrating Learning and Planning\n3 parts in this lecture:\n\nmodel based reinforcement learning\nintegrated architecture\nsimulation-based search\n\nLearn by model. What we mean by the model is 2 parts: understand transitions (how one state will transition to another state) and reward. If the agent has this understanding, then one can plan with that.\nModel-Free RL\n\nNo model\nLearn value function (and/or policy) from experience\n\nModel-Based RL\n\nLearn a model from experience\nPlan value function (and/or policy) from model\n\nModel-Based RL (using Sample-Based Planning)\n\nLearn a model from real experience\nPlan value function (and/or policy) from simulated experience\n\nDyna-Q is a way to combine real experience with simulation.\nSimulation-Based Search\n\nForward search paradigm using sample-based planning\nSimulate episodes of experience from now with the model\nApply model-free RL to simulated episodes\n\nSimulate episodes of experience from now with the model\n\\[\n\\Big\\{ S_t^k, A_t^k, R_{t+1}^k, ..., S_T^k \\Big\\}_{k=1}^K \\sim \\mathcal{M}_v\n\\]\nApply model-free RL to simulated episodes\n\nMonte-Carlo control → Monte-Carlo search\nSarsa → TD search"
  },
  {
    "objectID": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-9-exploration-and-exploitation",
    "href": "posts/2021-03-09-Introduction to Reinforcement Learning with David Silver.html#lecture-9-exploration-and-exploitation",
    "title": "Introduction to Reinforcement Learning with David Silver",
    "section": "7/1/21 - Lecture 9: Exploration and exploitation",
    "text": "7/1/21 - Lecture 9: Exploration and exploitation\nDavid starts with a multi-armed bandit case. We can think about it as a one-step MDP.\nBut in that case we don’t have states anymore.\nDefinition of regret as the total opportunity loss (how far we are from the best value). And maximizing cumulative reward is the same as minimizing total regret.\nGreedy and \\(\\epsilon\\)-greedy have linear total regret.\nOptimism in face of uncertainty: don’t play the one with best mean value but play the one with best potential (characterized with the highest tail) = select action maximising Upper Confidence Bound (UCB)\n\\(\\epsilon\\)-greedy is behaving right when properly tuned or can be a disaster otherwise. UCB is comparable to properly tuned \\(\\epsilon\\)-greedy.\nthesis from a French guy about thompson sampling in optimisation control problems: Exploration-Exploitation with Thompson Sampling in Linear"
  },
  {
    "objectID": "posts/2021-03-24-stable-baselines-3.html",
    "href": "posts/2021-03-24-stable-baselines-3.html",
    "title": "Stable baselines 3 - 1st steps",
    "section": "",
    "text": "I have just read about this new release. This is a complete rewrite of stable baselines 2, without any reference to tensorflow, and based on pytorch (>1.4+).\nThere is a lot of running implementations of RL algorithms, based on gym. A very good introduction in this blog entry\nStable-Baselines3: Reliable Reinforcement Learning Implementations | Antonin Raffin | Homepage\nLinks\n\nGitHub repository: https://github.com/DLR-RM/stable-baselines3\nDocumentation: https://stable-baselines3.readthedocs.io/\nRL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\nContrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\nRL Tutorial: https://github.com/araffin/rl-tutorial-jnrr19"
  },
  {
    "objectID": "posts/2021-03-24-stable-baselines-3.html#tips-and-tricks",
    "href": "posts/2021-03-24-stable-baselines-3.html#tips-and-tricks",
    "title": "Stable baselines 3 - 1st steps",
    "section": "Tips and tricks",
    "text": "Tips and tricks\nThis page covers general advice about RL (where to start, which algorithm to choose, how to evaluate an algorithm, …), as well as tips and tricks when using a custom environment or implementing an RL algorithm.\n\nBe familiar with RL, see resource page\nread SB3 documentation\ndo the tutorial\n\nTune hyperparameters RL zoo is introduced. It contains some hyperparameter optimization.\nRL evaluation We suggest you reading Deep Reinforcement Learning that Matters for a good discussion about RL evaluation.\nwhich algorithm to choose 1st criteria is discrete vs continuous actions. And 2nd is capacity to parallelize training.\nDiscrete Actions\n\nDiscrete Actions - Single Process\n\nDQN with extensions (double DQN, prioritized replay, …) are the recommended algorithms. We notably provide QR-DQN in our contrib repo. DQN is usually slower to train (regarding wall clock time) but is the most sample efficient (because of its replay buffer).\n\nDiscrete Actions - Multiprocessed\n\nYou should give a try to PPO or A2C.\nContinuous Actions\n\nContinuous Actions - Single Process\n\nCurrent State Of The Art (SOTA) algorithms are SAC, TD3 and TQC (available in our contrib repo). Please use the hyperparameters in the RL zoo for best results.\n\nContinuous Actions - Multiprocessed\n\nTake a look at PPO or A2C. Again, don’t forget to take the hyperparameters from the RL zoo for continuous actions problems (cf Bullet envs).\nCreating a custom env\nmultiple times there are advices about normalizing: observation and action space. A good practice is to rescale your actions to lie in [-1, 1]. This does not limit you as you can easily rescale the action inside the environment\ntips and tricks to reproduce a RL paper\nReinforcement Learning Tips and Tricks — Stable Baselines3 1.1.0a1 documentation\n\nA personal pick (by @araffin) for environments with gradual difficulty in RL with continuous actions:\n\nPendulum (easy to solve)\nHalfCheetahBullet (medium difficulty with local minima and shaped reward)\nBipedalWalkerHardcore (if it works on that one, then you can have a cookie)\n\nin RL with discrete actions:\n\nCartPole-v1 (easy to be better than random agent, harder to achieve maximal performance)\nLunarLander\nPong (one of the easiest Atari game)\nother Atari games (e.g. Breakout)"
  },
  {
    "objectID": "posts/2021-03-24-stable-baselines-3.html#resource-page",
    "href": "posts/2021-03-24-stable-baselines-3.html#resource-page",
    "title": "Stable baselines 3 - 1st steps",
    "section": "Resource page",
    "text": "Resource page\nReinforcement Learning Resources — Stable Baselines3 1.1.0a1 documentation\nStable-Baselines3 assumes that you already understand the basic concepts of Reinforcement Learning (RL).\nHowever, if you want to learn about RL, there are several good resources to get started:\n\nOpenAI Spinning Up\nDavid Silver’s course\nLilian Weng’s blog\nBerkeley’s Deep RL Bootcamp\nBerkeley’s Deep Reinforcement Learning course\nMore resources"
  },
  {
    "objectID": "posts/2021-03-24-stable-baselines-3.html#examples",
    "href": "posts/2021-03-24-stable-baselines-3.html#examples",
    "title": "Stable baselines 3 - 1st steps",
    "section": "Examples",
    "text": "Examples\nI will run these examples in 01 -hands-on.ipynb from handson_stablebaselines3\nDQN lunarlander \nMy module is never landing :(\nNote: animated gif created with peek.\nPPO with multiprocessing cartpole\n\nMonitor training using callback\nThis could be useful when you want to monitor training, for instance display live learning curves in Tensorboard (or in Visdom) or save the best agent.\n\nAtari game such as pong (A2C with 6 envt) or breakout\n\nHere the list of valid gym atari environments: https://gym.openai.com/envs/#atari\n\n\n\nsb3_breakout.gif\n\n\npybullet\nThis is a SDK to real-time collision detection and multi-physics simulation for VR, games, visual effects, robotics, machine learning etc.\nhttps://github.com/bulletphysics/bullet3/\n\nWe need to install it: pip install pybullet\nI don’t have rendering capacity when playing with it. Because robotic is far from my need, I will skip on this one\nHindsight Experience Replay (HER)\nusing Highway-Env\ninstallation with pip install highway-env\nAfter 1h15m of training, some 1st results:\n\nAnd after that some technical stuff such as: * Learning Rate Schedule: start with high value and reduce it as learning goes * Advanced Saving and Loading: how to easily create a test environment to evaluate an agent periodically, use a policy independently from a model (and how to save it, load it) and save/load a replay buffer. * Accessing and modifying model parameters: These functions are useful when you need to e.g. evaluate large set of models with same network structure, visualize different layers of the network or modify parameters manually. * Record a video or make a gif\nMake a GIF of a Trained Agent\npip install imageio\nand this time the lander is getting closer to moon but not at all between flags."
  },
  {
    "objectID": "posts/2021-03-25-contribute to a project with git.html",
    "href": "posts/2021-03-25-contribute to a project with git.html",
    "title": "Git - How To Contribute To A Project",
    "section": "",
    "text": "Based on http://qpleple.com/how-to-contribute-to-a-project-on-github/"
  },
  {
    "objectID": "posts/2021-03-25-contribute to a project with git.html#using-clustergit-as-an-example",
    "href": "posts/2021-03-25-contribute to a project with git.html#using-clustergit-as-an-example",
    "title": "Git - How To Contribute To A Project",
    "section": "Using clustergit as an example",
    "text": "Using clustergit as an example\nFork\nMake your own working copy of the project by forking it: go on the project page (https://github.com/mnagel/clustergit) and click “Fork”. You can access you copy at: https://github.com/castorfou/clustergit\nClone\nClone your fork git repository on your local computer:\ngit clone git@github.com:castorfou/clustergit.git\nBranch\ngit branch master-to-main\ngit checkout master-to-main\nThis is very important, create one branch per patch. And never submit a patch that has been done on the branch master or main!\nDevelop\nHere I want to reflect change from Oct/20 where default branch name in github is now main\nsed -i 's/master/main/g' clustergit\nCommit\ngit add -u\ngit commit -m \"default branch name 'main'\"\nPush to github\ngit push origin master-to-main\nCreate pull request\nGo on your fork page (https://github.com/castorfou/clustergit), then select master-to-main in the branch list and click “Pull Request”.\nSubmit patch\nCheck the diff, write a message explaining what you have done and why the repository owner should accept your pull request and submit."
  },
  {
    "objectID": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html",
    "href": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html",
    "title": "Headless raspberry pi: create a wifi to ethernet bridge",
    "section": "",
    "text": "After my internet provider router stopped unexpectedly yesterday, I had to find a solution with internet access from phones and raspberry pi to broadcast internet to full home devices."
  },
  {
    "objectID": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html#headless-raspberry-pi",
    "href": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html#headless-raspberry-pi",
    "title": "Headless raspberry pi: create a wifi to ethernet bridge",
    "section": "Headless raspberry pi",
    "text": "Headless raspberry pi\nInstallation on SD from ubuntu\nfor a reason, raspberry pi imager snap doesn’t work (due to a bug linked to QT+wayland).\nI download deb ubuntu version (imager_1.6_amd64.deb) from https://www.raspberrypi.org/software and install with dpkg. (sudo dpkg -i imager_1.6_amd64.deb)\nWith rpi-imager, I can install by selecting the default OS (raspberry Pi OS 32-bit), and SD card as storage.\nHeadless wifi\nAs explained in https://www.raspberrypi.org/documentation/configuration/wireless/headless.md\nCreate (touch) wpa_supplicant.conf in /boot of SD card and paste this content:\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\ncountry=FR\n\nnetwork={\n ssid=\"AndroidAP\"\n psk=\"<Password for your wireless LAN>\"\n}\nHeadless ssh\nAs explained in https://www.raspberrypi.org/documentation/remote-access/ssh/README.md\nCreate (touch) ssh in /boot of SD card\nIf it is found, SSH is enabled and the file is deleted. The content of the file does not matter; it could contain text, or nothing at all."
  },
  {
    "objectID": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html#test-installation",
    "href": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html#test-installation",
    "title": "Headless raspberry pi: create a wifi to ethernet bridge",
    "section": "Test installation",
    "text": "Test installation\nBoot. After a couple of minutes, I have a notification on phone saying a device is connected on my phone hotspot.\n\nAnd ssh raspberry (default username/password are pi/raspberry)\n$ ssh -l pi 192.168.43.179\npi@192.168.43.179's password: \nLinux raspberrypi 5.4.83-v7+ #1379 SMP Mon Dec 14 13:08:57 GMT 2020 armv7l\n\nThe programs included with the Debian GNU/Linux system are free software;\nthe exact distribution terms for each program are described in the\nindividual files in /usr/share/doc/*/copyright.\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\npermitted by applicable law.\nLast login: Thu Mar 25 06:23:17 2021\n\nSSH is enabled and the default password for the 'pi' user has not been changed.\nThis is a security risk - please login as the 'pi' user and type 'passwd' to set a new password.\nHeadless raspberry is ready to be used."
  },
  {
    "objectID": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html#wifi-to-ethernet-bridge",
    "href": "posts/2021-03-25-headless-raspberry-pi-bridge-network.html#wifi-to-ethernet-bridge",
    "title": "Headless raspberry pi: create a wifi to ethernet bridge",
    "section": "Wifi to ethernet bridge",
    "text": "Wifi to ethernet bridge\nI will use https://willhaley.com/blog/raspberry-pi-wifi-ethernet-bridge/\nThe only package that is needed is dnsmasq however from a clean install it is a good idea to make sure everything is up-to-date:\nget up-to-date system\nsudo apt-get update && sudo apt-get upgrade -y && sudo apt-get install rpi-update dnsmasq -y\nsudo rpi-update\nOption 1 - Same Subnet\nSave this script as a file named bridge.sh on your Pi.\n#!/usr/bin/env bash\n\nset -e\n\n[ $EUID -ne 0 ] && echo \"run as root\" >&2 && exit 1\n\n##########################################################\n# You should not need to update anything below this line #\n##########################################################\n\n# parprouted  - Proxy ARP IP bridging daemon\n# dhcp-helper - DHCP/BOOTP relay agent\n\napt update && apt install -y parprouted dhcp-helper\n\nsystemctl stop dhcp-helper\nsystemctl enable dhcp-helper\n\n# Enable ipv4 forwarding.\nsed -i'' s/#net.ipv4.ip_forward=1/net.ipv4.ip_forward=1/ /etc/sysctl.conf\n\n# Service configuration for standard WiFi connection. Connectivity will\n# be lost if the username and password are incorrect.\nsystemctl restart wpa_supplicant.service\n\n# Enable IP forwarding for wlan0 if it's not already enabled.\ngrep '^option ip-forwarding 1$' /etc/dhcpcd.conf || printf \"option ip-forwarding 1\\n\" >> /etc/dhcpcd.conf\n\n# Disable dhcpcd control of eth0.\ngrep '^denyinterfaces eth0$' /etc/dhcpcd.conf || printf \"denyinterfaces eth0\\n\" >> /etc/dhcpcd.conf\n\n# Configure dhcp-helper.\ncat > /etc/default/dhcp-helper <<EOF\nDHCPHELPER_OPTS=\"-b wlan0\"\nEOF\n\n# Enable avahi reflector if it's not already enabled.\nsed -i'' 's/#enable-reflector=no/enable-reflector=yes/' /etc/avahi/avahi-daemon.conf\ngrep '^enable-reflector=yes$' /etc/avahi/avahi-daemon.conf || {\n  printf \"something went wrong...\\n\\n\"\n  printf \"Manually set 'enable-reflector=yes in /etc/avahi/avahi-daemon.conf'\\n\"\n}\n\n# I have to admit, I do not understand ARP and IP forwarding enough to explain\n# exactly what is happening here. I am building off the work of others. In short\n# this is a service to forward traffic from WiFi to Ethernet.\ncat <<'EOF' >/usr/lib/systemd/system/parprouted.service\n[Unit]\nDescription=proxy arp routing service\nDocumentation=https://raspberrypi.stackexchange.com/q/88954/79866\nRequires=sys-subsystem-net-devices-wlan0.device dhcpcd.service\nAfter=sys-subsystem-net-devices-wlan0.device dhcpcd.service\n\n[Service]\nType=forking\n# Restart until wlan0 gained carrier\nRestart=on-failure\nRestartSec=5\nTimeoutStartSec=30\n# clone the dhcp-allocated IP to eth0 so dhcp-helper will relay for the correct subnet\nExecStartPre=/bin/bash -c '/sbin/ip addr add $(/sbin/ip -4 -br addr show wlan0 | /bin/grep -Po \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\")/32 dev eth0'\nExecStartPre=/sbin/ip link set dev eth0 up\nExecStartPre=/sbin/ip link set wlan0 promisc on\nExecStart=-/usr/sbin/parprouted eth0 wlan0\nExecStopPost=/sbin/ip link set wlan0 promisc off\nExecStopPost=/sbin/ip link set dev eth0 down\nExecStopPost=/bin/bash -c '/sbin/ip addr del $(/sbin/ip -4 -br addr show wlan0 | /bin/grep -Po \"\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+\")/32 dev eth0'\n\n[Install]\nWantedBy=wpa_supplicant.service\nEOF\n\nsystemctl daemon-reload\nsystemctl enable parprouted\nsystemctl start parprouted dhcp-helper\nStep 2: Execute the script on your Pi like so.\nsudo bash bridge.sh\nStep 3: Reboot.\nsudo reboot\nDone!"
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "",
    "text": "https://rlvs.aniti.fr/\nSchedule is"
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#rlvs-schedule",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#rlvs-schedule",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "RLVS schedule",
    "text": "RLVS schedule\nThis condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2).\n\n\n\nSchedule\n\n\n\n\n\n\n\nMarch 25th\n9:00-9:10\nOpening remarks\nS. Gerchinovitz\n\n\n\n9:10-9:30\nRLVS Overview\nE. Rachelson\n\n\n\n9:30-13:00\nRL fundamentals\nE. Rachelson\n\n\n\n14:00-16:00\nIntroduction to Deep Learning\nD. Wilson\n\n\n\n16:30-17:30\nReward Processing Biases in Humans and RL Agents\nI. Rish\n\n\n\n17:45-18:45\nIntroduction to Hierarchical Reinforcement Learning\nD. Precup\n\n\nMarch 26th\n10:00-12:00\nStochastic bandits\nT. Lattimore\n\n\n\n14:00-16:00\nMonte Carlo Tree Search\nT. Lattimore\n\n\n\n16:30-17:30\nMulti-armed bandits in clinical trials\nD. A. Berry\n\n\nApril 1st\n9:00-15:00\nDeep Q-Networks and its variants\nB. Piot, C. Tallec\n\n\n\n15:15-16:15\nRegularized MDPs\nM. Geist\n\n\n\n16:30-17:30\nRegret bounds of model-based reinforcement learning\nM. Wang\n\n\nApril 2nd\n9:00-12:30\nPolicy Gradients and Actor Critic methods\nO. Sigaud\n\n\n\n14:00-15:00\nPitfalls in Policy Gradient methods\nO. Sigaud\n\n\n\n15:30-17:30\nExploration in Deep RL\nM. Pirotta\n\n\nApril 8th\n9:00-11:00\nEvolutionary Reinforcement Learning\nD. Wilson, J.-B. Mouret\n\n\n\n11:30-12:30\nEvolving Agents that Learn More Like Animals\nS. Risi\n\n\n\n14:00-16:00\nMicro-data Policy Search\nK. Chatzilygeroudis, J.-B. Mouret\n\n\n\n16:30-17:30\nEfficient Motor Skills Learning in Robotics\nD. Lee\n\n\nApril 9th\n9:00-13:00\nRL tips and tricks\nA. Raffin\n\n\n\n14:30-15:30\nSymbolic representations and reinforcement learning\nM. Garnelo\n\n\n\n15:45-16:45\nLeveraging model-learning for extreme generalization\nL. P. Kaelbling\n\n\n\n17:00-18:00\nRLVS wrap-up\nE. Rachelson"
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#deep-q-networks-and-its-variants",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#deep-q-networks-and-its-variants",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "(4/1/21) - Deep Q-Networks and its variants",
    "text": "(4/1/21) - Deep Q-Networks and its variants\nSpeaker is Bilal Piot.\nDeep Q network as a solution for a practicable control theory.\nIntroduction of ALE (Atari Learning Environment)\nDQN is (almost) end-to-end: from raw observations to actions. Bilal explains the preprocessing part (from 160x210x3 to 84x84 + stacking 4 frames + downsampling to 15 Hz)\nValue Iteration (VI) algorithm: Recurrent algorithm to get Q. \\(Q_{k+1}=T^*Q\\)\nBut it is not practical in a real-world case. What we can do is use interactions with real world. And estimate \\(Q^*\\) using a regression.\nWould be interesting to have slides. I like the link between regression notations and VI notation.\nFrom neural Fitted-\\(Q\\) to DQN. Main difference is data collection (in DQN you have updated interactions and it allows exploration, and size of architecture)\nWith DQN we have acting part and learning part. Acting is the data collection. (using \\(\\epsilon\\)-greedy policy)\nhands-on based on DQN tutorial notebook.\nhad to export LD_LIBRARY_PATH=/home/explore/miniconda3/envs/aniti/lib/\nNice introduction to JAX and haiku. Haiku is similar modules in pytorch and can turn NN into pure version. Which is useful for Jax.\noverview of the literature"
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#from-policy-gradients-to-actor-critic-methods",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#from-policy-gradients-to-actor-critic-methods",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "(4/2/21) - From Policy Gradients to Actor Critic methods",
    "text": "(4/2/21) - From Policy Gradients to Actor Critic methods\nOlivier Sigaud is the speaker.\nHe has pre-recorded his lecture in videos. I have missed the start so I will have to watch them later.\nPolicy Gradient in pratice\nDon’t become an alchemist ;)\nAs stochastic policies, squashed gaussian is interesting because it allows continuous variable + bounds.\nExploration in Deep RL"
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#evolutionary-reinforcement-learning",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#evolutionary-reinforcement-learning",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "(4/8/21) - Evolutionary Reinforcement Learning",
    "text": "(4/8/21) - Evolutionary Reinforcement Learning\npdf version of the slides are available here\nthen Evolving Agents that Learn More Like Animals\nThis morning was more about what we can do when we have infinite calculation power and data.\nAfternoon will be the opposite."
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#micro-data-policy-search",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#micro-data-policy-search",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "(4/8/21) - Micro-data Policy Search",
    "text": "(4/8/21) - Micro-data Policy Search\nMost policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word “big-data”, we refer to this challenge as “micro-data reinforcement learning”. We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup.\nall material: https://rl-vs.github.io/rlvs2021/micro-data.html"
  },
  {
    "objectID": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#rl-in-practice-tips-and-tricks-and-practical-session-with-stable-baselines3",
    "href": "posts/2021-04-01-Aniti - RLVS - seminaire RL.html#rl-in-practice-tips-and-tricks-and-practical-session-with-stable-baselines3",
    "title": "ANITI’s first Reinforcement Learning Virtual School",
    "section": "(4/9/21) - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3",
    "text": "(4/9/21) - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3\n​ Abstract: The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library.\nPre-requisites: Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab).\nAdditional material: Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/\nOutline: Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots\n\nIntroduction (3 minutes)\nRL Tips and tricks (45 minutes)\n\nGeneral Nuts and Bolts of RL experimentation (10 minutes)\nRL in practice on a custom task (custom environment) (30 minutes)\nQuestions? (5 minutes)\n\nThe Challenges of Applying RL to Real Robots (45 minutes)\n\nLearning to control an elastic robot - DLR David Neck Example (15 minutes)\nLearning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes)\nLearning to walk with an elastic quadruped robot - DLR bert example (10 minutes)\nQuestions? (5 minutes+)\n\n\nPart II: Practical Session with Stable-Baselines3\n\nStable-Baselines3 Overview (20 minutes)\nQuestions? (5 minutes)\nPractical Session - Code along (1h+)\n\naction space\nWhen using continuous space, you need to normalize! (normalized action space -1, -1)\nthere is a checker for that in stable baselines 3.\nreward\nstart with reward shaping.\ntermination condition\nearly stopping makes learning faster (and safer for robots)\n\nfor hyperparameter tuning, Antonin recommends Optuna.\nabout the Henderson paper: Deep Reinforcement Learning that Matters\n\nand then the controller will use latent representation / current speed + history as observation space.\nLearning to drive takes then 10 min, and to race 2 hours.\n\nhandson\nslides: https://araffin.github.io/slides/rlvs-sb3-handson/\nnotebook: https://github.com/araffin/rl-handson-rlvs21\nRL zoo: https://github.com/DLR-RM/rl-baselines3-zoo\ndocumentation for SB3 usefull for completing exercises: https://stable-baselines3.readthedocs.io/en/master/\nhttps://excalidraw.com/"
  },
  {
    "objectID": "posts/2021-04-01-logbook-April.html",
    "href": "posts/2021-04-01-logbook-April.html",
    "title": "Logbook for April 21",
    "section": "",
    "text": "Thursday 4/1\nAniti RLVS - Deep Q-Networks and its variants\nCollège de France - Algorithmes quantiques : quand la physique quantique défie la thèse de Church-Turing Leçon inaugurale\nFriday 4/2\nAniti RLVS - From Policy Gradients to Actor Critic methods\nAniti RLVS - Policy Gradient in pratice\ngit to use socks server to github (to go through local firewall)\nAniti RLVS - Exploration in Deep RL"
  },
  {
    "objectID": "posts/2021-04-01-logbook-April.html#week-14---apr-21",
    "href": "posts/2021-04-01-logbook-April.html#week-14---apr-21",
    "title": "Logbook for April 21",
    "section": "Week 14 - Apr 21",
    "text": "Week 14 - Apr 21\nWednesday 4/7\ntabnet: pytorch (and fastai with Zach Mueller) implementations\nThursday 4/8\nAniti RLVS - Evolutionary Reinforcement Learning\nJupyter notebook turned into slides with RISE\nAniti RLVS - Micro-data Policy Search\nFriday 4/9\nAniti RLVS - RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3\nsetup wsl-vpnkit to workaround wsl2 and network issues (explained here)\ninstall wsl2-cuda-conda"
  },
  {
    "objectID": "posts/2021-04-01-logbook-April.html#week-15---apr-21",
    "href": "posts/2021-04-01-logbook-April.html#week-15---apr-21",
    "title": "Logbook for April 21",
    "section": "Week 15 - Apr 21",
    "text": "Week 15 - Apr 21\nThursday 4/15\nMIT 6S191 Learning for Information Extraction (lecture 9)."
  },
  {
    "objectID": "posts/2021-04-01-logbook-April.html#week-16---apr-21",
    "href": "posts/2021-04-01-logbook-April.html#week-16---apr-21",
    "title": "Logbook for April 21",
    "section": "Week 16 - Apr 21",
    "text": "Week 16 - Apr 21\nTuesday 4/20\nHow To run macOS on KVM / QEMU\nWednesday 4/21\nEnrolled to Machine learning in python with scikit-learn by scikit-learn team!\nAudio classification with pytorch"
  },
  {
    "objectID": "posts/2021-04-01-logbook-April.html#week-17---apr-21",
    "href": "posts/2021-04-01-logbook-April.html#week-17---apr-21",
    "title": "Logbook for April 21",
    "section": "Week 17 - Apr 21",
    "text": "Week 17 - Apr 21\nMonday 4/26\ndatacamp - Bayesian Data Analysis in Python, at the end, Think Bayes 2 by Allen B.Downey\nTuesday 4/27\nRL Course by David Silver Value Function Approximation (lecture 6)\nMIT 6S191 Taming Dataset Bias (lecture 10)\nWednesday 4/28\ndatacamp - Exploratory Data Analysis in Python, by Allen B.Downey (that’s why)\nThursday 4/29\ndatacamp - Working with date and time, quite interesting to see timezones considerations.\nFriday 4/30\ndatacamp - Cleaning data. Impressed by fuzzywuzzy and recordlinkage packages. End of this track (Data Scientist - new version)\nMIT 6S191 Towards AI for 3D Content Creation (lecture 11) and AI in Healthcare (lecture 12) and this is the end of this course"
  },
  {
    "objectID": "posts/2021-04-06-git avec proxy socks.html",
    "href": "posts/2021-04-06-git avec proxy socks.html",
    "title": "using SOCKS5 proxy - with git, apt, pip, …",
    "section": "",
    "text": "using dante server\nInstallation\nsudo apt-get install dante-server\nConf file\nsudo nano /etc/danted.conf\n\nlogoutput: stderr\ninternal: enp3s0 port = 1080\nexternal: enp3s0\nsocksmethod: none\nclientmethod: none\nuser.privileged: proxy\nuser.unprivileged: nobody\nuser.libwrap: nobody\nclient pass {\n        from: 0.0.0.0/0 to: 0.0.0.0/0\n        log: error connect disconnect\n}\nclient block {\n        from: 0.0.0.0/0 to: 0.0.0.0/0\n        log: connect error\n}\nsocks pass {\n        from: 0.0.0.0/0 to: 0.0.0.0/0\n        log: error connect disconnect\n}\nsocks block {\n        from: 0.0.0.0/0 to: 0.0.0.0/0\n        log: connect error\n}\nStart and monitor usage\nsudo service danted restart\ntail -f /var/log/syslog"
  },
  {
    "objectID": "posts/2021-04-06-git avec proxy socks.html#git-setup",
    "href": "posts/2021-04-06-git avec proxy socks.html#git-setup",
    "title": "using SOCKS5 proxy - with git, apt, pip, …",
    "section": "Git setup",
    "text": "Git setup\n$ cat .ssh/config\nHost github.com\nIdentityFile ~/.ssh/id_rsa_gmail\nProxyCommand /bin/nc -X 5 -x 192.168.50.202:1080 %h %p"
  },
  {
    "objectID": "posts/2021-04-06-git avec proxy socks.html#proxychains",
    "href": "posts/2021-04-06-git avec proxy socks.html#proxychains",
    "title": "using SOCKS5 proxy - with git, apt, pip, …",
    "section": "Proxychains",
    "text": "Proxychains\ninstallation\n# to be downloaded from apt mirrors:\n# libproxychains proxychains\nsudo dpkg -i libproxychains3_3.1-7_amd64.deb proxychains_3.1-7_all.deb\nconfiguration\nsudo vi /etc/proxychains.conf\n\n[ProxyList]\n# add proxy here ...\n# meanwile\n# defaults set to \"tor\"\nsocks5          192.168.50.202  1080\nusage\nsudo proxychains apt update\nsudo proxychains apt upgrade\n\nproxychains pip install pycaret"
  },
  {
    "objectID": "posts/2021-04-09-wsl2 cuda conda.html",
    "href": "posts/2021-04-09-wsl2 cuda conda.html",
    "title": "setup wsl2 with cuda and conda",
    "section": "",
    "text": "workaround explained in this blog entry\nwsl -d Ubuntu-20.04 sudo ~/Applications/wsl-vpnkit/wsl-vpnkit-main/wsl-vpnkit"
  },
  {
    "objectID": "posts/2021-04-09-wsl2 cuda conda.html#cuda",
    "href": "posts/2021-04-09-wsl2 cuda conda.html#cuda",
    "title": "setup wsl2 with cuda and conda",
    "section": "cuda",
    "text": "cuda\nhttps://docs.nvidia.com/cuda/wsl-user-guide/index.html#installing-nvidia-drivers\ninstall nvidia cuda specific driver for WSL: https://developer.nvidia.com/cuda/wsl on windows. (version 470.14_quadro_win10-dch_64bit_international in my case)\nproxychains wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\nsudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub\nsudo proxychains add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /\"\nsudo proxychains apt-get update\nsudo proxychains apt-get -y install cuda-toolkit-11-2\nhttps://christianjmills.com/Using-PyTorch-with-CUDA-on-WSL2/\nnew version using WSL-ubuntu as distro\nhttps://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&target_distro=WSLUbuntu&target_version=20&target_type=deblocal\nproxychains wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\nproxychains wget https://developer.download.nvidia.com/compute/cuda/11.2.2/local_installers/cuda-repo-wsl-ubuntu-11-2-local_11.2.2-1_amd64.deb\nsudo dpkg -i cuda-repo-wsl-ubuntu-11-2-local_11.2.2-1_amd64.deb\nsudo apt-key add /var/cuda-repo-wsl-ubuntu-11-2-local/7fa2af80.pub\nsudo proxychains apt-get update\nsudo proxychains apt-get -y install cuda\ntest cuda\nconda activate pytorch\nipython\nimport torch\ntorch.cuda.is_available()"
  },
  {
    "objectID": "posts/2021-04-09-wsl2 cuda conda.html#conda",
    "href": "posts/2021-04-09-wsl2 cuda conda.html#conda",
    "title": "setup wsl2 with cuda and conda",
    "section": "conda",
    "text": "conda\nfrom https://docs.conda.io/en/latest/miniconda.html\ndownload https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nand install with ./Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3"
  },
  {
    "objectID": "posts/2021-04-09-wsl2 cuda conda.html#pycaret",
    "href": "posts/2021-04-09-wsl2 cuda conda.html#pycaret",
    "title": "setup wsl2 with cuda and conda",
    "section": "pycaret",
    "text": "pycaret\nconda create --name pycaret python=3.7\nconda activate pycaret\n\nproxychains pip install pycaret shap\nproxychains conda install -c conda-forge  nb_conda jupyter_contrib_nbextensions fire pyfiglet openpyxl\njupyter contrib nbextensions install --user\nproxychains conda upgrade nbconvert"
  },
  {
    "objectID": "posts/2021-04-09-wsl2 cuda conda.html#pytorch",
    "href": "posts/2021-04-09-wsl2 cuda conda.html#pytorch",
    "title": "setup wsl2 with cuda and conda",
    "section": "pytorch",
    "text": "pytorch\nproxychains conda create -n pytorch python=3.8\nproxychains conda activate pytorch\nproxychains conda install -c pytorch pytorch=1.7.1 torchvision\nproxychains conda install jupyter\nproxychains conda install -c conda-forge jupyter_contrib_nbextensions"
  },
  {
    "objectID": "posts/2021-05-01-logbook-May.html",
    "href": "posts/2021-05-01-logbook-May.html",
    "title": "Logbook for May 21",
    "section": "",
    "text": "Monday 5/3\nReinforcement Learning Specialization - enrolled - Coursera (University of Alberta) - start of course 1 (Fundamentals of Reinforcement Learning) week 1 (An introduction to Sequential Decision-Making)\nTuesday 5/4\nReinforcement Learning Specialization - end of assignement for C1W1\nRL Course by David Silver Policy Gradient Methods (lecture 7)\nFriday 5/7\nReinforcement Learning Specialization - C1W2 - Markov Decision Process"
  },
  {
    "objectID": "posts/2021-05-01-logbook-May.html#week-19---may-21",
    "href": "posts/2021-05-01-logbook-May.html#week-19---may-21",
    "title": "Logbook for May 21",
    "section": "Week 19 - May 21",
    "text": "Week 19 - May 21\nMonday 5/10\nReinforcement Learning Specialization - C1W3 - Value Functions & Bellman Equations"
  },
  {
    "objectID": "posts/2021-05-01-logbook-May.html#week-20---may-21",
    "href": "posts/2021-05-01-logbook-May.html#week-20---may-21",
    "title": "Logbook for May 21",
    "section": "Week 20 - May 21",
    "text": "Week 20 - May 21\nMonday 5/17\nReinforcement Learning Specialization - C1W3 - Value Functions & Bellman Equations\nTuesday 5/18\nReinforcement Learning Specialization - C1W4 - Dynamic Programming\nFriday 5/21\nReinforcement Learning Specialization - C1W4 - Dynamic Programming - assignment completed and end of course 1 ;)\nStart of Machine learning in python with scikit-learn by Inria"
  },
  {
    "objectID": "posts/2021-05-01-logbook-May.html#week-21---may-21",
    "href": "posts/2021-05-01-logbook-May.html#week-21---may-21",
    "title": "Logbook for May 21",
    "section": "Week 21 - May 21",
    "text": "Week 21 - May 21\nTuesday 5/25\nMachine learning in python with scikit-learn end of module 1\nReinforcement Learning Specialization - C2W1 - Monte-Carlo\nWednesday 5/26\nReinforcement Learning Specialization - C2W1 - Monte-Carlo - end of week 1\nThursday 5/27\nMachine learning in python with scikit-learn start of module 2. Selecting the best model\nReinforcement Learning Specialization - C2W2 - Temporal Difference for Prediction - start"
  },
  {
    "objectID": "posts/2021-05-01-logbook-May.html#week-22---may-21",
    "href": "posts/2021-05-01-logbook-May.html#week-22---may-21",
    "title": "Logbook for May 21",
    "section": "Week 22 - May 21",
    "text": "Week 22 - May 21\nMonday 5/31\nMachine learning in python with scikit-learn start of module 3. Hyperparameter tuning"
  },
  {
    "objectID": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html",
    "href": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html",
    "title": "Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning",
    "section": "",
    "text": "Coursera website: course 1 - Fundamentals of Reinforcement Learning of Reinforcement Learning Specialization\nmy notes on course 2 - Sample-based Learning Methods, course 3 - Prediction and Control with Function Approximation, course 4 - A Complete Reinforcement Learning System (Capstone)\nSyllabus\n4 courses on 16 weeks by Martha White and Adam White.\nspecialization roadmap\ncourse 1 - we begin our study with multi-arm bandit problems. Here, we get our first taste of the complexities of incremental learning, exploration, and exploitation. After that, we move onto Markov decision processes to broaden the class of problems we can solve with reinforcement learning methods. Here we will learn about balancing short-term and long-term reward. We will introduce key ideas like policies and value functions using almost all RL systems. We conclude Course 1 with classic planning methods called dynamic programming. These methods have been used in large industrial control problems and can compute optimal policies given a complete model of the world.\ncourse 2 - In Course 2, we built on these ideas and design algorithms for learning without a model of the world. We study three classes of methods designed for learning from trial and error interaction. We start with Monte Carlo methods and then move on to temporal difference learning, including Q learning. We conclude Course 2 with an investigation of methods for planning with learned models.\ncourse 3 - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with function approximation. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger infinite state spaces. We will cover feature construction, neural network learning, policy gradient methods, and other particularities of the function approximation setting.\ncourse 4 - The final course in this specialization brings everything together in a Capstone project. Throughout this specialization, as in Rich and Andy’s book, we stress a rigorous and scientific approach to RL. We conduct numerous experiments designed to carefully compare algorithms. It takes careful planning and a lot of hard work to produce a meaningful empirical results. In the Capstone, we will walk you through each step of this process so that you can conduct your own scientific experiment. We will explore all the stages from problem specification, all the way to publication quality plots. This is not just academic. In real problems, it’s important to verify and understand your system. After that, you should be ready to test your own new ideas or tackle a new exciting application of RL in your job. We hope you enjoyed the show half as much as we enjoyed making it for you.\nAlberta is in Canada."
  },
  {
    "objectID": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-1---an-introduction-to-sequential-decision-making",
    "href": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-1---an-introduction-to-sequential-decision-making",
    "title": "Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning",
    "section": "5/3/21 - Course 1 - Week 1 - An introduction to Sequential Decision-Making",
    "text": "5/3/21 - Course 1 - Week 1 - An introduction to Sequential Decision-Making\nI have set recommended goals 3 times a week.\nabout supervised learning, unsupervised learning and RL\nYou might wonder what’s the difference between supervised learning, unsupervised learning, and reinforcement learning? The differences are quite simple. In supervised learning we assume the learner has access to labeled examples giving the correct answer. In RL, the reward gives the agent some idea of how good or bad its recent actions were. You can think of supervised learning as requiring a teacher that helps you by telling you the correct answer. A reward on the other hand, is like having someone who can identify what good behavior looks like but can’t tell you exactly how to do it. Unsupervised learning sounds like it could be related but really has a very different goal. Unsupervised learning is about extracting underlying structure in data. It’s about the data representation. It can be used to construct representations that make a supervised or RL system better. In fact, as you’ll see later in this course, techniques from both supervised learning and unsupervised learning can be used within RL to aid generalization\nindustrial control\nSo I think the place we’re really going to see it take off is an industrial control. In industrial control, we have experts that are really looking for ways to improve the optimal- how well their systems work. So we’re going to see it do things like reduce energy costs or save on other types of costs that we have in these industrial control systems. In the hands of experts, we can really make these algorithms work well in the near future. So I really see it as a tool that’s going to facilitate experts in their work rather than say, doing something like replacing people or automating them away.\nReinforcement Learning Textbook\nas always, Reinforcement Learning: An introduction (Second Edition) by Richard S. Sutton and Andrew G. Barto is THE reference. I didn’t know that Adam White was student from Sutton. Lucky guy ;)\nK-armed Bandit problem\n\nStarts with reading of RLbook p25-36 (Chapter 2 Multi-armed Bandits)\nEvaluative vs instructive feedback. Nonassociative refers to cases where you take one action per state. At the end there is a generalization where bandit problem becomes associative, that is, when actions are taken in more than one situation.\nIt is a stationary case meaning that value of actions are fixed during experiences. If the bandit task were nonstationary, that is, the true values of the actions changed over time. In this case exploration is needed even in the deterministic case to make sure one of the nongreedy actions has not changed to become better than the greedy one.\nsample-average action-value estimates\n\\[\nQ_t(a) = \\frac{\\text{sum of rewards when } \\mathit{a} \\text{ taken prior to }\\mathit{t}}{\\text{number of times } \\mathit{a} \\text{ taken prior to }\\mathit{t}} \\\\\nQ_t(a)  = \\frac{\\displaystyle\\sum_{i=1}^{t-1} R_i.\\mathcal{1}_{A_i=a}}{\\displaystyle\\sum_{i=1}^{t-1} \\mathcal{1}_{A_i=a}}\n\\]\n\\(\\epsilon\\)-greedy action selection\n\\[\nA_t=\\underset{a}{\\mathrm{argmax}}{\\text{ }Q_t(a)}\n\\]\nWith nonstationary problem, we want to give more weights to recent rewards. It can be done with \\[\nQ_{n+1}=Q_n+\\alpha[R_n-Q_n]\n\\] Where \\[\\alpha\\] is a constant step-size parameter, \\[\\alpha \\in [0,1]\\]. So it can be written that way \\[\nQ_{n+1}=(1-\\alpha)^nQ_1+\\displaystyle\\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i}R_i\n\\] . Weighted average because the sum of the weights is 1.\n2 other topics are discussed: optimistic initial values (that can push exploration in 1st steps) and upper-confidence-bound (UCB) action selection. With optimistic initial values the idea is too have high initial value for reward so that the 1st actions are disappointing pushing for explorations. With UCB\n\\[\nA_t= \\underset{a} {\\mathrm{argmax}} {\\text{ }\\bigg[Q_t(a)+c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\bigg]}\n\\]\n\nThe idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a’s value. The quantity being max’ed over is thus a sort of upper bound on the possible true value of action a, with c determining the confidence level. Each time a is selected the uncertainty is presumably reduced: N t (a) increments, and, as it appears in the denominator, the uncertainty term decreases. On the other hand, each time an action other than a is selected, t increases but N t (a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time.\n\nExploration vs Exploitation trade-off\nHow do we choose when to explore, and when to exploit? Randomly\n\nAssignement\nimplementation of greedy agent, \\(\\epsilon\\)-greedy agent. Comparisons. Various \\(\\epsilon\\) values, various step-sizes (1/N(a), …)\nnotebooks in github\nend of C1W1 (course 1 week 1)"
  },
  {
    "objectID": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-2---markov-decision-process",
    "href": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-2---markov-decision-process",
    "title": "Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning",
    "section": "5/7/21 - Course 1 - Week 2 - Markov Decision Process",
    "text": "5/7/21 - Course 1 - Week 2 - Markov Decision Process\n\nModule 2 Learning Objectives\nLesson 1: Introduction to Markov Decision Processes\n\nUnderstand Markov Decision Processes, or MDPs\nDescribe how the dynamics of an MDP are defined\nUnderstand the graphical representation of a Markov Decision Process\nExplain how many diverse processes can be written in terms of the MDP framework\n\nLesson 2: Goal of Reinforcement Learning\n\nDescribe how rewards relate to the goal of an agent\nUnderstand episodes and identify episodic tasks\n\nLesson 3: Continuing Tasks\n\nFormulate returns for continuing tasks using discounting\nDescribe how returns at successive time steps are related to each other\nUnderstand when to formalize a task as episodic or continuing\n\n\n\nLesson 1: Introduction to Markov Decision Processes\nReading chapter 3.1 to 3.3 (p47-56) in Sutton’s book\nFinite Markov Decision Processes\n\n3.1 - the Agent-Environment Interface\n3.2 - Goals and Rewards\n3.3 - Returns and Episodes\n\nIn a Markov decision process, the probabilities given by p completely characterize the environment’s dynamics. That is, the probability of each possible value for \\(S_t\\) and \\(R_t\\) depends only on the immediately preceding state and action, \\(S_{t-1}\\) and \\(A_{t-1}\\) , and, given them, not at all on earlier states and actions.\n\\[\np(s',r|s,a) \\doteq Pr\\{S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a\\}\n\\]\nThe state must include information about all aspects of the past agent–environment interaction that make a difference for the future. In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them.\nThe agent–environment boundary represents the limit of the agent’s absolute control, not of its knowledge.\nGoal can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). The reward signal is your way of communicating to the agent what you want it to achieve, not how you want it achieved.\nExpected return \\(G_t\\) is defined as some specific function of the reward sequence. In the simplest case the return is the sum of the rewards: \\[\nG_t \\doteq R_{t+1}+R_{t+2}+R_{t+3}+...+R_{T}\n\\] where \\(T\\) is the final time step.\nWith continuing tasks, we can have \\(T=\\infty\\), we can then introduce discounting. Agent chooses \\(A_t\\) to maximize the expected discounted return: \\[\nG_t \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...=\\displaystyle\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\] where \\(\\gamma\\) is called the discount rate. \\[\nG_t = R_{t+1}+\\gamma G_{t+1}\n\\] Video MDP by Martha. By the end of this video: Understand Markov Decision Process (MDP), Describe how the dynamics of an MDP are defined.\nMartha highlights differences between k-armed bandit and MDP. The k-armed bandit agent is presented with the same situation at each time and the same action is always optimal. In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future. In particular if state changes, k-armed bandit don’t adapt. It is why we need MDP.\nVideo examples of MDPs by Adam . By the end of this video: Gain experience formalizing decision-making problems as MDPs, Appreciate the flexibility of the MDP formalism.\nAdam uses 2 examples: robot recycling cans and robot arm.\n\n\nLesson 2: Goal of Reinforcement Learning\nVideo the Goal of Reinforcement Learning by Adam. By the end of this video: Describe how rewards relate to the goal of an agent, Identify episodic tasks.\nWith MDP, agents can have long-term goals.\nVideo the Reward Hypothesis by Michael Littman.\nHe gives a nice idea when defining reward hypothesis: a contrast between the simplicity of the idea of rewards with the complexity of the real world.\n\n\nLesson 3: Continuing Tasks\nVideo Continuing Tasks by Martha. By the end of this video: Differentiate between episodic and continuing tasks. Formulate returns for continuing tasks using discounting. Describe how returns at successive time steps are related to each other.\nAdam uses a link to Sutton’s book., This is a 2020 version of this book.\nVideo Examples of Episodic and Continuing Tasks by Martha. By the end of this video: Understand when to formalize a task as episodic or continuing.\nMartha gives 2 examples: one of an episodic tasks where episode ends when player is touched by an enemy, one of continuous tasks where an agent accepts or rejects tasks depending on priority and servers available (never ending episode).\nWeekly assessment.\nThis is a quizz and a peer-graded assignment. I had to describe 3 MDPs with all its detail (states actions, rewards)."
  },
  {
    "objectID": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-3---value-functions-bellman-equations",
    "href": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-3---value-functions-bellman-equations",
    "title": "Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning",
    "section": "5/10/21 - Course 1 - Week 3 - Value Functions & Bellman Equations",
    "text": "5/10/21 - Course 1 - Week 3 - Value Functions & Bellman Equations\n\nModule 3 Learning Objectives\nLesson 1: Policies and Value Functions\n\nRecognize that a policy is a distribution over actions for each possible state\nDescribe the similarities and differences between stochastic and deterministic policies\nIdentify the characteristics of a well-defined policy\nGenerate examples of valid policies for a given MDP\nDescribe the roles of state-value and action-value functions in reinforcement learning\nDescribe the relationship between value functions and policies\nCreate examples of valid value functions for a given MDP\n\nLesson 2: Bellman Equations\n\nDerive the Bellman equation for state-value functions\nDerive the Bellman equation for action-value functions\nUnderstand how Bellman equations relate current and future values\nUse the Bellman equations to compute value functions\n\nLesson 3: Optimality (Optimal Policies & Value Functions)\n\nDefine an optimal policy\nUnderstand how a policy can be at least as good as every other policy in every state\nIdentify an optimal policy for given MDPs\nDerive the Bellman optimality equation for state-value functions\nDerive the Bellman optimality equation for action-value functions\nUnderstand how the Bellman optimality equations relate to the previously introduced Bellman equations\nUnderstand the connection between the optimal value function and optimal policies\nVerify the optimal value function for given MDPs\n\n\n\nLesson 1: Policies and Value Functions\nReading chapter 3.5 to 3.8 (p58-67) in Sutton’s book\nAlmost all reinforcement learning algorithms involve estimating value functions—functions of states (or of state–action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state).\nSearching for additional informations, I have fallen into ShangtongZhang page and repos. Only 2 of them but seem to be great: reinforcement-learning-an-introduction contains implementations in Python of all concepts from Sutton’s book. DeepRL seems to be a pytorch implementations (DQN, A2C, PPO, …)\nHere we see Bellman equation for state-value function \\(v_\\pi(s)\\)\n\\[\nv_\\pi(s) \\doteq \\mathbb{E}[G_t|S_t=s]\n\\\\\nv_\\pi(s) = \\displaystyle\\sum_{a} \\pi(a|s) \\displaystyle\\sum_{s',r} p(s', r|s, a)\\big[r+\\gamma.v_\\pi(s')\\big]\n\\]\nBellman equation for action-value function \\(q_\\pi(s,a)\\)\n\\[\nq_\\pi(s,a) \\doteq \\mathbb{E}[R_{t+1}+\\gamma.G_{t+1}|S_t=s, A_t=a]\n\\\\\nq_\\pi(s, a) = \\displaystyle\\sum_{s',r} p(s', r|s, a) \\big[ r + \\gamma\\displaystyle\\sum_{a'} \\pi(s', a')q_\\pi(s',a') \\big]\n\\]\nOptimal state-value function \\(v_*\\):\n\\[\nv_*(s)\\doteq \\max\\limits_{\\pi} v_\\pi(s), \\forall s \\in S\n\\]\nOptimal action-value function \\(q_*\\): \\[\nq_*(s,a) \\doteq \\max\\limits_{\\pi} q_\\pi(s,a) = \\mathbb{E}[R_{t+1}+\\gamma.v_*(S_{t+1})|S_t=s, A_t=a]\n\\]\nWe denote all optimal policies by \\(\\pi_*\\)\nBellman optimality equation for \\(v_*\\)\n\\[\nv_*(s) = \\max\\limits_{a} \\displaystyle\\sum_{s', r} p(s',r|s, a)\\big[ r + \\gamma .v_*(s') \\big]\n\\]\nBellman optimality equation for \\(q_*\\)\n\\[\nq_*(s,a) = \\displaystyle\\sum_{s',r} p(s', r|s, a) \\big[ r + \\gamma.\\max\\limits_{a'} q_*(s',a') \\big]\n\\]\n\nVideo Specifying Policies by Adam.\nBy the end of this video, you’ll be able to\nRecognize that a policy is a distribution over actions for each possible state, describe the similarities and differences between stochastic and deterministic policies, and generate examples of valid policies for a given MDP or Markup Decision Process.\nVideo Value Functions by Adam.\nBy the end of this video, you’ll be able to\ndescribe the roles of the state-value and action-value functions in reinforcement learning, describe the relationship between value-functions and policies, and create examples of value-functions for a given MDP.\nVideo Rich Sutton and Andy Barto: A brief History of RL\n\n\nLesson 2: Bellman Equations\nVideo Bellman Equation Derivation by Martha\nBy the end of this video, you’ll be able to derive the Bellman equation for state-value functions, derive the Bellman equation for action-value functions, and understand how Bellman equations relate current and future values.\nVideo Why Bellman Equations? by Martha\nBy the end of this video, you’ll be able to use the Bellman equations to compute value functions\n\n\nLesson 3: Optimality (Optimal Policies & Value Functions)\nVideo Optimal Policies by Martha\nBy the end of this video, you will be able to define an optimal policy, understand how policy can be at least as good as every other policy in every state, and identify an optimal policy for a given MDP.\nVideo Optimal Value Functions by Martha\nBy the end of this video, you will be able to derive the Bellman optimality equation for the state-value function, derive the Bellman optimality equation for the action-value function, and understand how the Bellman optimality equations relate to the previously introduced Bellman equations.\nVideo Using Optimal Value Functions to Get Optimal Policies by Martha\nBy the end of this video, you’ll be able to understand the connection between the optimal value function and optimal policies and verify the optimal value function for a given MDP\nVideo week 3 summary by Adam\n\n\n\nPolicies"
  },
  {
    "objectID": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-4---dynamic-programming",
    "href": "posts/2021-05-03-reinforcement-learning-specialization-coursera.html#course-1---week-4---dynamic-programming",
    "title": "Reinforcement Learning Specialization - Coursera - course 1 - Fundamentals of Reinforcement Learning",
    "section": "5/18/21 - Course 1 - Week 4 - Dynamic Programming",
    "text": "5/18/21 - Course 1 - Week 4 - Dynamic Programming\n\nModule 4 Learning Objectives\nLesson 1: Policy Evaluation (Prediction)\n\nUnderstand the distinction between policy evaluation and control\nExplain the setting in which dynamic programming can be applied, as well as its limitations\nOutline the iterative policy evaluation algorithm for estimating state values under a given policy\n\nApply iterative policy evaluation to compute value functions\n\nLesson 2: Policy Iteration (Control)\n\nUnderstand the policy improvement theorem\nUse a value function for a policy to produce a better policy for a given MDP\nOutline the policy iteration algorithm for finding the optimal policy\nUnderstand “the dance of policy and value”\nApply policy iteration to compute optimal policies and optimal value functions\n\nLesson 3: Generalized Policy Iteration\n\nUnderstand the framework of generalized policy iteration\nOutline value iteration, an important example of generalized policy iteration\nUnderstand the distinction between synchronous and asynchronous dynamic programming methods\nDescribe brute force search as an alternative method for searching for an optimal policy\nDescribe Monte Carlo as an alternative method for learning a value function\nUnderstand the advantage of Dynamic programming and “bootstrapping” over these alternative strategies for finding the optimal policy\n\n\n\nLesson 1: Policy Evaluation (Prediction)\nReading chapter 4.1, 4.2, 4.3, 4.4, 4.6, 4.7 (pages 73-88) in Sutton’s book (with the help of Solutions_to_Reinforcement_Learning_by_Sutton_Chapter_4_r5.pdf)\nA common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply finite-state DP methods.\nVideo Policy Evaluation vs. Control by Martha\nBy the end of this video you will be able to understand the distinction between policy evaluation and control, and explain the setting in which dynamic programming can be applied as well as its limitations.\nVideo Iterative Policy Evaluation by Martha\nBy the end of this video you will be able to outline the iterative policy evaluation algorithm for estimating state values for a given policy, and apply iterative policy evaluation to compute value functions.\nThe magic here is to turn the bellman equation into an iterative evaluation which converges to \\(v_\\pi\\).\n\n\n\nLesson 2: Policy Iteration (Control)\nVideo Policy Improvement by Marta\nBy the end of this video, you will be able to understand the policy improvement theorem, and how it can be used to construct improved policies, and use the value function for a policy to produce a better policy for a given MDP.\nGreedified policy is a strict improvement.\n\nVideo Policy Iteration by Marta\nBy the end of this video, you will be able to outline the policy iteration algorithm for finding the optimal policy, understand the dance of policy and value, how policy iteration reaches the optimal policy by alternating between evaluating policy and improving it, and apply policy iteration to compute optimal policies and optimal value functions.\n\n\n\n\nLesson 3: Generalized Policy Iteration\nVideo Flexibility of the Policy Iteration Framework by Adam\nBy the end of this video, you’ll be able to understand the framework of generalized policy iteration, outline value iteration and important special case of generalized policy iteration, and differentiate synchronous and asynchronous dynamic programming methods.\nVideo Efficiency of Dynamic Programming by Adam\nBy the end of this video, you’ll be able to describe Monte Carlo sampling as an alternative method for learning a value function. Describe brute force-search as an alternative method for finding an optimal policy. And understand the advantages of dynamic programming and bootstrapping over these alternatives.\nThe most important takeaway is that bootstrapping can save us from performing a huge amount of unnecessary work by exploiting the connection between the value of a state and its possible successors.\nVideo Warren Powell: Approximate Dynamic Programming for Fleet Management (Short)\nVideo Week 4 Summary by Adam\nReading chapter summary Chapter 4.8, (pages 88-89)\n\n\nAssignment\nOptimal Policies with Dynamic Programming\nnotebooks in github\nend of C1W4 (course 1 week 4)\nend of course 1 (and with a certificate ;) )"
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html",
    "title": "Machine learning in python with scikit-learn",
    "section": "",
    "text": "This is a MOOC by Inria team, in charge of scikit-learn.\nSyllabus\nIntroduction: Machine Learning concepts, then\nModule 1. The Predictive Modeling Pipeline\nModule 2. Selecting the best model\nModule 3. Hyperparameter tuning\nModule 4. Linear Models\nModule 5. Decision tree models\nModule 6. Ensemble of models\nModule 7. Evaluating model performance\nINRIA github contains everything of this mooc: slides, datasets, notebooks (not videos)\nI have forked it, and I use local envt for assignments."
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#introduction---machine-learning-concepts",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#introduction---machine-learning-concepts",
    "title": "Machine learning in python with scikit-learn",
    "section": "Introduction - Machine Learning concepts",
    "text": "Introduction - Machine Learning concepts\nslides"
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-1.-the-predictive-modeling-pipeline",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-1.-the-predictive-modeling-pipeline",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 1. The Predictive Modeling Pipeline",
    "text": "Module 1. The Predictive Modeling Pipeline\n\nModule overview\n\nThe objective in the module are the following:\n\nbuild intuitions regarding an unknown dataset;\nidentify and differentiate numerical and categorical features;\ncreate an advanced predictive pipeline with scikit-learn.\n\n\n\n\nTabular data exploration\nexploration of data: 01_tabular_data_exploration.ipynb\nexercise M1.01: 01_tabular_data_exploration_ex_01.ipynb\n\n\nFitting a scikit-learn model on numerical data\nfirst model with scikit-learn: 02_numerical_pipeline_introduction.ipynb\nexercise M1.02: 02_numerical_pipeline_ex_00.ipynb\nworking with numerical data: 02_numerical_pipeline_hands_on.ipynb\nexercise M1.03: 02_numerical_pipeline_ex_01.ipynb\npreprocessing for numerical features: 02_numerical_pipeline_scaling.ipynb\n\n\nHandling categorical data\nEncoding of categorical variables: 03_categorical_pipeline.ipynb\n\nThus, in general OneHotEncoder is the encoding strategy used when the downstream models are linear models while OrdinalEncoder is used with tree-based models.\n\nExercise M1.04: 03_categorical_pipeline_ex_01.ipynb\nUsing numerical and categorical variables together: 03_categorical_pipeline_column_transformer.ipynb\nExercise M1.05: 03_categorical_pipeline_ex_02.ipynb\n\n\nWrap-up quiz\nmodule 1 - wrap-up quizz.ipynb"
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-2.-selecting-the-best-model",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-2.-selecting-the-best-model",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 2. Selecting the best model",
    "text": "Module 2. Selecting the best model\n\nModule overview\n\nThe objective in the module are the following:\n\nunderstand the concept of overfitting and underfitting;\nunderstand the concept of generalization;\nunderstand the general cross-validation framework used to evaluate a model.\n\n\n\n\nOverfitting and Underfitting\nvideo and slides\n\n\nThe framework and why do we need it: cross_validation_train_test.ipynb\n\n\nValidation and learning curves\nvideo and slides\n\n\nOverfit-generalization-underfit: cross_validation_validation_curve.ipynb\nEffect of the sample size in cross-validation: cross_validation_learning_curve.ipynb\nExercise M2.01: cross_validation_ex_01.ipynb solution\n\n\nBias versus variance trade-off\nvideo and slides\n\n\n\n\n\nWrap-up quiz\nmodule 2 - wrap-up quizz.ipynb\n\n\nOverfitting is caused by the limited size of the training set, the noise in the data, and the high flexibility of common machine learning models.\nUnderfitting happens when the learnt prediction functions suffer from systematic errors. This can be caused by a choice of model family and parameters, which leads to a lack of flexibility to capture the repeatable structure of the true data generating process.\nFor a fixed training set, the objective is to minimize the test error by adjusting the model family and its parameters to find the best trade-off between overfitting for underfitting.\nFor a given choice of model family and parameters, increasing the training set size will decrease overfitting but can also cause an increase of underfitting.\nThe test error of a model that is neither overfitting nor underfitting can still be high if the variations of the target variable cannot be fully determined by the input features. This irreducible error is caused by what we sometimes call label noise. In practice, this often happens when we do not have access to important features for one reason or another."
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-3.-hyperparameter-tuning",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-3.-hyperparameter-tuning",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 3. Hyperparameter tuning",
    "text": "Module 3. Hyperparameter tuning\n\nModule overview\n\nThe objective in the module are the following:\n\nunderstand what is a model hyperparameter;\nunderstand how to get and set the value an hyperparameter of a scikit-learn model;\nbe able to fine tune a full predictive modeling pipeline;\nunderstand and visualize the combination of parameters that improves the performance of a model.\n\n\n\n\nManual tuning\nSet and get hyperparameters in scikit-learn: parameter_tuning_manual.ipynb\nExercise M3.01: parameter_tuning_ex_02.ipynb\n\n\nAutomated tuning\nHyperparameter tuning by grid-search: parameter_tuning_grid_search.ipynb\nHyperparameter tuning by randomized-search: parameter_tuning_randomized_search.ipynb\nCross-validation and hyperparameter tuning: parameter_tuning_nested.ipynb\nExercise M3.01: parameter_tuning_ex_03.ipynb solution\n\nNice to play with interactive plotly parallel_coordinates to identify best params.\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\ndef shorten_param(param_name):\n    if \"__\" in param_name:\n        return param_name.rsplit(\"__\", 1)[1]\n    return param_name\ncv_results = pd.read_csv(\"../figures/randomized_search_results.csv\",\n                         index_col=0)\n\nfig = px.parallel_coordinates(\n    cv_results.rename(shorten_param, axis=1).apply({\n        \"learning_rate\": np.log10,\n        \"max_leaf_nodes\": np.log2,\n        \"max_bins\": np.log2,\n        \"min_samples_leaf\": np.log10,\n        \"l2_regularization\": np.log10,\n        \"mean_test_score\": lambda x: x}),\n    color=\"mean_test_score\",\n    color_continuous_scale=px.colors.sequential.Viridis,\n)\nfig.show()\n\n\nWrap-up quiz\nmodule 3 - wrap-up quizz.ipynb\n\n\nHyperparameters have an impact on the models’ performance and should be wisely chosen;\nThe search for the best hyperparameters can be automated with a grid-search approach or a randomized search approach;\nA grid-search is expensive and does not scale when the number of hyperparameters to optimize increase. Besides, the combination are sampled only on a regular grid.\nA randomized-search allows a search with a fixed budget even with an increasing number of hyperparameters. Besides, the combination are sampled on a non-regular grid."
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-4.-linear-models",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-4.-linear-models",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 4. Linear models",
    "text": "Module 4. Linear models\n\nModule overview\n\nIn this module, your objectives are to:\n\nunderstand the linear models parametrization;\nunderstand the implication of linear models in both regression and classification;\nget intuitions of linear models applied in higher dimensional dataset;\nunderstand the effect of regularization and how to set it;\nunderstand how linear models can be used even with data showing non-linear relationship with the target to be predicted.\n\n\n\n\nIntuitions on linear models\nvideo and slides\n\n\nFor regression: linear regression\nfrom sklearn.linear_model import LinearRegression\nlinear_regression = LinearRegression()\nlinear_regression.fit(X, y)\nFor classification: logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\n\n\nLinear regression\nLinear regression without scikit-learn: linear_regression_without_sklearn.ipynb\nExercise M4.01: linear_models_ex_01.ipynb solution\nusage of np.ravel in\ndef goodness_fit_measure(true_values, predictions):\n    # we compute the error between the true values and the predictions of our model\n    errors = np.ravel(true_values) - np.ravel(predictions)\n    return np.mean(np.abs(errors))\nLinear regression using scjkit-learn: linear_regression_in_sklearn.ipynb\nfrom sklearn.metrics import mean_squared_error\ninferred_body_mass = linear_regression.predict(data)\nmodel_error = mean_squared_error(target, inferred_body_mass)\nprint(f\"The mean squared error of the optimal model is {model_error:.2f}\")\n\n\nModeling non-linear features-target relationships\nExercise M4.02: linear_models_ex_02.ipynb solution\nLinear regression with non-linear link between data and target: linear_regression_non_linear_link.ipynb\nExercise M4.03: linear_models_ex_03.ipynb solution\n\n\nRegularization in linear model\nvideo and slides\n\n\nRidge regression\nfrom sklearn.linear_model import Ridge\nmodel = Ridge(alpha=0.01).fit(X, y)\nalways use Ridge with a carefully tuned alpha!\nfrom sklearn.linear_model import RidgeCV\nmodel = RidgeCV( alphas=[0.001, 0.1, 1, 10, 1000] )\nmodel.fit(X, y)\nprint(model.alpha_)\nRegularization of linear regression model: linear_models_regularization.ipynb\nExercise M4.04: linear_models_ex_04.ipynb solution\n\n\nLinear model for classification\nLinear model for classification: logistic_regression.ipynb\nExercise M4.05: linear_models_ex_05.ipynb solution\nBeyond linear separation in classification: logistic_regression_non_linear.ipynb\n\n\nWrap-up quiz\nmodule 4 - wrap-up quizz.ipynb\n\nIn this module, we saw that:\n\nthe predictions of a linear model depend on a weighted sum of the values of the input features added to an intercept parameter;\nfitting a linear model consists in adjusting both the weight coefficients and the intercept to minimize the prediction errors on the training set;\nto train linear models successfully it is often required to scale the input features approximately to the same dynamic range;\nregularization can be used to reduce over-fitting: weight coefficients are constrained to stay small when fitting;\nthe regularization hyperparameter needs to be fine-tuned by cross-validation for each new machine learning problem and dataset;\nlinear models can be used on problems where the target variable is not linearly related to the input features but this requires extra feature engineering work to transform the data in order to avoid under-fitting."
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-5.-decision-tree-models",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-5.-decision-tree-models",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 5. Decision tree models",
    "text": "Module 5. Decision tree models\n\nModule overview\n\nThe objective in the module are the following:\n\nunderstand how decision trees are working in classification and regression;\ncheck which tree parameters are important and their influences.\n\n\n\n\nIntuitions on tree-based models\nvideo and slides\n\n\n\n\nDecision tree in classification\nBuild a classification decision tree: trees_classification.ipynb\nExercise M5.01: trees_ex_01.ipynb solution\nFit and decision boundaries\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\n\n# create a palette to be used in the scatterplot\npalette = [\"tab:red\", \"tab:blue\", \"black\"]\n\ntree = DecisionTreeClassifier(max_depth=2)\ntree.fit(data_train, target_train)\n\nax = sns.scatterplot(data=penguins, x=culmen_columns[0], y=culmen_columns[1],\n                     hue=target_column, palette=palette)\nplot_decision_function(tree, range_features, ax=ax)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n_ = plt.title(\"Decision boundary using a decision tree\")\nDecision tree\nfrom sklearn.tree import plot_tree\n\n_, ax = plt.subplots(figsize=(17, 12))\n_ = plot_tree(tree, feature_names=culmen_columns,\n              class_names=tree.classes_, impurity=False, ax=ax)\nAccuracy\ntree.fit(data_train, target_train)\ntest_score = tree.score(data_test, target_test)\nprint(f\"Accuracy of the DecisionTreeClassifier: {test_score:.2f}\")\n\n\nDecision tree in regression\nDecision tree for regression: trees_regression.ipynb\nExercise M5.02: trees_ex_02.ipynb solution\n\n\nHyperparameters of decision tree\nImportance of decision tree hyperparameters on generalization: trees_hyperparameters.ipynb\n\n\nWrap-up quiz\nmodule 5 - wrap-up quizz.ipynb\nMain take-away | Main take-away | 41026 Courseware | FUN-MOOC\n\nIn this module, we presented decision trees in details. We saw that they:\n\nare suited for both regression and classification problems;\nare non-parametric models;\nare not able to extrapolate;\nare sensible to hyperparameter tuning."
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-6.-ensemble-of-models",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-6.-ensemble-of-models",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 6. Ensemble of models",
    "text": "Module 6. Ensemble of models\n\nModule overview\n\nThe objective in the module are the following:\n\nunderstanding the principles behind bootstrapping and boosting;\nget intuitions with specific models such as random forest and gradient boosting;\nidentify the important hyperparameters of random forest and gradient boosting decision trees as well as their typical values.\n\n\n\n\nIntuitions on ensemble of tree-based models\nvideo and slides\n\n\n“Bagging” stands for Bootstrap AGGregatING. It uses bootstrap resampling (random sampling with replacement) to learn several models on random variations of the training set. At predict time, the predictions of each learner are aggregated to give the final predictions.\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nRandom Forests are bagged randomized decision trees\n\nAt each split: a random subset of features are selected\nThe best split is taken among the restricted subset\nExtra randomization decorrelates the prediction errors\nUncorrelated errors make bagging work better\n\nGradient Boosting\n\nEach base model predicts the negative error of previous models\nsklearn use decision trees as the base model\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nImplementation of the traditional (exact) method\nFine for small data sets\nToo slow for n_samples > 10,000\n\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nDiscretize numerical features (256 levels)\nEfficient multi core implementation\nMuch, much faster when n_samples is large\n\nTake away\n\nBagging and random forests fit trees independently\n\neach deep tree overfits individually\naveraging the tree predictions reduces overfitting\n\n(Gradient) boosting fits trees sequentially\n\neach shallow tree underfits individually\nsequentially adding trees reduces underfitting\n\nGradient boosting tends to perform slightly better than bagging and random forest and furthermore shallow trees predict faster.\n\nIntroductory example to ensemble models: ensemble_introduction.ipynb\n\n\nEnsemble method using bootstrapping\nBagging: ensemble_bagging.ipynb\nWikipedia reference to bootstrapping in statistics.\nExercise M6.01: ensemble_ex_01.ipynb (solution)\nRandom Forest: ensemble_random_forest.ipynb\nExercise M6.01: ensemble_ex_02.ipynb (solution)\n\n\nEnsemble method using boosting\nAdaptive Boosting (AdaBoost): ensemble_adaboost.ipynb\nExercise M6.03: ensemble_ex_03.ipynb (solution)\nGradient-boosting decision tree (GBDT): ensemble_gradient_boosting.ipynb\nExercise M6.04: ensemble_ex_04.ipynb (solution)\nSpeeding-up gradient-boosting: ensemble_hist_gradient_boosting.ipynb\n\n\nHyperparameter tuning with ensemble methods\nHyperparameter tuning: ensemble_hyperparameters.ipynb\nExercise M6.05: ensemble_ex_05.ipynb (solution)\n\n\nWrap-up quiz\nmodule 6 - wrap-up quizz.ipynb\nUse of Imbalanced-learn library relying on scikit-learn and provides methods to deal with classification with imbalanced classes."
  },
  {
    "objectID": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-7.-evaluating-model-performance",
    "href": "posts/2021-05-21-Machine-learning-in-python-with-scikit-learn.html#module-7.-evaluating-model-performance",
    "title": "Machine learning in python with scikit-learn",
    "section": "Module 7. Evaluating model performance",
    "text": "Module 7. Evaluating model performance\n\nModule overview\n\nThe objective in the module are the following:\n\nunderstand the necessity of using an appropriate cross-validation strategy depending on the data;\nget the intuitions behind comparing a model with some basic models that can be used as baseline;\nunderstand the principles behind using nested cross-validation when the model needs to be evaluated as well as optimized;\nunderstand the differences between regression and classification metrics;\nunderstand the differences between metrics.\n\n\n\n\nComparing a model with simple baselines\nComparing results with baseline and chance level: cross_validation_baseline.ipynb\nExercise M7.01: cross_validation_ex_02.ipynb (solution)\n\n\nChoice of cross-validation\nIntroductory exercise regarding stratification: cross_validation_ex_03.ipynb\nStratification: cross_validation_stratification.ipynb\nIntroductory exercise for sample grouping: cross_validation_ex_04.ipynb\nSample grouping: cross_validation_grouping.ipynb\nIntroductory exercise for non i.i.d. data: cross_validation_ex_05.ipynb\nNon i.i.d. data: cross_validation_time.ipynb\n\n\nNested cross-validation\nNested cross-validation: cross_validation_nested.ipynb\n\n\nIntroduction of the evaluation metrics: Classification metrics\nClassification: metrics_classification.ipynb\nExercise M7.02: metrics_ex_01.ipynb (solution)\n\n\nIntroduction of the evaluation metrics: Regression metrics\nRegression: metrics_regression.ipynb\nExercise M7.03: metrics_ex_02.ipynb (solution)\n\n\nWrap-up quiz\nmodule 7 - wrap-up quizz.ipynb\nAnd this completes the course"
  },
  {
    "objectID": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html",
    "href": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html",
    "title": "Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods",
    "section": "",
    "text": "Coursera website: course 2 - Sample-based Learning Methods of Reinforcement Learning Specialization\nmy notes on course 1 - Fundamentals of Reinforcement Learning, course 3 - Prediction and Control with Function Approximation, course 4 - A Complete Reinforcement Learning System (Capstone)\nspecialization roadmap - course 2 - Sample-based Learning Methods\ncourse 2 - In Course 2, we built on these ideas and design algorithms for learning without a model of the world. We study three classes of methods designed for learning from trial and error interaction. We start with Monte Carlo methods and then move on to temporal difference learning, including Q learning. We conclude Course 2 with an investigation of methods for planning with learned models.\nWeek 1 - Monte-Carlo Methods for Prediction & Control\nWeek 2 - Temporal Difference Learning Methods for Prediction\nWeek 3 - Temporal Difference Learning Methods for Control\nWeek 4 - Planning, Learning & Acting"
  },
  {
    "objectID": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-1---monte-carlo-methods-for-prediction-control",
    "href": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-1---monte-carlo-methods-for-prediction-control",
    "title": "Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods",
    "section": "Course 2 - Week 1 - Monte-Carlo Methods for Prediction & Control",
    "text": "Course 2 - Week 1 - Monte-Carlo Methods for Prediction & Control\n\nModule 1 Learning Objectives\nLesson 1: Introduction to Monte-Carlo Methods\n\nUnderstand how Monte-Carlo methods can be used to estimate value functions from sampled interaction\nIdentify problems that can be solved using Monte-Carlo methods\nUse Monte-Carlo prediction to estimate the value function for a given policy.\n\nLesson 2: Monte-Carlo for Control\n\nEstimate action-value functions using Monte-Carlo\nUnderstand the importance of maintaining exploration in Monte-Carlo algorithms\nUnderstand how to use Monte-Carlo methods to implement a GPI algorithm\nApply Monte-Carlo with exploring starts to solve an MDP\n\nLesson 3: Exploration Methods for Monte-Carlo\n\nUnderstand why exploring starts can be problematic in real problems\nDescribe an alternative exploration method for Monte-Carlo control\n\nLesson 4: Off-policy learning for prediction\n\nUnderstand how off-policy learning can help deal with the exploration problem\nProduce examples of target policies and examples of behavior policies\nUnderstand importance sampling\nUse importance sampling to estimate the expected value of a target distribution using samples from a different distribution\nUnderstand how to use importance sampling to correct returns\nUnderstand how to modify the Monte-Carlo prediction algorithm for off-policy learning.\n\n\n\nLesson 1: Introduction to Monte Carlo Methods\nReading Chapter 5.0-5.5 (pp. 91-104) in the Reinforcement Learning textbook\n\nAlthough a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP).\n\nVideo What is Monte Carlo by Martha\nBy the end of this video you will be able to understand how Monte Carlo methods can be used to estimate value functions from sampled interaction and identify problems that can be solved using Monte Carlo methods.\nVideo Using Monte Carlo for Prediction by Martha\nBy the end of this video, you will be able to use Monte Carlo prediction to estimate the value function for a given policy.\n\n\n\nLesson 2: Monte Carlo for Control\nVideo Using Monte Carlo for Action Values by Adam\nBy the end of this video, you’ll be able to estimate action-value functions using Monte Carlo and understand the importance of maintaining exploration in Monte Carlo algorithms.\nVideo Using Monte Carlo methods for generalized policy iteration by Adam\nBy the end of this video, you will understand how to use Monte Carlo methods to implement a generalized policy iteration GPI algorithm.\n\nVideo Solving the BlackJack Example by Adam\nBy the end of this video, you’ll be able to apply Monte Carlo with Exploring Starts to solve an example MDP.\n\n\nLesson 3: Exploration Methods for Monte Carlo\nVideo Epsilon-soft policies by Adam\nBy the end of this video you will understand why exploring starts can be problematic in real problems and you will be able to describe an alternative expiration method to maintain exploration in Monte Carlo control.\n\n\nLesson 4: Off-policy Learning for Prediction\nVideo Why does off-policy learning matter? by Martha\nBy the end of this video you will be able to understand how off policy learning can help deal with the expiration problem. You will also be able to produce examples of Target policies and examples of behavior policies.\nThe key points to take away from today are that off policy learning is another way to obtain continual exploration. The policy that we are learning is called the target policy and the policy that we are choosing actions from is the behavior policy.\nVideo Importance Sampling by Martha\nBy the end of this video, you will be able to use importance sampling to estimate the expected value of a target distribution using samples from a different distribution.\nVideo Off-Policy Monte Carlo Prediction by Martha\nBy the end of this video, you will be able to understand how to use important sampling to correct returns, and you will understand how to modify the Monte Carlo prediction algorithm for off-policy learning.\nVideo Emma Brunskill: Batch Reinforcement Learning\nVideo Week 1 Summary by Martha\nReading Chapter 5.10 (pp. 115-116) in the Reinforcement Learning textbook"
  },
  {
    "objectID": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-2---temporal-difference-learning-methods-for-prediction",
    "href": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-2---temporal-difference-learning-methods-for-prediction",
    "title": "Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods",
    "section": "Course 2 - Week 2 - Temporal Difference Learning Methods for Prediction",
    "text": "Course 2 - Week 2 - Temporal Difference Learning Methods for Prediction\n\nModule 2 Learning Objectives\nLesson 1: Introduction to Temporal Difference Learning\n\nDefine temporal-difference learning\nDefine the temporal-difference error\nUnderstand the TD(0) algorithm\n\nLesson 2: Advantages of TD\n\nUnderstand the benefits of learning online with TD\nIdentify key advantages of TD methods over Dynamic Programming and Monte Carlo methods\nIdentify the empirical benefits of TD learning\n\n\n\nLesson 1: Introduction to Temporal Difference Learning\nReading Chapter 6-6.3 (pp. 116-128) in the Reinforcement Learning textbook\nVideo What is Temporal Difference (TD) learning? by Adam\nBy the end of this video, you’ll be able to define temporal difference learning, define the temporal difference error, and understand the TD(0) algorithm.\n\nVideo Rich Sutton: The Importance of TD Learning by Richard Sutton\n\n\nLesson 2: Advantages of TD\nVideo The advantages of temporal difference learning by Martha\nBy the end of this video, you will be able to understand the benefits of learning online with TD and identify key advantages of TD methods over dynamic programming and Monte Carlo.\nVideo Comparing TD and Monte Carlo by Adam\nBy the end of this video, you’ll be able to identify the empirical benefits of TD Learning.\nVideo Andy Barto and Rich Sutton: More on the History of RL\nVideo Week 2 Summary by Adam\n\n\nAssignment\nPolicy Evaluation in Cliff Walking Environment\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-3---temporal-difference-learning-methods-for-control",
    "href": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-3---temporal-difference-learning-methods-for-control",
    "title": "Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods",
    "section": "Course 2 - Week 3 - Temporal Difference Learning Methods for Control",
    "text": "Course 2 - Week 3 - Temporal Difference Learning Methods for Control\n\nModule 3 Learning Objectives\nLesson 1: TD for Control\n\nExplain how generalized policy iteration can be used with TD to find improved policies\nDescribe the Sarsa control algorithm\nUnderstand how the Sarsa control algorithm operates in an example MDP\nAnalyze the performance of a learning algorithm\n\nLesson 2: Off-policy TD Control: Q-learning\n\nDescribe the Q-learning algorithm\nExplain the relationship between Q-learning and the Bellman optimality equations.\nApply Q-learning to an MDP to find the optimal policy\nUnderstand how Q-learning performs in an example MDP\nUnderstand the differences between Q-learning and Sarsa\nUnderstand how Q-learning can be off-policy without using importance sampling\nDescribe how the on-policy nature of Sarsa and the off-policy nature of Q-learning affect their relative performance\n\nLesson 3: Expected Sarsa\n\nDescribe the Expected Sarsa algorithm\nDescribe Expected Sarsa’s behaviour in an example MDP\nUnderstand how Expected Sarsa compares to Sarsa control\nUnderstand how Expected Sarsa can do off-policy learning without using importance sampling\nExplain how Expected Sarsa generalizes Q-learning\n\n\n\nLesson 1: TD for Control\nReading Chapter 6.4-6.6 (pp. 129-134) in the Reinforcement Learning textbook\n\n\nVideo Sarsa: GPI with TD by Martha\nBy the end of this video, you’ll be able to explain how generalized policy iteration can be used with TD to find improved policies, as well as describe the Sarsa control algorithm\nVideo Sarsa in the Windy Grid World by Adam\nBy the end of this video, you will understand how the Sarsa control algorithm operates in an example MDP. You will also gain experience analyzing the performance of a learning algorithm.\n\n\nLesson 2: Off-policy TD Control: Q-learning\nVideo What is Q-learning? by Martha\nBy the end of this video, you will be able to describe the Q-learning algorithm, and explain the relationship between Q-learning and the Bellman optimality equations.\nVideo Q-learning in the Windy Grid World by Adam\nBy the end of this video, you will gain insight into how Q-Learning performs in an example MDP. And gain experience comparing the performance of multiple learning algorithms on a single MDP.\nVideo How is Q-learning off-policy? by Martha\nBy the end of this video, you will understand how Q-learning can be off-policy without using important sampling and be able to describe how learning on-policy or off-policy might affect performance in control.\n\n\nLesson 3: Expected Sarsa\nVideo Expected Sarsa by Martha\nBy the end of this video, you will be able to explain the expected Sarsa algorithm.\nVideo Expected Sarsa in the Cliff World by Adam\nBy the end of this video, you will be able to describe expected Sarsas’s behavior in an example MDP and empirically compare expected Sarsa and Sarsa.\nVideo Generality of Expected Sarsa by Martha\nBy the end of this video, you will understand how Expected Sarsa can do off-policy learning without using importance sampling and explain how Expected Sarsa generalizes Q-learning.\nVideo Week 3 summary by Adam\n\nSarsa uses a sample based version of the Bellman equation. It learns Q-pi.\nQ-learning uses the Bellman optimality equation. It learns Q-star.\nExpected sarsa uses the same Bellman equation as Sarsa, but samples it differently. It takes an expectation over the next action values.\nWhat’s the story with on-policy and off-policy learning?\nSarsa is a on-policy algorithm that learns the action values for the policy it’s currently following. Q-learning is an off-policy algorithm that learns the optimal action values. And Expected Sarsa is both an on-policy and an off-policy algorithm that can learn the action values for any policy.\n\n\nAssignment\nQ-Learning and Expected Sarsa\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-4---planning-learning-acting",
    "href": "posts/2021-05-25-reinforcement-learning-specialization-coursera-course2.html#course-2---week-4---planning-learning-acting",
    "title": "Reinforcement Learning Specialization - Coursera - course 2 - Sample-based Learning Methods",
    "section": "Course 2 - Week 4 - Planning, Learning & Acting",
    "text": "Course 2 - Week 4 - Planning, Learning & Acting\n\nModule 4 Learning Objectives\nLesson 1: What is a model?\n\nDescribe what a model is and how they can be used\nClassify models as distribution models or sample models\nIdentify when to use a distribution model or sample model\nDescribe the advantages and disadvantages of sample models and distribution models\nExplain why sample models can be represented more compactly than distribution models\n\nLesson 2: Planning\n\nExplain how planning is used to improve policies\nDescribe random-sample one-step tabular Q-planning\n\nLesson 3: Dyna as a formalism for planning\n\nRecognize that direct RL updates use experience from the environment to improve a policy or value function\nRecognize that planning updates use experience from a model to improve a policy or value function\nDescribe how both direct RL and planning updates can be combined through the Dyna architecture\nDescribe the Tabular Dyna-Q algorithm\nIdentify the direct-RL and planning updates in Tabular Dyna-Q\nIdentify the model learning and search control components of Tabular Dyna-Q\nDescribe how learning from both direct and simulated experience impacts performance\nDescribe how simulated experience can be useful when the model is accurate\n\nLesson 4: Dealing with inaccurate models\n\nIdentify ways in which models can be inaccurate\nExplain the effects of planning with an inaccurate model\nDescribe how Dyna can plan successfully with a partially inaccurate model\nExplain how model inaccuracies produce another exploration-exploitation trade-off\nDescribe how Dyna-Q+ proposes a way to address this trade-off\n\nLesson 5: Course wrap-up\n\n\nLesson 1: What is a model?\nReading Chapter 8.1-8.3 (pp. 159-166) in the Reinforcement Learning textbook\n\nModel-based methods rely on planning as their primary component, while model-free methods primarily rely on learning.\n\nVideo What is a Model? by Martha\nBy the end of the video, you will be able to describe a model and how it can be used, classify models as distribution models or sample models, and identify when to use a distribution model or sample model.\nVideo Comparing Sample and Distribution Models by Martha\nBy the end of this video, you will be able to describe the advantages and disadvantages of sample models and distribution models, and you will also be able to explain why sample models can be represented more compactly than distribution models.\n\n\nLesson 2: Planning\nVideo Random Tabular Q-planning by Martha\nBy the end of this video, you’ll be able to explain how planning is used to improve policies and describe random-sample one-step tabular Q-planning.\n\n\nLesson 3: Dyna as a formalism for planning\nVideo The Dyna Architecture by Adam\nBy the end of this video, you will be able to understand how simulate experience from the model differs from interacting with the environment. You will also understand how the Dyna architecture mixes direct RL updates and planning updates.\n\nVideo The Dyna Algorithm by Adam\nBy the end of this video, you should be able to describe how Tabular Dyna-Q works. You will also be able to identify the direct-RL, planning updates in Tabular Dyna-Q, and identify the model learning and search control components of Tabular Dyna-Q.\n\nVideo Dyna & Q-learning in a Simple Maze by Adam\nBy the end of this video you will be able to describe how learning from both environment-real and model experience impacts performance. You will also be able to explain how an accurate model allows the agent to learn from fewer environment interactions.\n\n\nLesson 4: Dealing with inaccurate models\nVideo What if the model is inaccurate? by Martha\nBy the end of this video you will be able to identify ways in which models can be inaccurate, explain the effects of planning with an inaccurate model, and describe how Dyna can plan successfully with an incomplete model.\nVideo In-depth with changing environments by Adam\nBy the end of this video, you’ll be able to explain how model inaccuracies produce another exploration-exploitation trade-off, and describe how Dyna-Q+ addresses this trade-off.\n\nVideo Drew Bagnell: self-driving, robotics, and Model Based RL\nVideo week 4 summary by Martha\n\n\nAssignment\nDyna-Q and Dyna-Q+\nnotebooks in github\nChapter summary Chapter 8.12 (pp. 188)\n\n\nPlanning, acting, and model-learning interact in a circular fashion (as in the figure above), each producing what the other needs to improve; no other interaction among them is either required or prohibited.\n\n\nText Book Part 1 Summary\n\nFor a summary of what we’ve covered in the specialization so far, read: pp. 189-191 in Reinforcement Learning: an introduction .\n\nAll of the methods we have explored so far in this book have three key ideas in common: first, they all seek to estimate value functions; second, they all operate by backing up values along actual or possible state trajectories; and third, they all follow the general strategy of generalized policy iteration (GPI), meaning that they maintain an approximate value function and an approximate policy, and they continually try to improve each on the basis of the other. These three ideas are central to the subjects covered in this book. We suggest that value functions, backing up value updates, and GPI are powerful organizing principles potentially relevant to any model of intelligence, whether artificial or natural.\n\n\nCourse wrap-up"
  },
  {
    "objectID": "posts/2021-06-01-logbook-June.html",
    "href": "posts/2021-06-01-logbook-June.html",
    "title": "Logbook for June 21",
    "section": "",
    "text": "Tuesday 6/1\nMachine learning in python with scikit-learn end of module 3. Hyperparameter tuning\nReinforcement Learning Specialization - C2W3 - Temporal Difference for Control - start\nWednesday 6/2\nSHAP: An introduction to explainable AI with Shapley values, Be careful when interpreting predictive models in search of causal insights\nMachine learning in python with scikit-learn module 4. Linear models\nThursday 6/3\nTalk (30’) from Michael Bronstein on Geometric Deep Learning - permutations invariant is a domain research I could use\nMachine learning in python with scikit-learn end of module 4. Linear models\nFriday 6/4\nReinforcement Learning Specialization - C2W4 - Planning, Learning and Acting\nEnd of course 2 of Reinforcement Learning Specialization"
  },
  {
    "objectID": "posts/2021-06-01-logbook-June.html#week-23---june-21",
    "href": "posts/2021-06-01-logbook-June.html#week-23---june-21",
    "title": "Logbook for June 21",
    "section": "Week 23 - June 21",
    "text": "Week 23 - June 21\nMonday 6/7\nReinforcement Learning Specialization - C3W1 - On-policy Prediction with Approximation\nThursday 6/10\nReinforcement Learning Specialization - C3W2 - Constructing Features for Prediction\nFriday 6/11\nMachine learning in python with scikit-learn module 5. Decision tree models\nReinforcement Learning Specialization - C3W3 - Control with Approximation"
  },
  {
    "objectID": "posts/2021-06-01-logbook-June.html#week-24---june-21",
    "href": "posts/2021-06-01-logbook-June.html#week-24---june-21",
    "title": "Logbook for June 21",
    "section": "Week 24 - June 21",
    "text": "Week 24 - June 21\nMonday 6/14\nReinforcement Learning Specialization - C3W4 - Policy Gradient\nReinforcement Learning Specialization - C4W1 - start of course 4. A Complete Reinforcement Learning System (Capstone) - week 1 to week 4\nWednesday 6/16\nReinforcement Learning Specialization - C4W4 - Milestone 3: Identify Key Performance Parameters, C4W5 - Milestone 4: Implement your agent\nThursday 6/17\nReinforcement Learning Specialization - end of C4W5 - Milestone 4: Implement your agent\nFriday 6/18\nReinforcement Learning Specialization - C4W6 - Milestone 5: Submit your Parameter Study!\nEnd of Specialization"
  },
  {
    "objectID": "posts/2021-06-01-logbook-June.html#week-25---june-21",
    "href": "posts/2021-06-01-logbook-June.html#week-25---june-21",
    "title": "Logbook for June 21",
    "section": "Week 25 - June 21",
    "text": "Week 25 - June 21\nMonday 6/21\nMachine learning in python with scikit-learn module 6. Ensemble of models\nRL Course by David Silver Integrating learning and planning (lecture 8)\nThursday 6/24\nMachine learning in python with scikit-learn module 7. Evaluating model performance -End of this course"
  },
  {
    "objectID": "posts/2021-06-01-logbook-June.html#week-26---june-21",
    "href": "posts/2021-06-01-logbook-June.html#week-26---june-21",
    "title": "Logbook for June 21",
    "section": "Week 26 - June 21",
    "text": "Week 26 - June 21\nMonday 6/28\nAI Tech watchfulness in Manufacturing using Arxiv Sanity Presever\nPaper reviewed on arxiv about local post-hoc explanations for predictive process monitoring in manufacturing. arXiv:2009.10513v2. SHAP, ICE and why this approach makes sense in manufacturing domains.\nWednesday 6/30\nSurvey Paper reviewed on Journal of Manufacturing Systems about Deep learning for smart manufacturing: Methods and applications. j.jmsy.2018.01.003. Review use of deep learning algorithms: CNN, RBM, auto encoders, RNN and applications for smart manufacturing: quality inspection, fault assessment, defect prognosis (RUL). Unfortunately prescriptive usage are missing. Points to multiple references origins of these algorithms and applications. Great survey paper!"
  },
  {
    "objectID": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html",
    "href": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html",
    "title": "Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation",
    "section": "",
    "text": "Coursera website: course 3 - Prediction and Control with Function Approximation of Reinforcement Learning Specialization\nmy notes on course 1 - Fundamentals of Reinforcement Learning, course 2 - Sample-based Learning Methods, course 4 - A Complete Reinforcement Learning System (Capstone)\nspecialization roadmap - course 3 - Prediction and Control with Function Approximation (syllabus)\ncourse 3 - In Course 3, we leave the relative comfort of small finite MDPs and investigate RL with function approximation. Here we will see that the main concepts from Courses 1 and 2 transferred to problems with larger infinite state spaces. We will cover feature construction, neural network learning, policy gradient methods, and other particularities of the function approximation setting.\nWeek 1 - On-policy Prediction with Approximation\nWeek 2 - Constructing Features for Prediction\nWeek 3 - Control with Approximation\nWeek 4 - Policy Gradient"
  },
  {
    "objectID": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-1---on-policy-prediction-with-approximation",
    "href": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-1---on-policy-prediction-with-approximation",
    "title": "Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation",
    "section": "Course 3 - Week 1 - On-policy Prediction with Approximation",
    "text": "Course 3 - Week 1 - On-policy Prediction with Approximation\n\nModule 1 Learning Objectives\nLesson 1: Estimating Value Functions as Supervised Learning\n\nUnderstand how we can use parameterized functions to approximate value functions\nExplain the meaning of linear value function approximation\nRecognize that the tabular case is a special case of linear value function approximation.\nUnderstand that there are many ways to parameterize an approximate value function\nUnderstand what is meant by generalization and discrimination\nUnderstand how generalization can be beneficial\nExplain why we want both generalization and discrimination from our function approximation\nUnderstand how value estimation can be framed as a supervised learning problem\nRecognize not all function approximation methods are well suited for reinforcement learning\n\nLesson 2: The Objective for On-policy Prediction\n\nUnderstand the mean-squared value error objective for policy evaluation\nExplain the role of the state distribution in the objective\nUnderstand the idea behind gradient descent and stochastic gradient descent\nOutline the gradient Monte Carlo algorithm for value estimation\nUnderstand how state aggregation can be used to approximate the value function\nApply Gradient Monte-Carlo with state aggregation\n\nLesson 3: The Objective for TD\n\nUnderstand the TD-update for function approximation\nHighlight the advantages of TD compared to Monte-Carlo\nOutline the Semi-gradient TD(0) algorithm for value estimation\nUnderstand that TD converges to a biased value estimate\nUnderstand that TD converges much faster than Gradient Monte Carlo\n\nLesson 4: Linear TD\n\nDerive the TD-update with linear function approximation\nUnderstand that tabular TD(0) is a special case of linear semi-gradient TD(0)\nHighlight the advantages of linear value function approximation over nonlinear\nUnderstand the fixed point of linear TD learning\nDescribe a theoretical guarantee on the mean squared value error at the TD fixed point\n\n\n\nLesson 1: Estimating Value Functions as Supervised Learning\nReading Chapter 9.1-9.4 (pp. 197-209) in the Reinforcement Learning textbook\n\nIn many of the tasks to which we would like to apply reinforcement learning the state space is combinatorial and enormous; the number of possible camera images, for example, is much larger than the number of atoms in the universe.\nIn many of our target tasks, almost every state encountered will never have been seen before. To make sensible decisions in such states it is necessary to generalize from previous encounters with different states that are in some sense similar to the current one. In other words, the key issue is that of generalization. How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset?\nFortunately, generalization from examples has already been extensively studied, and we do not need to invent totally new methods for use in reinforcement learning. To some extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called function approximation because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function. Function approximation is an instance of supervised learning, the primary topic studied in machine learning, artificial neural networks, pattern recognition, and statistical curve fitting.\n\nVideo Moving to Parameterized Functions by Adam\nBy the end of this video, you’ll be able to understand how we can use parameterized functions to approximate values, explain linear value function approximation, recognize that the tabular case is a special case of linear value function approximation, and understand that there are many ways to parameterize an approximate value function.\nVideo Generalization and Discrimination by Martha\nBy the end of this video, you will be able to understand what is meant by generalization and discrimination, understand how generalization can be beneficial, and explain why we want both generalization and discrimination from our function approximation.\nVideo Framing Value Estimation as Supervised Learning by Martha\nBy the end of this video, you will be able to understand how value estimation can be framed as a supervised learning problem, and recognize that not all function approximation methods are well suited for reinforcement learning.\n\n\nLesson 2: The Objective for On-policy Prediction\nVideo The Value Error Objective by Adam\nBy the end of this video you will be able to understand the Mean Squared Value Error objective for policy evaluation and explain the role of the state distribution in the objective.\n\\[\n\\overline{VE}=\\displaystyle\\sum_{s}\\mu(s)[v_\\pi(s)-\\hat{v}(s,w)]^2\n\\] This is the Mean Squared Value Error Objective where \\(\\mu\\) reflects how much we care about each state (a probability distribution)\nVideo Introducing Gradient Descent by Martha\nBy the end of this video, you will be able to understand the idea of gradient descent, and understand that gradient descent converges to stationary points.\nVideo Gradient Monte for Policy Evaluation by Martha\nBy the end of this video, you will be able to understand how to use gradient descent and stochastic gradient descent to minimize value error and outline the Gradient Monte Carlo algorithm for value estimation.\n\nVideo State Aggregation with Monte Carlo by Adam\nBy the end of this video, you will be able to understand how state aggregation can be used to approximate the value function and apply gradient Monte Carlo with state aggregation.\n\n\nLesson 3: The Objective for TD\nVideo Semi-Gradient TD for Policy Evaluation by Adam\nBy the end of this video you will be able to understand the TD update for function approximation, and outline the semi-gradient TD(0) algorithm for value estimation.\n\nVideo Comparing TD and Monte Carlo with State Aggregation by Adam\nBy the end of this video, you’ll be able to understand that TD converges to a bias value estimate and understand that TD can learn faster than Gradient Monte Carlo.\nVideo Doina Precup: Building Knowledge for AI Agents with Reinforcement Learning\n\n\nLesson 4: Linear TD\nVideo The Linear TD Update by Martha\nBy the end of this video, you’ll be able to derive the TD update with linear function approximation, understand that tabular TD(0) as a special case of linear semi gradient TD(0), and understand why we care about linear TD as a special case.\nVideo The True Objective for TD by Martha\nBy the end of this video, you will be able to understand the fixed point of linear TD and describe a theoretical guarantee on the mean squared value error at the TD fixed point.\nVideo Week 1 Summary by Adam\n\n\nAssignment\nTD with State Aggregation\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-2---constructing-features-for-prediction",
    "href": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-2---constructing-features-for-prediction",
    "title": "Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation",
    "section": "Course 3 - Week 2 - Constructing Features for Prediction",
    "text": "Course 3 - Week 2 - Constructing Features for Prediction\n\nModule 2 Learning Objectives\nLesson 1: Feature Construction for Linear Methods\n\nDescribe the difference between coarse coding and tabular representations\nExplain the trade-off when designing representations between discrimination and generalization\nUnderstand how different coarse coding schemes affect the functions that can be represented\nExplain how tile coding is a (computationally?) convenient case of coarse coding\nDescribe how designing the tilings affects the resultant representation\nUnderstand that tile coding is a computationally efficient implementation of coarse coding\n\nLesson 2: Neural Networks\n\nDefine a neural network\nDefine activation functions\nDefine a feedforward architecture\nUnderstand how neural networks are doing feature construction\nUnderstand how neural networks are a non-linear function of state\nUnderstand how deep networks are a composition of layers\nUnderstand the tradeoff between learning capacity and challenges presented by deeper networks\n\nLesson 3: Training Neural Networks\n\nCompute the gradient of a single hidden layer neural network\nUnderstand how to compute the gradient for arbitrarily deep networks\nUnderstand the importance of initialization for neural networks\nDescribe strategies for initializing neural networks\nDescribe optimization techniques for training neural networks\n\n\n\nLesson 1: Feature Construction for Linear Methods\nReading Chapter 9.4-9.5.0 (pp. 204-210), 9.5.3-9.5.4 (pp. 215-222) and 9.7 (pp. 223-228) in the Reinforcement Learning textbook\nVideo Coarse Coding by Adam\nBy the end of this video, you’ll be able to describe coarse coding and describe how it relates to state aggregation.\n\nVideo Generalization Properties of Coarse Coding by Martha\nBy the end of this video, you’ll be able to describe how coarse coding parameters affect generalization and discrimination, and understand how that affects learning speed and accuracy.\nVideo Tile Coding by Adam\nBy the end of this video, you’ll be able to explain how tile coding achieves both generalization and discrimination, and understand the benefits and limitations of tile coding.\nVideo Using Tile Coding in TD by Adam\nBy the end of this video, you’ll be able to explain how to use tile coding with TD learning and identify important properties of tile code representations.\n\n\nLesson 2: Neural Networks\nVideo What is a Neural Network? by Martha\nBy the end of this video, you’ll be able to define a neural network, define an activation function and understand how a neural network is a parameterized function.\nVideo Non-linear Approximation with Neural Networks by Martha\nBy the end of this video, you will understand how neural networks do feature construction, and understand how neural networks are a non-linear function of state.\nVideo Deep Neural Networks by Adam\nBy the end of this video, you will understand how deep neural networks are composed of many layers and understand that depth can facilitate learning features through composition and abstraction.\n\n\nLesson 3: Training Neural Networks\nVideo Gradient Descent for Training Neural Networks by Martha\nBy the end of this video, you’ll be able to derive the gradient of a neural network and implement gradient descent on a neural network.\nVideo Optimization Strategies for NNs by Martha\nBy the end of this video, you will be able to understand the importance of initialization for neural networks and describe optimization techniques for training neural networks.\nVideo David Silver on Deep Learning + RL = AI?\nVideo Week 2 Review by Adam\n\n\nAssignment\nSemi-gradient TD with a Neural Network\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-3---control-with-approximation",
    "href": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-3---control-with-approximation",
    "title": "Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation",
    "section": "Course 3 - Week 3 - Control with Approximation",
    "text": "Course 3 - Week 3 - Control with Approximation\n\nModule 3 Learning Objectives\nLesson 1: Episodic Sarsa with Function Approximation\n\nExplain the update for Episodic Sarsa with function approximation\nIntroduce the feature choices, including passing actions to features or stacking state features\nVisualize value function and learning curves\nDiscuss how this extends to Q-learning easily, since it is a subset of Expected Sarsa\n\nLesson 2: Exploration under Function Approximation\n\nUnderstanding optimistically initializing your value function as a form of exploration\n\nLesson 3: Average Reward\n\nDescribe the average reward setting\nExplain when average reward optimal policies are different from discounted solutions\nUnderstand how differential value functions are different from discounted value functions\n\n\n\nLesson 1: Episodic Sarsa with Function Approximation\nReading Chapter 10 (pp. 243-246) and 10.3 (pp. 249-252) in the Reinforcement Learning textbook\nVideo Episodic Sarsa with Function Approximation by Adam\nBy the end of this video, you’ll be able to understand how to construct action-dependent features for approximate action values and explain how to use Sarsa in episodic tasks with function approximation.\n\nVideo Episodic Sarsa in Mountain Car by Adam\nBy the end of this video, you will gain experience analyzing the performance of an approximate TD control method.\nVideo Expected Sarsa with Function Approximation by Adam\nBy the end of this video, you’ll be able to explain the update for expected Sarsa with function approximation, and explain the update for Q-learning with function approximation.\n\n\n\nLesson 2: Exploration under Function Approximation\nVideo Exploration under Function Approximation by Martha\nBy the end of this video, you’ll be able to describe how optimistic initial values and \\(\\epsilon\\)-greedy can be used with function approximation.\n\n\nLesson 3: Average Reward\nVideo Average Reward: A New Way of Formulating Control Problems by Martha\nBy the end of this video, you’ll be able to describe the average reward setting, explain when average reward optimal policies are different from policies obtained under discounting and understand differential value functions.\nSatinder Singh on Intrinsic Rewards\nVideo Week 3 Review by Martha\n\n\nAssignment\nFunction Approximation and Control\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-4---policy-gradient",
    "href": "posts/2021-06-07-reinforcement-learning-specialization-coursera-course3.html#course-3---week-4---policy-gradient",
    "title": "Reinforcement Learning Specialization - Coursera - course 3 - Prediction and Control with Function Approximation",
    "section": "Course 3 - Week 4 - Policy Gradient",
    "text": "Course 3 - Week 4 - Policy Gradient\n\nModule 4 Learning Objectives\nLesson 1: Learning Parameterized Policies\n\nUnderstand how to define policies as parameterized functions\nDefine one class of parameterized policies based on the softmax function\nUnderstand the advantages of using parameterized policies over action-value based methods\n\nLesson 2: Policy Gradient for Continuing Tasks\n\nDescribe the objective for policy gradient algorithms\nDescribe the results of the policy gradient theorem\nUnderstand the importance of the policy gradient theorem\n\nLesson 3: Actor-Critic for Continuing Tasks\n\nDerive a sample-based estimate for the gradient of the average reward objective\nDescribe the actor-critic algorithm for control with function approximation, for continuing tasks\n\nLesson 4: Policy Parameterizations\n\nDerive the actor-critic update for a softmax policy with linear action preferences\nImplement this algorithm\nDesign concrete function approximators for an average reward actor-critic algorithm\nAnalyze the performance of an average reward agent\nDerive the actor-critic update for a gaussian policy\nApply average reward actor-critic with a gaussian policy to a particular task with continuous actions\n\n\n\nLesson 1: Learning Parameterized Policies\nReading Chapter 13 (pp. 321-336) in the Reinforcement Learning textbook\nVideo Learning Policies Directly by Adam\nBy the end of this video, you’ll be able to understand how to define policies as parameterized functions and define one class of parametrized policies based on the softmax function.\nVideo Advantages of Policy Parameterization by Adam\nBy the end of this video, you’ll be able to understand some of the advantages of using parameterized policies.\n\n\nLesson 2: Policy Gradient for Continuing Tasks\nVideo The Objective for Learning Policies by Martha\nBy the end of this video, you’ll be able to describe the objective for policy gradient algorithms.\n\nVideo The Policy Gradient Theorem by Martha\nBy the end of this video, you will be able to describe the result of the policy gradient theorem and understand the importance of the policy gradient theorem.\n\n\n\nLesson 3: Actor-Critic for Continuing Tasks\nVideo Estimating the Policy Gradient by Martha\nBy the end of this video, you will be able to derive a sample-based estimate for the gradient of the average reward objective.\n\nVideo Actor-Critic Algorithm by Adam\nBy the end of this video, you’ll be able to describe the actor-critic algorithm for control with function approximation for continuing tasks.\n\n\n\nLesson 4: Policy Parameterizations\nVideo Actor-Critic with Softmax Policies by Adam\nBy the end of this video you’ll be able to derive the actor critic update for a Softmax policy with linear action preferences and implement this algorithm.\nVideo Demonstration with Actor-Critic by Adam\nBy the end of this video, you’ll be able to design a function approximator for an average reward actor-critic algorithm and analyze the performance of an average reward agent.\nVideo Gaussian Policies for Continuous Actions by Martha\nBy the end of this video, you’ll be able to derive the actor-critic update for a Gaussian policy and apply average reward actor-critic with a Gaussian policy to task with continuous actions.\nVideo Week 4 Summary by Martha\n\n\n\nAssignment\nAverage Reward Softmax Actor-Critic using Tile-coding\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html",
    "href": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html",
    "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
    "section": "",
    "text": "Coursera website: course 4 - A Complete Reinforcement Learning System (Capstone) of Reinforcement Learning Specialization\nmy notes on course 1 - Fundamentals of Reinforcement Learning, course 2 - Sample-based Learning Methods, course 3 - Prediction and Control with Function Approximation\nspecialization roadmap - course 4 - A Complete Reinforcement Learning System (Capstone) (syllabus)\nWeek 1 - Welcome to the Course Week 2 - Formalize Word Problem as MDP Week 3 - Choosing The Right Algorithm Week 4 - Identify Key Performance Parameters Week 5 - Implement Your Agent Week 6 - Submit Your Parameter Study!"
  },
  {
    "objectID": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-2---formalize-word-problem-as-mdp",
    "href": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-2---formalize-word-problem-as-mdp",
    "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
    "section": "Course 4 - Week 2 - Formalize Word Problem as MDP",
    "text": "Course 4 - Week 2 - Formalize Word Problem as MDP\n\nFinal Project: Milestone 1\nVideo Initial Project Meeting with Martha: Formalizing the Problem\n\nVideo Andy Barto on What are Eligibility Traces and Why are they so named?\nBy the end of this video, you’ll understand the origin of the idea of eligibility traces and you’ll actually see that you’ve been using a variant of eligibility traces all along.\n\n\n\nProject Resources\nVideo Let’s Review: Markov Decision Processes\nBy the end of this video, you’ll be able to understand Markov decision processes or MDPs and describe how the dynamics of MDP are defined.\nVideo Let’s Review: Examples of Episodic and Continuing Tasks\nBy the end of this video, you will be able to understand when to formalize a task as episodic or continuing.\n\n\nAssignment\nMoonShot Technologies\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-3---choosing-the-right-algorithm",
    "href": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-3---choosing-the-right-algorithm",
    "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
    "section": "Course 4 - Week 3 - Choosing The Right Algorithm",
    "text": "Course 4 - Week 3 - Choosing The Right Algorithm\n\nWeekly Learning Goals\nVideo Meeting with Niko: Choosing the Learning Algorithm\n\n\n\nProject Resources\nVideo Let’s Review: Expected Sarsa\n\nVideo Let’s Review: What is Q-learning?\nVideo Let’s Review: Average Reward- A New Way of Formulating Control Problems\nVideo Let’s Review: Actor-Critic Algorithm\nVideo Csaba Szepesvari on Problem Landscape\nVideo Andy and Rich: Advice for Students"
  },
  {
    "objectID": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-4---identify-key-performance-parameters",
    "href": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-4---identify-key-performance-parameters",
    "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
    "section": "Course 4 - Week 4 - Identify Key Performance Parameters",
    "text": "Course 4 - Week 4 - Identify Key Performance Parameters\n\nWeekly Learning Goals\nVideo Agent Architecture Meeting with Martha: Overview of Design Choices\nNow, let’s discuss the meta parameter choices that you will have to make to fully implement the agent. This means we need to decide on the function approximator, choices in the optimizer for updating the action values, and how to do exploration.\n\n\nProject Resources\nVideo Let’s Review: Non-linear Approximation with Neural Networks\nBy the end of this video, you will understand how neural networks do feature construction, and you will understand how neural networks are a non-linear function of state.\nVideo Drew Bagnell on System ID + Optimal Control\nVideo Susan Murphy on RL in Mobile Health"
  },
  {
    "objectID": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-5---implement-your-agent",
    "href": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-5---implement-your-agent",
    "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
    "section": "Course 4 - Week 5 - Implement your agent",
    "text": "Course 4 - Week 5 - Implement your agent\n\nWeekly Learning Goals\nVideo Meeting with Adam: Getting the Agent Details Right\n\n\nProject Resources\nVideo Let’s Review: Optimization Strategies for NNs\nBy the end of this video, you will be able to understand the importance of initialization for neural networks and describe optimization techniques for training neural networks.\nOne simple yet effective initialization strategy, is to randomly sample the initial weights from a normal distribution with small variance. This way, each neuron has a different output from other neurons within its layer. This provides a more diverse set of potential features. By keeping the variants small, we ensure that the output of each neuron is within the same range as its neighbors. One downside to this strategy is that, as we add more inputs to a neuron, the variance of the output grows. We can get around this issue by scaling the variance of the weights, by one over the square root of the number of inputs.\n\nHere’s the stochastic gradient descent update rule and here’s the update modified to include momentum. Notice, it is similar to the regular stochastic gradient descent update plus an extra term called the momentum M. The momentum term summarizes the history of the gradients using a decaying sum of gradients with decay rate Lambda. If recent gradients have all been in similar directions, then we gained momentum in that direction. This means, we make a large step in that direction. If recent updates have conflicting directions, then it kills the momentum. The momentum term will have little impact on the update and we will make a regular gradient descent step. Momentum provably accelerates learning, meaning it gets to a stationary point more quickly.\n\nSo far, we have only talked about a global scalar step size. This is well-known to be problematic because this can result in updates that are too big for some weights and too small for other weights. Adapting the step sizes for each weight, based on statistics about the learning process in practice results in much better performance. Now, how does the update change? The change is very simple. Instead of updating with a scalar Alpha, there’s a vector of step sizes indexed by t to indicate that it can change on each time-step. Each dimension of the gradient, is scaled by its corresponding step size instead of the global step size. There are a variety of methods to adapt a vector of step sizes. You’ll get to implement one in your assignment.\n\nVideo Let’s Review: Expected Sarsa with Function Approximation\nBy the end of this video, you’ll be able to explain the update for expected Sarsa with function approximation, and explain the update for Q-learning with function approximation.\n\n\nVideo Let’s Review: Dyna & Q-learning in a Simple Maze\nBy the end of this video you will be able to describe how learning from both real and model experience impacts performance. You will also be able to explain how a model allows the agent to learn from fewer interactions with the environment.\nVideo Meeting with Martha: In-depth on Experience Replay\nIn Course 3, the agents you implemented update the value function or policy only once with each sample. But this is likely not the most sample efficient way to use our data. You have actually seen a smarter approach in Course 2 where we talked about Dyna as a way to be more sample efficient. But we only talked about Dyna for the tabular setting.\nIn this video, we will talk about how to make your agent more sample efficient when using function approximation. We will discuss a simple method called experience replay and how it relates to Dyna. To get some intuition for experience replay, let’s first remember a method that we know well, Dyna-Q. The idea is to learn a model using sample experience. Then simulated experience can be obtained from this model to update the values. This procedure of using simulated experience to improve the value estimates is called planning.\nExperience replay is a simple method for trying to get the advantages of Dyna. The basic idea is to save a buffer of experience and let the data be the model. We sample experience from this buffer and update the value function with those samples similarly to how we sample from the model and update the values in Dyna.\n\nVideo Martin Riedmiller on The ‘Collect and Infer’ framework for data-efficient RL\nMartin Riedmiller, head of the control team at Deepmind has been working for more than 20 years on New Reinforcement Learning Agents for the control of dynamical systems.\nThe control of dynamical systems is an attractive application area for reinforcement learning controllers. They all share the same principle feedback control structure, a controller gets the observation, computes an action and applies it to the environment. Classical control theory would first model the process as a set of differential equations for which then a control law must be analytically derived. A tedious job in particular if the systems are complex or highly nonlinear. Reinforcement learning in contrast promises to be able to learn the controller autonomously. If only the overall control goal is specified. This is typically done by defining the immediate reward. The RL controller optimizes the expected cumulated sum of rewards over time.\n\nThese two steps together build the so-called collecting and infer framework of reinforcement learning. This perspective keeps us focused on the two main question of data efficient RL. Infer, which means squeezing out the most of a given set of transition data. And collect, which means sampling the most formative data from the environment.\n\n\n\n\nAssignment\nImplement your agent\nnotebooks in github"
  },
  {
    "objectID": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-6---submit-your-parameter-study",
    "href": "posts/2021-06-14-reinforcement-learning-specialization-coursera-course4.html#course-4---week-6---submit-your-parameter-study",
    "title": "Reinforcement Learning Specialization - Coursera - course 4 - A Complete Reinforcement Learning System (Capstone)",
    "section": "Course 4 - Week 6 - Submit your Parameter Study!",
    "text": "Course 4 - Week 6 - Submit your Parameter Study!\n\nWeekly Learning Goals\nVideo Meeting with Adam: Parameter Studies in RL\n\n\nProject Resources\nVideo Let’s Review: Comparing TD and Monte Carlo\nVideo Joelle Pineau about RL that Matters\n\n\nAssignment\nCompleting the parameter study\nnotebooks in github\n\n\nCongratulations!\nVideo Meeting with Martha: Discussing Your Results\nVideo Course Wrap-up\nVideo Specialization Wrap-up"
  },
  {
    "objectID": "posts/2021-06-25-slide-show-jupyter.html",
    "href": "posts/2021-06-25-slide-show-jupyter.html",
    "title": "Slideshows from Jupyter notebook",
    "section": "",
    "text": "Run slides\nWe can launch slideshow with RISE button (or Alt-R)\nOr we can serve pages through nbconvert:\njupyter nbconvert my_notebook.ipynb --to slides --post serve\n\n!jupyter nbconvert 2021-06-25-slide-show-jupyter.ipynb --to slides --post serve\n\n[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n[NbConvertApp] Converting notebook 2021-06-25-slide-show-jupyter.ipynb to slides\nServing your slides at http://127.0.0.1:8000/2021-06-25-slide-show-jupyter.slides.html\nUse Control-C to stop this server\nWARNING:tornado.access:404 GET /favicon.ico (127.0.0.1) 0.41ms\n^C\n\nInterrupted\n\n\n\n\nRISE vs nbconvert\n\n\n\n\n\n\n\n\n\nRISE\nnbconvert\n\n\n\n\nrun\nAlt-R or click Rise icon\ncmd line: jupyter nbconvert <name>.ipynb --to slides --post serve\n\n\nsplit cells\nsupported\nnot supported\n\n\nkeyboard shortcuts\n← → ⇞ ⇟\n← ↑ → ↓\n\n\n\n\n\nsome extensions\ntips from Mark Roekpe’s blog. Valid mainly for RISE.\n\nsplicell\nhide code from slideshow\ncustom css\nopen slides automatically"
  },
  {
    "objectID": "posts/2021-06-30-documentation-python.html",
    "href": "posts/2021-06-30-documentation-python.html",
    "title": "Autogenerate documentation from custom python classes",
    "section": "",
    "text": "!conda env list\n\n# conda environments:\n#\nbase                     /home/explore/miniconda3\naniti                    /home/explore/miniconda3/envs/aniti\naudioclass               /home/explore/miniconda3/envs/audioclass\ncoursera_rl              /home/explore/miniconda3/envs/coursera_rl\nd059                  *  /home/explore/miniconda3/envs/d059\ndatacamp                 /home/explore/miniconda3/envs/datacamp\ndeeplearning_specialization     /home/explore/miniconda3/envs/deeplearning_specialization\ndeeplearning_specialization_keras     /home/explore/miniconda3/envs/deeplearning_specialization_keras\ndeeplearning_specialization_tf1     /home/explore/miniconda3/envs/deeplearning_specialization_tf1\ndrl_handson              /home/explore/miniconda3/envs/drl_handson\ndrl_simonini             /home/explore/miniconda3/envs/drl_simonini\nfastai                   /home/explore/miniconda3/envs/fastai\nfastaudio                /home/explore/miniconda3/envs/fastaudio\ngan                      /home/explore/miniconda3/envs/gan\ngan_tensorflow           /home/explore/miniconda3/envs/gan_tensorflow\nmacos                    /home/explore/miniconda3/envs/macos\nminecraft                /home/explore/miniconda3/envs/minecraft\nmit_6002x                /home/explore/miniconda3/envs/mit_6002x\nmit_6S191                /home/explore/miniconda3/envs/mit_6S191\npycaret                  /home/explore/miniconda3/envs/pycaret\npytorch                  /home/explore/miniconda3/envs/pytorch\nscikit-learn-course      /home/explore/miniconda3/envs/scikit-learn-course\nshap                     /home/explore/miniconda3/envs/shap\nsqueezebox               /home/explore/miniconda3/envs/squeezebox\nstablebaselines3         /home/explore/miniconda3/envs/stablebaselines3\nzoe                      /home/explore/miniconda3/envs/zoe\n\n\n\n\nimport sys\n!conda install --yes --prefix {sys.prefix} pdoc3\n\nCollecting package metadata (current_repodata.json): done\nSolving environment: / \nThe environment is inconsistent, please check the package plan carefully\nThe following packages are causing the inconsistency:\n\n  - defaults/linux-64::matplotlib==3.3.2=0\n  - defaults/linux-64::matplotlib-base==3.3.2=py38h817c723_0\n  - defaults/linux-64::spacy==2.3.2=py38hfd86e86_0\n  - pytorch/linux-64::torchvision==0.8.1=py38_cu110\n  - defaults/linux-64::numpy==1.19.2=py38h54aff64_0\n  - defaults/linux-64::thinc==7.4.1=py38hfd86e86_0\n  - defaults/linux-64::cython-blis==0.4.1=py38h7b6447c_1\n  - fastai/noarch::fastprogress==1.0.0=pyh39e3cac_0\n  - fastai/noarch::fastai==2.1.4=py_0\n  - defaults/linux-64::mkl_random==1.1.1=py38h0573a6f_0\n  - pytorch/linux-64::pytorch==1.7.0=py3.8_cuda11.0.221_cudnn8.0.3_0\n  - defaults/linux-64::pandas==1.1.3=py38he6710b0_0\n  - fastai/noarch::fastscript==1.0.0=0\n  - defaults/linux-64::scipy==1.5.2=py38h0b6359f_0\n  - defaults/linux-64::mkl_fft==1.2.0=py38h23d657b_0\n  - fastai/noarch::fastbook==0.0.14=py_0\n  - defaults/linux-64::scikit-learn==0.23.2=py38h0573a6f_0\ndone\n\n## Package Plan ##\n\n  environment location: /home/explore/miniconda3/envs/d059\n\n  added / updated specs:\n    - pdoc3\n\n\nThe following NEW packages will be INSTALLED:\n\n  mako               pkgs/main/noarch::mako-1.1.4-pyhd3eb1b0_0\n  markdown           pkgs/main/linux-64::markdown-3.3.4-py38h06a4308_0\n  pdoc3              pkgs/main/noarch::pdoc3-0.9.2-pyhd3eb1b0_0\n\nThe following packages will be UPDATED:\n\n  ca-certificates                              2020.10.14-0 --> 2021.5.25-h06a4308_1\n  certifi            pkgs/main/noarch::certifi-2020.6.20-p~ --> pkgs/main/linux-64::certifi-2021.5.30-py38h06a4308_0\n  mkl_fft                              1.2.0-py38h23d657b_0 --> 1.3.0-py38h54f3939_0\n  openssl                                 1.1.1h-h7b6447c_0 --> 1.1.1k-h27cfd23_0\n  scipy                                1.5.2-py38h0b6359f_0 --> 1.6.2-py38h91f5cce_0\n\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done"
  },
  {
    "objectID": "posts/2021-06-30-documentation-python.html#force-refresh-of-doc",
    "href": "posts/2021-06-30-documentation-python.html#force-refresh-of-doc",
    "title": "Autogenerate documentation from custom python classes",
    "section": "force refresh of doc",
    "text": "force refresh of doc\nin case of existing html file\npdoc --html --output-dir exp/html --force exp/my_classe.py"
  },
  {
    "objectID": "posts/2021-07-01-logbook-July.html",
    "href": "posts/2021-07-01-logbook-July.html",
    "title": "Logbook for July 21",
    "section": "",
    "text": "Thursday 7/1\nPaper reviewed on arxiv about revisiting Deep Learning Models for Tabular Data. arXiv:2106.11959v1 (FT-Transformer, ResNet, MLP) architectures can compete with GBDT models. Examples are not detailed in paper but code seems available at https://github.com/yandex-research/rtdl (rtdl (Revisiting Tabular Deep Learning) based on PyTorch)\nRL Course by David Silver Exploration and Exploitation (lecture 9)"
  },
  {
    "objectID": "posts/2021-07-01-logbook-July.html#week-27---july-21",
    "href": "posts/2021-07-01-logbook-July.html#week-27---july-21",
    "title": "Logbook for July 21",
    "section": "Week 27 - July 21",
    "text": "Week 27 - July 21\nWednesday 7/7\nStart of Deep Neural Networks with PyTorch on Coursera by IBM.\nFriday 7/9\nDeep Neural Networks with PyTorch - week 2 - linear regression"
  },
  {
    "objectID": "posts/2021-07-01-logbook-July.html#week-28---july-21",
    "href": "posts/2021-07-01-logbook-July.html#week-28---july-21",
    "title": "Logbook for July 21",
    "section": "Week 28 - July 21",
    "text": "Week 28 - July 21\nMonday 7/12\nDeep Neural Networks with PyTorch - week 3 - multiple input output linear regression, logistic regression for classification\nDeep Neural Networks with PyTorch - week 4 - softmax regression\nThursday 7/15\nDeep Neural Networks with PyTorch - week 4 - shallow neural networks\nDeep Neural Networks with PyTorch - week 5 - deep neural networks\nFriday 7/16\nDeep Learning with PyTorch book\n\n\n\nimg\n\n\nPyTorch Ecosystem day 2021:\n\nlightning to remove boilerplate code - to be tested!\ntorchstudio to visualize dataset and models\npytorch-ignite from my colleague Sylvain - high-level library to help training evaluating NN\npytorchts to play with timeseries\nmultitask RL environments and baselines for RL\nrlstructures python library for RL research\nmbrl-lib to write model-based RL algorithms\npystiche framework for style transfer\n\nDeep Neural Networks with PyTorch - week 6 - convolutional neural networks - even if I don’t need it"
  },
  {
    "objectID": "posts/2021-07-01-logbook-July.html#week-30---july-21",
    "href": "posts/2021-07-01-logbook-July.html#week-30---july-21",
    "title": "Logbook for July 21",
    "section": "Week 30 - July 21",
    "text": "Week 30 - July 21\nMonday 7/26\nDeep Neural Networks with PyTorch - week 7 - fashion mnist\nReading of some papers:\n\nDeep Reinforcement Learning Approaches for Process Control by Steven Spielberg Pon Kumar - thesis, UBC 2017\nA Deep Learning Architecture for Predictive Control by Steven Spielberg Pon Kumar - ScienceDirect 2018\nDeep Reinforcement Learning for Process Control: A Primer for Beginners by Steven Spielberg et al. 2004\nReinforcement Learning for Statistical Process Control in Manufacturing - by Zsolt János Viharos 2020"
  },
  {
    "objectID": "posts/2021-07-07-deep-neural-network-coursera.html",
    "href": "posts/2021-07-07-deep-neural-network-coursera.html",
    "title": "Deep Neural Network with PyTorch - Coursera",
    "section": "",
    "text": "Course certificate\n\nWeek 1 - Tensor and Datasets\n\nLearning Objectives\n\nTensors 1D\nTwo-Dimensional Tensors\nData Set\nDifferentiation in PyTorch\n\n\n\nnotebook\nnotebook\n\n\nTensors 1D\n\nThe basics\n#initialize\nimport torch\na=torch.tensor([7,4,3,2,6])\n\n#dtype, type()\na.dtype\na.type()\n\n#convert with type\na=a.type(torch.FloatTensor)\n\n#size, ndimension\na.size()\na.ndimension()\n\n#convert to 2D\na_2D=a.view(-1, 1)\n\n#from_numpy, to numpy\nimport numpy as np\nnumpy_array = np.array([0.0, 1.0, 2.0, 3.0, 4.0])\ntorch_tensor = torch.from_numpy(numpy_array)\nback_to_numpy = torch_tensor.numpy()\n\n#from pandas\nimport pandas as pd\npandas_series = pd.Series([0.1, 2, 0.3, 10.1])\npandas_to_torch = torch.from_numpy(pandas_series.values)\n\n#to list\nthis_tensor = torch.tensor([0, 1, 2, 3])\ntorch_to_list = this_tensor.tolist()\n\n#item\nnew_tensor = torch.tensor([5, 2, 6, 1])\nnew_tensor[0].item()\n\n#indexing and slicing\nc[3:5]=torch.tensor([300.0, 4.0])\n\n\nbasic operations\n#hadamard product\nz = u*v\n\n#dot product, (produit scalaire)\nresult = torch.dot(u, v)\n\n\nuniversal functions, mean, max, mathematical functions, plot with linspace\n#mean\na.mean()\n\n#max\nb.max()\n\n#plot y=sin(x)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx = torch.linspace(0, 2 * np.pi, 100)\ny = torch.sin(x)\nplt.plot(x.numpy(), y.numpy())\n\n\nUngraded lab\n1.1_1Dtensors_v2.ipynb\n\n\n\nTensors 2D\nnotebook\n\nTensor creation in 2D\na = [ [11, 12, 13], [21, 22, 23], [31, 32, 33] ]\nA = torch.tensor(a)\n\nA.ndimension()\n>> 2\n\nA.shape\n>> torch.Size([3, 3])\n\nA.size()\n>> torch.Size([3, 3])\n\n#number of elements\nA.numel()\n>> 9\n\n\nIndexing and slicing in 2D\nA[0, 0:2]\n>> tensor([11, 12])\n\nA[1:3, 2]\n>> tensor([23, 33])\n\n\nBasic operations in 2D: hadamard product, matrix multiplication\nX = torch.tensor([[1,0], [0,1]])\nY = torch.tensor([[2,1], [1,2]])\n\n#hadamard product\nZ = X*Y\nZ\n>> tensor([[2, 0],\n           [0, 2]])\n\nA = torch.tensor([ [0, 1, 1], [1, 0, 1]])\nB = torch.tensor([ [1, 1], [1, 1], [-1, 1]])\n\n#matrix multiplication\nC = torch.mm(A, B)\nC\n>> tensor([[0, 2],\n           [0, 2]])\n\n\nUngraded lab\n1.1_2 Two-Dimensional Tensors_v2.ipynb\n\n\n\nDerivatives in Pytorch\n\nDerivatives\nusing \\(y(x)=x^2\\)\nx = torch.tensor(2., requires_grad=True)\ny = x ** 2\n\n#calculate derivative df/dx\ny.backward()\n#evaluate at x : df/dx(x)\nx.grad\n>> tensor(4.)\nusing \\(z(x)=x^2+2x+1\\)\nx = torch.tensor(2., requires_grad=True)\nz = x**2 + 2*x + 1\nz.backward()\nx.grad\n>> tensor(6.)\nNote: in my version of pytorch (1.7.1), I cannot use torch.int dtypes.\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-92-979d0f10c1e7> in <module>\n----> 3 x = torch.tensor(2, requires_grad=True)\n      4 z = x**2 + 2*x + 1\n      5 z.backward()\nRuntimeError: Only Tensors of floating point and complex dtype can require gradients\n\n\nPartial derivatives\nusing \\(f(u, v)=uv+u^2\\), \\(\\frac{\\partial f(u,v)}{\\partial u} = v+2u\\), \\(\\frac{\\partial f(u,v)}{\\partial v} = u\\)\nu = torch.tensor(1., requires_grad=True)\nv = torch.tensor(2., requires_grad=True)\n\nf = u*v + u**2\n\n#calculate all partial derivatives df/du and df/dv\nf.backward()\n#evaluate partial derivative with respect to u df/du at u, v : df/du(u, v)\nu.grad\n>> tensor(4.)\n#evaluate partial derivative with respect to v df/dv at u, v : df/dv(u, v)\nv.grad\n>> tensor(1.)\n\n\nUngraded lab\n1.2derivativesandGraphsinPytorch_v2.ipynb\nWith some explanation about .detach() pointing to torch.autograd documentation. In this page, there is a link to walkthrough of backprop video.\nWill have to go back to .detach()\n\n\n\nSimple Dataset\n\nBuild a Dataset Class and Object\nfrom torch.utils.data import Dataset\n\nclass toy_set(Dataset):\n    def __init__(self, length=100, transform=None):\n        self.x = 2*torch.ones(length, 2)\n        self.y = torch.ones(length, 1)\n        self.len = length\n        self.transform = transform\n    def __getitem__(self, index):\n        sample=self.x[index], self.y[index]\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n    def __len__(self):\n        return self.len\n    \ndataset = toy_set()\nlen(dataset)\n>> 100\ndataset[0]\n(tensor([2., 2.]), tensor([1.]))\n\n\nBuild a Dataset Transform (e.g. normalize or standardize)\nclass add_mult(object):\n    def __init__(self, addx=1, muly=1):\n        self.addx = addx\n        self.muly = muly\n    def __call__(self, sample):\n        x=sample[0]\n        y=sample[1]\n        x=x+self.addx\n        y=y*self.muly\n        sample=x, y\n        return sample\n    \n    \n# automatically apply the transform\na_m = add_mult()\ndataset_ = toy_set(transform=a_m)\ndataset_[0]\n>> (tensor([3., 3.]), tensor([1.]))\n\n\nCompose Transforms\nclass mult(object):\n    def __init__(self, mul=100):\n        self.mul = mul\n\n    def __call__(self, sample):\n        x = sample[0]\n        y = sample[1]\n        x = x * self.mul\n        y = y * self.mul\n        sample = x, y\n        return sample\n    \nfrom torchvision import transforms\ndata_transform = transforms.Compose([add_mult(), mult()])\n\n# automatically apply the composed transform\ndataset_tr = toy_set(transform=data_transform)\ndataset_tr[0]\n>> (tensor([300., 300.]), tensor([100.]))\n\n\nUngraded lab\n1.3.1_simple_data_set_v2.ipynb\n\n\n\nDataset\n\nDataset Class for Images\nfrom PIL import Image\nimport pandas as pd\nimport os\nfrom matplotlib.pyplot import imshow\nfrom torch.utils.data import Dataset, DataLoader\nclass Dataset(Dataset):\n    def __init__(self, csv_file, data_dir, transform=None):\n        self.transform = transform\n        self.data_dir = data_dir\n        data_dir_csv_file = os.path.join(self.data_dir, csv_file)\n        self.data_name = pd.read_csv(data_dir_csv_file)\n        self.len = self.data_name.shape[0]\n    def __len__(self):\n        return self.len\n    def __getitem__(self, idx):\n        img_name=os.path.join(self.data_dir, self.data_name.iloc[idx, 1])\n        image = Image.open(img_name)\n        y = self.data_name.iloc[idx, 0]\n        if self.transform:\n            image = self.transform(image)\n        return image, y\n    \ndef show_data(data_sample, shape = (28, 28)):\n    plt.imshow(data_sample[0].numpy().reshape(shape), cmap='gray')\n    plt.title('y = ' + data_sample[1])\ndataset = Dataset(csv_file=csv_file, data_dir=directory)\nshow_data(dataset[0])\n\n\nTorch Vision Transforms\nimport torchvision.transforms as transforms\ntransforms.CenterCrop(20)\ntransforms.ToTensor()\ncroptensor_data_transform = transforms.Compose( [ transforms.CenterCrop(20), transforms.ToTensor() ] )\ndataset = Dataset(csv_file=csv_file, data_dir=directory, transform=croptensor_data_transform)\ndataset[0][0].shape\n>> torch.Size([1, 20, 20])\n\n\nTorch Vision Datasets\nMNIST example\nimport torchvision.datasets as dsets\ndataset = dsets.MNIST(root='./data', train = False, download = True, transform = transforms.ToTensor())\n\n\nUngraded lab\n1.3.2_Datasets_and_transforms.ipynb\n1.3.3_pre-Built Datasets_and_transforms_v2.ipynb\n\n\n\n\nWeek 2 - Linear Regression\n\nLearning Objectives\n\nLinear Regression Prediction\nLinear Regression Training\nLoss\nGradient Descent\nCost\nLinear Regression Training PyTorch\n\n\n\nnotebook\nnotebook\n\n\nLinear Regression in 1D - Prediction\n\nSimple linear regression - prediction\nimport torch\nw = torch.tensor(2.0, requires_grad=True)\nb = torch.tensor(-1.0, requires_grad=True)\ndef forward(x):\n    y=w*x+b\n    return y\nx=torch.tensor([1.0])\nyhat=forward(x)\nyhat\n>> tensor([1.], grad_fn=<AddBackward0>)\nx=torch.tensor([[1.0],[2.0]])\nforward(x)  \n>> tensor([[1.],\n        [3.]], grad_fn=<AddBackward0>)\n\n\nPyTorch - Class Linear\nfrom torch.nn import Linear\ntorch.manual_seed(1)\nmodel = Linear(in_features=1, out_features=1)\nlist(model.parameters())\n>> [Parameter containing:\n     tensor([[0.5153]], requires_grad=True),\n     Parameter containing:\n     tensor([-0.4414], requires_grad=True)]\nx=torch.tensor([[1.0],[2.0]])\nmodel(x)\n>> tensor([[0.0739],\n        [0.5891]], grad_fn=<AddmmBackward>)\n\n\nPyTorch - Custom Modules\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, in_size, output_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(in_size, output_size)\n    def forward(self, x):\n        out = self.linear(x)\n        return out\nmodel = LR(1, 1)\nlist(model.parameters())\n>> [Parameter containing:\n     tensor([[-0.9414]], requires_grad=True),\n     Parameter containing:\n     tensor([0.5997], requires_grad=True)]\nx=torch.tensor([[1.0],[2.0]])\nmodel(x)\n>> tensor([[-0.3417],\n        [-1.2832]], grad_fn=<AddmmBackward>)\nModel state_dict()\nthis returns a python dictionary. We will use it as our models get more complex. One Function is to map the relationship of the linear layers to its parameters. we can print out the keys and values.\nmodel.state_dict()\n>> OrderedDict([('linear.weight', tensor([[-0.9414]])),\n             ('linear.bias', tensor([0.5997]))])\n\n\nUngraded lab\n2.1Prediction1Dregression_v3.ipynb\n\n\n\nLinear Regression Training\nloss function presented is mean squared error\n\\(l(w,b)=\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}(y_n-(wx_n+b))^2\\)\n\n\nGradient Descent and cost\n\n\nPyTorch Slope\nimport torch\nw=torch.tensor(-10.0, requires_grad=True)\nX=torch.arange(-3,3,0.1).view(-1, 1)\nf = -3*X\nimport matplotlib.pyplot as plt\nplt.plot(X.numpy(), f.numpy())\nplt.show()\nY = f+0.1*torch.randn(X.size())\nplt.plot(X.numpy(), Y.numpy(), 'ro')\nplt.show()\ndef forward(x):\n    return w*x\n\ndef criterion(yhat, y):\n    return torch.mean((yhat-y)**2)\nlr = 0.1\nfor epoch in range(4):\n    Yhat = forward(X)\n    loss= criterion(Yhat, Y)\n    loss.backward()\n    w.data = w.data - lr*w.grad.data\n    w.grad.data.zero_()\n\nUngraded lab\n2.2_linear_regression_one_parameter_v3.ipynb\n\n\n\nLinear Regression Training in PyTorch\n\nCost surface\n# The class for plot the diagram\n\nclass plot_error_surfaces(object):\n    \n    # Constructor\n    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n        W = np.linspace(-w_range, w_range, n_samples)\n        B = np.linspace(-b_range, b_range, n_samples)\n        w, b = np.meshgrid(W, B)    \n        Z = np.zeros((30,30))\n        count1 = 0\n        self.y = Y.numpy()\n        self.x = X.numpy()\n        for w1, b1 in zip(w, b):\n            count2 = 0\n            for w2, b2 in zip(w1, b1):\n                Z[count1, count2] = np.mean((self.y - w2 * self.x + b2) ** 2)\n                count2 += 1\n            count1 += 1\n        self.Z = Z\n        self.w = w\n        self.b = b\n        self.W = []\n        self.B = []\n        self.LOSS = []\n        self.n = 0\n        if go == True:\n            plt.figure()\n            plt.figure(figsize = (7.5, 5))\n            plt.axes(projection='3d').plot_surface(self.w, self.b, self.Z, rstride = 1, cstride = 1,cmap = 'viridis', edgecolor = 'none')\n            plt.title('Cost/Total Loss Surface')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.show()\n            plt.figure()\n            plt.title('Cost/Total Loss Surface Contour')\n            plt.xlabel('w')\n            plt.ylabel('b')\n            plt.contour(self.w, self.b, self.Z)\n            plt.show()\n    \n    # Setter\n    def set_para_loss(self, W, B, loss):\n        self.n = self.n + 1\n        self.W.append(W)\n        self.B.append(B)\n        self.LOSS.append(loss)\n    \n    # Plot diagram\n    def final_plot(self): \n        ax = plt.axes(projection = '3d')\n        ax.plot_wireframe(self.w, self.b, self.Z)\n        ax.scatter(self.W,self.B, self.LOSS, c = 'r', marker = 'x', s = 200, alpha = 1)\n        plt.figure()\n        plt.contour(self.w,self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n    \n    # Plot diagram\n    def plot_ps(self):\n        plt.subplot(121)\n        plt.ylim\n        plt.plot(self.x, self.y, 'ro', label=\"training points\")\n        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label = \"estimated line\")\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.ylim((-10, 15))\n        plt.title('Data Space Iteration: ' + str(self.n))\n\n        plt.subplot(122)\n        plt.contour(self.w, self.b, self.Z)\n        plt.scatter(self.W, self.B, c = 'r', marker = 'x')\n        plt.title('Total Loss Surface Contour Iteration' + str(self.n))\n        plt.xlabel('w')\n        plt.ylabel('b')\n        plt.show()\n        \nget_surface = plot_error_surfaces(15, 15, X, Y, 30)\n\n\n\nimg\n\n\n\n\n\nimg\n\n\n\n\nPyTorch (hard way)\ndef forward(x):\n    y=w*x+b\n    return y\ndef criterion(yhat, y):\n    return torch.mean((yhat-y)**2)\n\nw = torch.tensor(-15.0, requires_grad=True)\nb = torch.tensor(-10.0, requires_grad=True)\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = 1*X-1\nY = f+0.1*torch.rand(X.size())\nlr = 0.1\nfor epoch in range(15):\n    Yhat=forward(X)\n    loss=criterion(Yhat, Y)\n    loss.backward()\n    w.data=w.data-lr*w.grad.data\n    w.grad.data.zero_()\n    b.data=b.data-lr*b.grad.data\n    b.grad.data.zero_()\n\n\nUngraded lab\n2.3_training_slope_and_bias_v3.ipynb\n\n\n\nStochastic Gradient Descent and the Data Loader\n\nStochastic Gradient Descent in PyTorch\nw = torch.tensor(-15.0, requires_grad=True)\nb = torch.tensor(-10.0, requires_grad=True)\nX = torch.arange(-3, 3, 0.1).view(-1, 1)\nf = -3*X\nY=f+0.1*torch.randn(X.size())\n\ndef forward(x):\n    y=w*x+b\n    return y\ndef criterion(yhat, y):\n    return torch.mean((yhat-y)**2)\nlr = 0.1\nfor epoch in range(4):\n    for x, y in zip(X, Y):\n        yhat=forward(x)\n        loss=criterion(yhat, y)\n        loss.backward()\n        w.data=w.data-lr*w.grad.data\n        w.grad.data.zero_()\n        b.data=b.data-lr*b.grad.data\n        b.grad.data.zero_()\n\n\nStochastic Gradient Descent DataLoader\ndataset\nfrom torch.utils.data import Dataset\n\nclass Data(Dataset):\n    def __init__(self):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.y = -3*X+1\n        self.len = self.x.shape[0]\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    def __len__(self):\n        return self.len\n    \ndataset = Data()\ndataloader\nfrom torch.utils.data import DataLoader\n\ndataset=Data()\ntrainloader = DataLoader(dataset=dataset, batch_size=1)\nstochastic gradient descent\nfor x, y in trainloader:\n    yhat = forward(x)\n    loss = criterion(yhat, y)\n    loss.backward()\n    w.data=w.data-lr*w.grad.data\n    b.data=b.data-lr*b.grad.data\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n\n\nUngraded lab\n3.1_stochastic_gradient_descent_v3.ipynb\n\n\n\nMini-Batch Gradient Descent\nIterations = \\(\\frac{\\text{training size}}{\\text{batch size}}\\)\n\nMini-Batch Gradient Descent in Pytorch\ndataset = Data()\ntrainloader = DataLoader(dataset=dataset, batch_size=5)\n\nlr=0.1\nLOSS = []\nfor epoch in range(4):\n    for x, y in trainloader:\n        yhat=forward(x)\n        loss = criterion(yhat, y)\n        loss.backward()\n        w.data=w.data-lr*w.grad.data\n        b.data=b.data-lr*b.grad.data\n        w.grad.data.zero_()\n        b.grad.data.zero_()      \n        LOSS.append(loss.item())\n\n\n\nOptimization in PyTorch\ncriterion = nn.MSELoss()\ntrainloader = DataLoader(dataset=dataset, batch_size=1)\nmodel = LR(1,1)\nfrom torch import nn, optim\noptimizer = optim.SGD(model.parameters(), lr = 0.01)\noptimizer.state_dict()\n>> {'state': {},\n 'param_groups': [{'lr': 0.01,\n   'momentum': 0,\n   'dampening': 0,\n   'weight_decay': 0,\n   'nesterov': False,\n   'params': [0, 1]}]}\nfor epoch in range(100):\n    for x, y in trainloader:\n        yhat = model(x)\n        loss = criterion(yhat, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n\nimage.png\n\n\n\nUngraded lab\n3.3_PyTorchway_v3.ipynb\n\n\n\nTraining, Validation and Test Split\nstandard explanation about Train, Validation, Test\n\nTraining, Validation and Test Split in PyTorch\nDataset to generate train_data and val_data\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data(Dataset):\n    def __init__(self, train = True):\n        self.x = torch.arange(-3, 3, 0.1).view(-1, 1)\n        self.f = -3*self.x+1\n        self.y = self.f+0.1*torch.randn(self.x.size())\n        self.len = self.x.shape[0]\n        if train == True:\n            self.y[0] = 0\n            self.y[50:55] = 20\n        else:\n            pass\n        \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return self.len\n                \n        \ntrain_data = Data()\nval_data = Data(train=False)\nLR model\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n    def forward(self, x):\n        out=self.linear(x)\n        return out\ncriterion = nn.MSELoss()\n\ntrainloader = DataLoader(dataset=train_data, batch_size=1)\nepochs = 10\nlearning_rates = [0.0001, 0.001, 0.01, 0.1, 1]\nvalidation_error = torch.zeros(len(learning_rates))\ntest_error=torch.zeros(len(learning_rates))\nMODELS=[]\nfrom torch import optim\nfrom tqdm import tqdm\nfor i, learning_rate in tqdm(enumerate(learning_rates)):\n    model = LR(1,1)\n    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n    \n    for epoch in range(epochs):\n        for x, y in trainloader:\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n    yhat=model(train_data.x)\n    loss=criterion(yhat, train_data.y)\n    test_error[i]=loss.item()\n\n    yhat=model(val_data.x)\n    loss=criterion(yhat, val_data.y)\n    validation_error[i]=loss.item()\n    MODELS.append(model)\nimport numpy as np\nplt.semilogx(np.array(learning_rates), validation_error.numpy(), label='training cost/total loss')\nplt.semilogx(np.array(learning_rates), test_error.numpy(), label='validation cost/total loss')\nplt.ylabel('Cost Total loss')\nplt.xlabel('learning rate')\nplt.legend()\nplt.show()\n\n\n\nimg\n\n\n\n\n\n\nWeek 3 - Multiple Input Output Linear Regression - Logistic Regression for Classification\n\nLearning Objectives\n\nMultiple Linear Regression\nMultiple Linear Regression Training\nLinear Regression Multiple Outputs\nLinear Regression Multiple Outputs Training\n\n\n\nnotebook\nnotebook\n\n\nMultiple Input Linear Regression Prediction\n\nClass Linear\nimport torch\nfrom torch.nn import Linear\ntorch.manual_seed(1)\nmodel = Linear(in_features=2, out_features=1)\nlist(model.parameters())\n>> [Parameter containing:\n tensor([[ 0.3643, -0.3121]], requires_grad=True),\n Parameter containing:\n tensor([-0.1371], requires_grad=True)]\nmodel.state_dict()\n>> OrderedDict([('weight', tensor([[ 0.3643, -0.3121]])),\n             ('bias', tensor([-0.1371]))])\n#predictions for multiple samples\nX = torch.tensor([[1.0, 1.0], [1.0, 2.0], [1.0, 3.0]])\nyhat = model(X)\nyhat\n>> tensor([[-0.0848],\n        [-0.3969],\n        [-0.7090]], grad_fn=<AddmmBackward>)\n\n\nCustom Modules\nimport torch.nn as nn\n\nclass LR(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n\nUngraded lab\n4.1.multiple_linear_regression_prediction_v2.ipynb\n\n\n\nMultiple Input Linear Regression Training\n\nCost function and Gradient Descent for Multiple Linear Regression\nCost function\n\\[l(w,b)=\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}(y_n-(x_nw+b))^2\\]\nGradient of loss function with respect to the weights\n\\[\\nabla l(w,b) = \\begin{bmatrix}\\frac{\\partial l(w,b)}{\\partial w_1}\\\\ \\vdots \\\\\\frac{\\partial l(w,b)}{\\partial w_d}\\end{bmatrix}\\]\nGradient of loss function with respect to the bias\n\\[\\frac{\\partial l(w,b)}{\\partial b}\\]\nUpdate of weights\n\\[w^{k+1} = w^k-\\eta \\nabla l(w^k,b^k)\\]\n\\[\\begin{bmatrix} w_1^{k+1}\\\\ \\vdots\\\\ w_d^{k+1}\\\\\\end{bmatrix}=\\begin{bmatrix} w_1^{k}\\\\ \\vdots\\\\ w_d^{k}\\\\\\end{bmatrix}-\\eta \\begin{bmatrix}\\frac{\\partial l(w^k,b^k)}{\\partial w_1}\\\\ \\vdots \\\\\\frac{\\partial l(w^k,b^k)}{\\partial w_d}\\end{bmatrix}\\]\nand update of bias\n\\[b^{k+1}=b^k-\\eta \\frac{\\partial l(w^k,b^k)}{\\partial b}\\]\n\n\nTrain the model in PyTorch\nfrom torch import nn, optim\nimport torch\n\nclass LR(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n    def forward(self, x):\n        out = self.linear(x)\n        return out\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data2D(Dataset):\n    def __init__(self):\n        self.x = torch.zeros(20,2)\n        self.x[:, 0] = torch.arange(-1,1,0.1)\n        self.x[:, 1] = torch.arange(-1,1,0.1)\n        self.w = torch.tensor([ [1.0], [1.0]])\n        self.b = 1\n        self.f = torch.mm(self.x, self.w)+self.b\n        self.y = self.f + 0.1*torch.randn((self.x.shape[0], 1))\n        self.len = self.x.shape[0]\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    def __len__(self):\n        return self.len\ndata_set = Data2D()\ncriterion = nn.MSELoss()\ntrainloader = DataLoader(dataset=data_set, batch_size=2)\nmodel = LR(input_size=2, output_size=1)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nfor epoch in range(100):\n    for x, y in trainloader:\n        yhat = model(x)\n        loss = criterion(yhat, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()   \n\n\nUngraded lab\n4.2.multiple_linear_regression_training_v2.ipynb\n\n\n\nMultiple Output Linear Regression\n\nLinear regression with multiple outputs\n\n\n\nimage.png\n\n\n\n\nCustom Modules\nimport torch.nn as nn\nimport torch\nclass LR(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(LR, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n    \ntorch.manual_seed(1)\nmodel = LR(input_size=2, output_size=2)\n\nlist(model.parameters())\n>> [Parameter containing:\n tensor([[ 0.3643, -0.3121],\n         [-0.1371,  0.3319]], requires_grad=True),\n Parameter containing:\n tensor([-0.6657,  0.4241], requires_grad=True)]\n\n#with 2 columns and 3 rows\nX=torch.tensor([[1.0, 1.0], [1.0,2.0], [1.0, 3.0]])\nYhat = model(X)\nYhat\n>> tensor([[-0.6135,  0.6189],\n        [-0.9256,  0.9508],\n        [-1.2377,  1.2827]], grad_fn=<AddmmBackward>)\n\n\nUngraded lab\n4.3.multi-target_linear_regression.ipynb\n\n\n\nMultiple Output Linear Regression Training\n\nTraining in PyTorch\nTraining is the same, what changes is Dataset:\nfrom torch.utils.data import Dataset, DataLoader\n\nclass Data2D(Dataset):\n    def __init__(self):\n        self.x = torch.zeros(20,2)\n        self.x[:, 0] = torch.arange(-1,1,0.1)\n        self.x[:, 1] = torch.arange(-1,1,0.1)\n        self.w = torch.tensor([ [1.0, -1.0], [1.0, -1.0]])\n        self.b = torch.tensor([[1.0, -1.0]])\n        self.f = torch.mm(self.x, self.w)+self.b\n        self.y = self.f + 0.1*torch.randn((self.x.shape[0], 1))\n        self.len = self.x.shape[0]\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    def __len__(self):\n        return self.len\nand model instantiation\nfrom torch import nn, optim\n\ndata_set = Data2D()\ncriterion = nn.MSELoss()\ntrainloader = DataLoader(dataset=data_set, batch_size=1)\nmodel = LR(input_size=2, output_size=2)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\nTraining:\nfor epoch in range(100):\n    for x, y in trainloader:\n        yhat = model(x)\n        loss = criterion(yhat, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\nUngraded lab\n4.4.training_multiple_output_linear_regression.ipynb\n\n\n\nLinear Classifier and Logistic Regression\n\\[\\sigma(z)=\\frac{1}{1+e^{-z}}\\]\nsigmoid is used as the threshold function in logistic regression\n\n\nLogistic Regression: Prediction\n\nlogistic function in PyTorch\nas a function: torch.sigmoid\nimport torch\nimport matplotlib.pyplot as plt\n\nz = torch.arange(-100, 100, 0.1).view(-1, 1)\nyhat = torch.sigmoid(z)\nplt.plot(z.numpy(), yhat.numpy())\n\n\n\nimg\n\n\nas a class: nn.Signmoid()\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\nz = torch.arange(-100, 100, 0.1).view(-1, 1)\nsig = nn.Sigmoid()\nyhat = sig(z)\nplt.plot(z.numpy(), yhat.numpy())\ntorch.nn.Sigmoid vs torch.sigmoid - PyTorch Forums\n\ntorch.nn.Sigmoid (note the capital “S”) is a class. When you\ninstantiate it, you get a function object, that is, an object that you\ncan call like a function. In contrast, torch.sigmoid is a function.\n\n\n\nnn.Sequential\n\n\n\nimage.png\n\n\nsequential_model = nn.Sequential(nn.Linear(1,1), nn.Sigmoid())\n\n\nnn.Module\nimport torch.nn as nn\n\nclass logistic_regression(nn.Module):\n    def __init__(self, in_size):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(in_size, 1)\n    def forward(self, x):\n        z = torch.sigmoid(self.linear(x))\n        return z\n    \ncustom_model = logistic_regression(1)\n\n\nMaking a prediction\nx=torch.tensor([[1.0], [2.0]])\ncustom_model(x)\n>> tensor([[0.4129],\n        [0.3936]], grad_fn=<SigmoidBackward>)\nsequential_model(x)\n>> tensor([[0.2848],\n        [0.2115]], grad_fn=<SigmoidBackward>)\n\n\nMultidimensional Logistic Regression\ncustom_2D_model = logistic_regression(2)\nsequential_2D_model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n\nx=torch.tensor([[1.0, 2.0]])\nyhat = sequential_2D_model(x)\nyhat\n>> tensor([[0.7587]], grad_fn=<SigmoidBackward>)\n\n\nUngraded lab\n5.1logistic_regression_prediction_v2.ipynb\n\n\n\nBernoulli Distribution and Maximum Likelihood Estimation\nTo fine the parameter values of the Bernoulli distribution, we do not maximize the likelihood function but the log of the likelihood function: Loss likelihood which is given by\n\\[l(\\theta) = \\ln(p(Y|\\theta))=\\displaystyle\\sum_{n=1}^{N}y_n \\ln(\\theta)+(1-y_n) \\ln(1-\\theta)\\]\nNote: We want to get\n\\[\\hat\\theta = argmax_\\theta(P(Y|\\theta))\\]\nwhere\n\\[P(Y|\\theta) = \\displaystyle\\prod_{n=1}^{N}\\theta^{y_n}(1-\\theta)^{1-y_n}\\]\n\n\nLogistic Regression Cross Entropy Loss\nLoss function \\(l(w,b)=\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}(y_n-\\sigma(wx_n+b))^2\\)\n\nCross entropy loss\n\\[l(\\theta)=-\\frac{1}{N}\\displaystyle\\sum_{n=1}^{N}y_n \\ln(\\sigma(wx_n+b))+(1-y_n)\\ln(1-\\sigma(wx_n+b))\\]\ndef criterion(yhat, y):\n    out = -1 * torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n    return out\n\n\nLogistic Regression in PyTorch\nCreate a model (using Sequential)\nmodel = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\nor create a custom one\nimport torch.nn as nn\n\nclass logistic_regression(nn.Module):\n    def __init__(self, in_size):\n        super(logistic_regression, self).__init__()\n        self.linear = nn.Linear(in_size, 1)\n    def forward(self, x):\n        z = torch.sigmoid(self.linear(x))\n        return z\nThen define our loss function\ndef criterion(yhat, y):\n    out = -1 * torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n    return out\nor simply BCE (binary cross entropy)\ncriterion = nn.BCELoss()\nPutting all pieces together:\n#dataset\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass Data(Dataset):\n    def __init__(self):\n        self.x = torch.arange(-1, 1, 0.1).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0], 1)\n        self.y[self.x[:, 0] > 0.2] = 1\n        self.len = self.x.shape[0]\n    def __getitem__(self, index):      \n        return self.x[index], self.y[index]\n    def __len__(self):\n        return self.len\n    \ndataset = Data()\n\n# dataloader\n\nfrom torch.utils.data import DataLoader\ntrainloader = DataLoader(dataset=dataset, batch_size=1)\n\n# model\n\nimport torch.nn as nn\nmodel = nn.Sequential(nn.Linear(1, 1), nn.Sigmoid())\n\n# optimizer\n\nfrom torch import optim\noptimizer = optim.SGD(model.parameters(), lr = 0.01)\n\n# loss\n\ncriterion = nn.BCELoss()\n\n# training\n\nfor epoch in range(100):\n    for x, y in trainloader:\n        yhat = model(x)\n        loss = criterion(yhat, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\nUngraded lab\n5.2.2bad_inshilization_logistic_regression_with_mean_square_error_v2.ipynb\n\n\n\n\nWeek 4 - Softmax regression\n\nLearning Objectives\n\nUsing Lines to Classify Data\nSoftmax Prediction \u000bin PyTorch\nSoftmax Pytorch MNIST\n\n\n\nnotebook\nnotebook\n\n\nSoftmax Prediction\nSoftmax is a combination of logistic regression and argmax\n\n\n\nimage.png\n\n\n\n\nSoftmax function\n\nCustom module using nn.module\nimport torch.nn as nn\n\nclass Softmax(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(Softmax, self).__init__()\n        self.linear = nn.Linear(in_size, out_size)\n    def forward(self, x):\n        out = self.linear(x)\n        return out\nimport torch\ntorch.manual_seed(1)\n# 2 dimensions input samples and 3 output classes\nmodel = Softmax(2,3)\n\nx = torch.tensor([[1.0, 2.0]])\nz = model(x)\nz\n>> tensor([[-0.4053,  0.8864,  0.2807]], grad_fn=<AddmmBackward>)\n\n_, yhat = z.max(1)\nyhat\n>> tensor([1])\nand with multiple samples\nX=torch.tensor([[1.0, 1.0],[1.0, 2.0],[1.0, -3.0]])\nz = model(X)\nz\n>> tensor([[-0.0932,  0.5545, -0.1433],\n        [-0.4053,  0.8864,  0.2807],\n        [ 1.1552, -0.7730, -1.8396]], grad_fn=<AddmmBackward>)\n\n_, yhat = z.max(1)\nyhat\n>> tensor([1, 1, 0])\n\n\n\nSoftmax PyTorch\n\nLoad Data\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\ntrain_dataset = dsets.MNIST(root='./data', train = True, download = True, transform=transforms.ToTensor())\n\nvalidation_dataset = dsets.MNIST(root='./data', train = False, download = True, transform=transforms.ToTensor())\ntrain_dataset[0] is a tuple with the image and the class:\n\n\nCreate Model\nimport torch.nn as nn\n\nclass Softmax(nn.Module):\n    def __init__(self, in_size, out_size):\n        super(Softmax, self).__init__()\n        self.linear = nn.Linear(in_size, out_size)\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n    \ninput_dim = 28 * 28\noutput_dim = 10\nmodel = Softmax(input_dim, output_dim)\ncriterion = nn.CrossEntropyLoss()\n\nimport torch.optim as optim\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nn_epochs = 100\naccuracy_list = []\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)\n\n\nTrain Model\nfrom tqdm import tqdm\n\nfor epoch in tqdm(range(n_epochs)):\n    for x, y in train_loader:\n        optimizer.zero_grad()\n        z = model(x.view(-1, 28 * 28))\n        loss = criterion(z, y)\n        loss.backward()\n        optimizer.step()\n    correct = 0\n    for x_test, y_test in validation_loader:\n        z = model(x_test.view(-1, 28 * 28))\n        _, yhat = torch.max(z.data, 1)\n        correct = correct+(yhat == y_test).sum().item()\n    accuracy = correct / y.shape[0]\n    accuracy_list.append(accuracy)\n\n\nUngraded lab\n5.4softmax_in_one_dimension_v2.ipynb\n\n\nUngraded lab\n6.2lab_predicting _MNIST_using_Softmax_v2.ipynb\n# The function to plot parameters\n\ndef PlotParameters(model): \n    W = model.state_dict()['linear.weight'].data\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(2, 5)\n    fig.subplots_adjust(hspace=0.01, wspace=0.1)\n    for i, ax in enumerate(axes.flat):\n        if i < 10:\n            \n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"class: {0}\".format(i))\n\n            # Plot the image.\n            ax.imshow(W[i, :].view(28, 28), vmin=w_min, vmax=w_max, cmap='seismic')\n\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n        # Ensure the plot is shown correctly with multiple plots\n        # in a single Notebook cell.\n    plt.show()\n    \n# Plot the parameters\n\nPlotParameters(model)\n\n\n\nimg\n\n\n\n\n\n\nWeek 4 - Shallow neural networks\n\nLearning Objectives\n\nSimple Neural Networks\nMore Hidden Neurons\nNeural Networks with Multiple Dimensional\nMulti-Class Neural Networks\nBackpropagation\nActivation Functions\n\n\n\nnotebook\nnotebook\n\n\nNeural networks in One Dimension\n\nusing nn.Module\nimport torch\nimport torch.nn as nn\nfrom torch import sigmoid\n\nclass Net(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=sigmoid(self.linear1(x))\n        x=sigmoid(self.linear2(x))\n        return x\nmodel = Net(1, 2, 1)\nx = torch.tensor([0.0])\nyhat = model(x)\nyhat\n>> tensor([0.5972], grad_fn=<SigmoidBackward>)\n\n# multiple samples\nx = torch.tensor([[0.0], [2.0], [3.0]])\nyhat = model(x)\nyhat\n>> tensor([[0.5972],\n        [0.5925],\n        [0.5894]], grad_fn=<SigmoidBackward>)\n\n# to get a discrete value we apply a threshold\nyhat = yhat < 0.59\nyhat\n>> tensor([[False],\n           [False],\n           [ True]])\nmodel.state_dict()\n>> OrderedDict([('linear1.weight',\n              tensor([[0.3820],\n                      [0.4019]])),\n             ('linear1.bias', tensor([-0.7746, -0.3389])),\n             ('linear2.weight', tensor([[-0.3466,  0.2201]])),\n             ('linear2.bias', tensor([0.4115]))])\n\n\nusing nn.Sequential\nmodel = nn.Sequential(nn.Linear(1, 2), nn.Sigmoid(), nn.Linear(2, 1), nn.Sigmoid())\n\n\ntrain the model\nwe create the data\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0]>-4) & (X[:, 0] <4)] = 1.0\nwe create a training function\nfrom tqdm import tqdm\n\ndef train(Y, X, model, optimizer, criterion, epochs=1000):\n    cost = []\n    total = 0\n    for epoch in tqdm(range(epochs)):\n        total = 0\n        for x, y in zip(X, Y):\n            yhat = model(x)\n            loss = criterion(yhat, y.view(-1))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total+=loss.item()\n        cost.append(total)\n    return cost\nand the training process is now\n#loss\ncriterion = nn.BCELoss()\n\n#data\nX = torch.arange(-20, 20, 1).view(-1, 1).type(torch.FloatTensor)\nY = torch.zeros(X.shape[0])\nY[(X[:, 0]>-4) & (X[:, 0] <4)] = 1.0\n\n#model\nmodel = Net(1, 2, 1)\n\n#optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n\n#train the model\ncost = train(Y, X, model, optimizer, criterion, epochs=1000)\n>> 100%|██████████| 1000/1000 [00:12<00:00, 76.96it/s]\n\n\nUngraded lab\n7.1_simple1hiddenlayer.ipynb\nI like how to display intermediate representations of learning performance:\n\n\n\nimg\n\n\n# The function for plotting the model\n\ndef PlotStuff(X, Y, model, epoch, leg=True):\n    \n    plt.plot(X.numpy(), model(X).detach().numpy(), label=('epoch ' + str(epoch)))\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    if leg == True:\n        plt.legend()\n    else:\n        pass\nactivation values (called in the training loop). Using model variables (model.a1) which seems a bad practice.\n\n\n\nimg\n\n\nplt.scatter(model.a1.detach().numpy()[:, 0], model.a1.detach().numpy()[:, 1], c=Y.numpy().reshape(-1))\nplt.title('activations')\nplt.show()\nand final loss curve\n\n\n\nimg\n\n\n\n\n\nNeural Networks More Hidden Neurons\n\nusing nn.Module\nimport torch\nimport torch.nn as nn\nfrom torch import sigmoid\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nclass to get our dataset\nclass Data(Dataset):\n    def __init__(self):\n        self.x = torch.linspace(-20, 20, 100).view(-1, 1)\n        self.y = torch.zeros(self.x.shape[0])\n        self.y[(self.x[:, 0]>-10) & (self.x[:, 0]<-5)] = 1\n        self.y[(self.x[:, 0]>5) & (self.x[:, 0]<10)] = 1\n        self.y = self.y.view(-1, 1)\n        self.len = self.x.shape[0]\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    def __len__(self):\n        return self.len\nclass for creating our model\nclass Net(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=sigmoid(self.linear1(x))\n        x=sigmoid(self.linear2(x))\n        return x\nand the function to train our model\n# The function for plotting the model\ndef PlotStuff(X, Y, model):  \n    plt.plot(X.numpy(), model(X).detach().numpy())\n    plt.plot(X.numpy(), Y.numpy(), 'r')\n    plt.xlabel('x')\n    \ndef train(data_set, model, criterion, train_loader, optimizer, epochs=5):\n    cost = []\n    total=0\n    for epoch in tqdm(range(epochs)):\n        total=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            loss.backward()\n            optimizer.step()\n            total+=loss.item() \n            PlotStuff(data_set.x, data_set.y, model)\n        cost.append(total)\n    return cost\nprocess for training is identical to logistic regression\ncriterion = nn.BCELoss()\ndata_set = Data()\ntrain_loader = DataLoader(dataset=data_set, batch_size=100)\nmodel = Net(1, 6, 1)\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\ntrain(data_set, model, criterion, train_loader, optimizer, epochs=1000)\n\n\nusing nn.Sequential\nmodel = nn.Sequential(\n    nn.Linear(1, 7),\n    nn.Sigmoid(),\n    nn.Linear(7, 1),\n    nn.Sigmoid()\n)\n\n\nUngraded lab\n7.2multiple_neurons.ipynb\n\n\n\nNeural Networks with Multiple Dimensional Input\n\nimplementation\nimport torch\nimport torch.nn as nn\nfrom torch import sigmoid\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nwe create a dataset class\nclass XOR_Data(Dataset):\n    def __init__(self, N_s=100):\n        self.x = torch.zeros((N_s, 2))\n        self.y = torch.zeros((N_s, 1))\n        for i in range(N_s // 4):\n            self.x[i, :] = torch.Tensor([0.0, 0.0])\n            self.y[i, 0] = torch.Tensor([0.0])\n            self.x[i + N_s // 4, :] = torch.Tensor([0.0, 1.0])\n            self.y[i + N_s // 4, 0] = torch.Tensor([1.0])\n            self.x[i + N_s // 2, :] = torch.Tensor([1.0, 0.0])\n            self.y[i + N_s // 2, 0] = torch.Tensor([1.0])\n            self.x[i + 3 * N_s // 4, :] = torch.Tensor([1.0, 1.0])\n            self.y[i + 3 * N_s // 4, 0] = torch.Tensor([0.0])\n            self.x = self.x + 0.01 * torch.randn((N_s, 2))\n        self.len = N_s\n            \n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    def __len__(self):\n        return self.len      \n    # Plot the data\n    def plot_stuff(self):\n        plt.plot(self.x[self.y[:, 0] == 0, 0].numpy(), self.x[self.y[:, 0] == 0, 1].numpy(), 'o', label=\"y=0\")\n        plt.plot(self.x[self.y[:, 0] == 1, 0].numpy(), self.x[self.y[:, 0] == 1, 1].numpy(), 'ro', label=\"y=1\")\n        plt.legend()\n\n\n\nimg\n\n\nWe create a class for creating our model\nclass Net(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=sigmoid(self.linear1(x))\n        x=sigmoid(self.linear2(x))\n        return x\nWe create a function to train our model\n# Calculate the accuracy\n\ndef accuracy(model, data_set):\n    return np.mean(data_set.y.view(-1).numpy() == (model(data_set.x)[:, 0] > 0.5).numpy())\n\ndef train(data_set, model, criterion, train_loader, optimizer, epochs=5):\n    COST = []\n    ACC = []\n    for epoch in tqdm(range(epochs)):\n        total=0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            yhat = model(x)\n            loss = criterion(yhat, y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #cumulative loss \n            total+=loss.item()\n        ACC.append(accuracy(model, data_set))\n        COST.append(total)\n        \n    return COST\nprocess for training is identical to logistic regression\ncriterion = nn.BCELoss()\ndata_set = XOR_Data()\ntrain_loader = DataLoader(dataset=data_set, batch_size=1)\nmodel = Net(2, 4, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\ntrain(data_set, model, criterion, train_loader, optimizer, epochs=500)\n\n\noverfitting and underfitting\nSolution:\n\nuse validation data to determine optimum number of neurons\nget more data\nregularization: for example dropout\n\n\n\nUngraded lab\n7.3xor_v2.ipynb\n\n\n\nMulti-Class Neural Networks\n\nusing nn.Module\nwe don’t have sigmoid for the output, and D_out is our number of classes\nclass Net(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=sigmoid(self.linear1(x))\n        x=(self.linear2(x))\n        return x\n\n\nusing nn.Sequential\ninput_dim = 2\nhidden_dim = 6\noutput_dim = 3\nmodel = nn.Sequential(\n    nn.Linear(input_dim, hidden_dim),\n    nn.Sigmoid(),\n    nn.Linear(hidden_dim, output_dim)\n)\n\n\ntraining\nwe create a validation and training dataset\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\ntrain_dataset = dsets.MNIST(root='./data', train = True, download = True, transform=transforms.ToTensor())\nvalidation_dataset = dsets.MNIST(root='./data', train = False, download = True, transform=transforms.ToTensor())\nwe create a validation and training loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=2000)\ncriterion = nn.CrossEntropyLoss()\nwe create the training function\nfrom tqdm import tqdm\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss': [],'validation_accuracy': []}  \n    for epoch in tqdm(range(epochs)):\n        for i, (x, y) in enumerate(train_loader): \n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n             #loss for every iteration\n            useful_stuff['training_loss'].append(loss.data.item())\n        correct = 0\n        for x, y in validation_loader:\n            #validation \n            z = model(x.view(-1, 28 * 28))\n            _, label = torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n    return useful_stuff\nWe instantiate and train the model\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim, output_dim)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)\nTo plot accuracy and lost\n# Define a function to plot accuracy and loss\n\ndef plot_accuracy_loss(training_results): \n    plt.subplot(2, 1, 1)\n    plt.plot(training_results['training_loss'], 'r')\n    plt.ylabel('loss')\n    plt.title('training loss iterations')\n    plt.subplot(2, 1, 2)\n    plt.plot(training_results['validation_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epochs')   \n    plt.show()\n    \nplot_accuracy_loss(training_results)\n\n\n\nimg\n\n\nTo plot improper classified items\ncount = 0\nfor x, y in validation_dataset:\n    z = model(x.reshape(-1, 28 * 28))\n    _,yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data(x)\n        count += 1\n    if count >= 5:\n        break\n\n\n\nimg\n\n\n\n\nUngraded lab\n7.4one_layer_neural_network_MNIST.ipynb\n\n\n\nBackpropagation\nFollowing the chain rule in gradient calculation, it happens that gradient results are getting closer and closer to 0. (i.e. vanishing gradient) therefore we cannot improve model parameters.\nOne way to deal with that is to change activation function.\n\n\nActivation functions\n\nsigmoid, tanh, relu\n\n\n\nimg\n\n\n\n\nsigmoid, tanh, relu in PyTorch\nclass Net_sigmoid(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=sigmoid(self.linear1(x))\n        x=(self.linear2(x))\n        return x\nclass Net_tanh(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=torch.tanh(self.linear1(x))\n        x=(self.linear2(x))\n        return x\nclass Net_relu(nn.Module):\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    def forward(self, x):\n        x=torch.relu(self.linear1(x))\n        x=(self.linear2(x))\n        return x\nusing nn.Sequential\nmodel_tanh = nn.Sequential(\n    nn.Linear(input_dim, hidden_dim),\n    nn.Tanh(),\n    nn.Linear(hidden_dim, output_dim)\n)\n\nmodel_relu = nn.Sequential(\n    nn.Linear(input_dim, hidden_dim),\n    nn.ReLU(),\n    nn.Linear(hidden_dim, output_dim)\n)\n\n\nUngraded lab\n7.5.1activationfuction_v2.ipynb\n\n\nUngraded lab\n7.5.2mist1layer_v2.ipynb\nto monitor gpu usage: nvidia-smi -l 1\n\n\n\nimage.png\n\n\n\n\n\n\nWeek 5 - Deep neural networks\n\nLearning Objectives\n\nbuilding deep networks\nDropout\nNeural Network initialization weights\nGradient Descent with Momentum\n\n\n\nnotebook\nnotebook\n\n\nDeep Neural Networks\nDeep, following this course definition, is when number of hidden layers > 1.\n\nusing nn.Module\nimport torch\nimport torch.nn as nn\nfrom torch import sigmoid\nclass Net(nn.Module):\n    def __init__(self, D_in, H1, H2, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H1)\n        self.linear2 = nn.Linear(H1, H2)\n        self.linear3 = nn.Linear(H2, D_out)\n    def forward(self, x):\n        x=sigmoid(self.linear1(x))       \n        x=sigmoid(self.linear2(x))\n        x=self.linear3(x)\n        return x\n\n\nusing nn.Sequential\ninput_dim = 2\nhidden_dim1 = 6\nhidden_dim2 = 4\noutput_dim = 3\nmodel = nn.Sequential(\n    nn.Linear(input_dim, hidden_dim1),\n    nn.Sigmoid(),\n    nn.Linear(hidden_dim1, hidden_dim2),\n    nn.Sigmoid(),    \n    nn.Linear(hidden_dim2, output_dim)\n)\n\n\ntraining\nthere is no change compare to other networks\nwe create a validation and training dataset\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\ntrain_dataset = dsets.MNIST(root='./data', train = True, download = True, transform=transforms.ToTensor())\nvalidation_dataset = dsets.MNIST(root='./data', train = False, download = True, transform=transforms.ToTensor())\nwe create a validation and training loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=2000)\ncriterion = nn.CrossEntropyLoss()\nwe create the training function\nfrom tqdm import tqdm\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n    i = 0\n    useful_stuff = {'training_loss': [],'validation_accuracy': []}  \n    for epoch in tqdm(range(epochs)):\n        for i, (x, y) in enumerate(train_loader): \n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n             #loss for every iteration\n            useful_stuff['training_loss'].append(loss.data.item())\n        correct = 0\n        for x, y in validation_loader:\n            #validation \n            z = model(x.view(-1, 28 * 28))\n            _, label = torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n    return useful_stuff\nWe instantiate and train the model\ninput_dim = 28 * 28\nhidden_dim1 = 50\nhidden_dim2 = 50\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim1, hidden_dim2, output_dim)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)\n\n\nUngraded lab - deep neural networks\n8.1.1mist2layer_v2.ipynb\n\n\n\nimg\n\n\n\n\n\nDeep Neural Networks : nn.ModuleList()\n\njdc\nthis is a nice library to allow breaking down definition of classes in separate notebook cells\nInstallation is as simple as pip install jdc\nand usage is\nimport jdc\nand start a cell with %%add_to <your class name>\n\n\npython implementation\nimport torch\nimport torch.nn as nn\nfrom torch import sigmoid\nimport jdc\nclass Net(nn.Module):\n    def __init__(self, Layers):\n        super(Net, self).__init__()\n        self.hidden = nn.ModuleList()\n        for input_size, output_size in zip(Layers, Layers[1:]):\n            self.hidden.append(nn.Linear(input_size, output_size))\nLayers = [2, 3, 4, 3]\nmodel = Net(Layers)\n%%add_to Net\n\ndef forward(self, x):\n    L = len(self.hidden)\n    for (l, linear_transform) in zip(range(L), self.hidden):\n        if (l < L-1):\n            x = torch.relu(linear_transform(x))\n        else:\n            x = linear_transform(x)\n    return x\n\n\nUngraded lab - nn.ModuleList()\n8.1.2mulitclassspiralrulu_v2.ipynb\n\n\n\nimg\n\n\n\n\n\nDropout\n\nusing nn.Module\nclass Net(nn.Module):\n    def __init__(self, in_size, n_hidden, out_size, p=0):\n        super(Net, self).__init__()\n        self.drop = nn.Dropout(p=p)\n        self.linear1 = nn.Linear(in_size, n_hidden)\n        self.linear2 = nn.Linear(n_hidden, n_hidden)\n        self.linear3 = nn.Linear(n_hidden, out_size)\n    def forward(self, x):\n        x=torch.relu(self.linear1(x))       \n        x=self.drop(x)\n        x=torch.relu(self.linear2(x))\n        x=self.drop(x)\n        x=self.linear3(x)\n        return x\n\n\nusing nn.Sequential\nmodel = nn.Sequential(\n    nn.Linear(1, 10),\n    nn.Dropout(0.5),\n    nn.ReLU(),\n    nn.Linear(10, 12),\n    nn.Dropout(0.5),\n    nn.ReLU(),\n    nn.Linear(12, 1),\n)\n\n\ntraining\ncreate data\nfrom torch.utils.data import Dataset, DataLoader \nimport numpy as np\n# Create data class for creating dataset object\n\nclass Data(Dataset):\n    \n    # Constructor\n    def __init__(self, N_SAMPLES=1000, noise_std=0.15, train=True):\n        a = np.matrix([-1, 1, 2, 1, 1, -3, 1]).T\n        self.x = np.matrix(np.random.rand(N_SAMPLES, 2))\n        self.f = np.array(a[0] + (self.x) * a[1:3] + np.multiply(self.x[:, 0], self.x[:, 1]) * a[4] + np.multiply(self.x, self.x) * a[5:7]).flatten()\n        self.a = a\n       \n        self.y = np.zeros(N_SAMPLES)\n        self.y[self.f > 0] = 1\n        self.y = torch.from_numpy(self.y).type(torch.LongTensor)\n        self.x = torch.from_numpy(self.x).type(torch.FloatTensor)\n        self.x = self.x + noise_std * torch.randn(self.x.size())\n        self.f = torch.from_numpy(self.f)\n        self.a = a\n        if train == True:\n            torch.manual_seed(1)\n            self.x = self.x + noise_std * torch.randn(self.x.size())\n            torch.manual_seed(0)\n        \n    # Getter        \n    def __getitem__(self, index):    \n        return self.x[index], self.y[index]\n    \n    # Get Length\n    def __len__(self):\n        return self.len\n    \n    # Plot the diagram\n    def plot(self):\n        X = data_set.x.numpy()\n        y = data_set.y.numpy()\n        h = .02\n        x_min, x_max = X[:, 0].min(), X[:, 0].max()\n        y_min, y_max = X[:, 1].min(), X[:, 1].max() \n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n        Z = data_set.multi_dim_poly(np.c_[xx.ravel(), yy.ravel()]).flatten()\n        f = np.zeros(Z.shape)\n        f[Z > 0] = 1\n        f = f.reshape(xx.shape)\n        \n        plt.title('True decision boundary  and sample points with noise ')\n        plt.plot(self.x[self.y == 0, 0].numpy(), self.x[self.y == 0,1].numpy(), 'bo', label='y=0') \n        plt.plot(self.x[self.y == 1, 0].numpy(), self.x[self.y == 1,1].numpy(), 'ro', label='y=1')\n        plt.contour(xx, yy, f,cmap=plt.cm.Paired)\n        plt.xlim(0,1)\n        plt.ylim(0,1)\n        plt.legend()\n    \n    # Make a multidimension ploynomial function\n    def multi_dim_poly(self, x):\n        x = np.matrix(x)\n        out = np.array(self.a[0] + (x) * self.a[1:3] + np.multiply(x[:, 0], x[:, 1]) * self.a[4] + np.multiply(x, x) * self.a[5:7])\n        out = np.array(out)\n        return out\n\n\n\nimg\n\n\ninstantiate the model\nmodel_drop = Net(2, 300, 2, p=0.5)\ntrain method tells the model we are in the training phase which will implement the dropout method, later we use the eval method to tell the model it is in the evaluation phase and that will turn off the dropout method\nmodel_drop.train()\noptimizer = torch.optim.Adam(model_drop.parameters(), lr = 0.01)\ncriterion = nn.CrossEntropyLoss()\ndata_set = Data()\nvalidation_set = Data(train=False)\n# Initialize the LOSS dictionary to store the loss\n\nLOSS = {}\nLOSS['training data dropout'] = []\nLOSS['validation data dropout'] = []\ntrain the model\n# Train the model\nfrom tqdm import tqdm\n\nepochs = 500\n\ndef train_model(epochs):\n    \n    for epoch in tqdm(range(epochs)):\n        #all the samples are used for training \n        yhat_drop = model_drop(data_set.x)\n        loss_drop = criterion(yhat_drop, data_set.y)\n\n        #store the loss for both the training and validation data for both models \n        LOSS['training data dropout'].append(loss_drop.item())\n        model_drop.eval()\n        LOSS['validation data dropout'].append(criterion(model_drop(validation_set.x), validation_set.y).item())\n        model_drop.train()\n\n        optimizer.zero_grad()\n        loss_drop.backward()\n        optimizer.step()\n        \ntrain_model(epochs)\n# The function for calculating accuracy\n\ndef accuracy(model, data_set):\n    _, yhat = torch.max(model(data_set.x), 1)\n    return (yhat == data_set.y).numpy().mean()\n\n# Print out the accuracy of the model with dropout\n\nprint(\"The accuracy of the model with dropout: \", accuracy(model_drop, validation_set))\n>> The accuracy of the model with dropout:  0.866\n\n\nUngraded lab - dropout classification\n8.2.1dropoutPredictin_v2.ipynb\n\n\nUngraded lab - dropout regression\n8.2.2dropoutRegression_v2.ipynb\n\n\n\nNeural Network initialization weights\nDifferent methods exist:\n\nuniform distribution for parameters: we simply make the lower bound of the range of the distribution the negative of the inverse of square root of L in. the upper bound of the range of the distribution is the positive of the inverse of square root of L in. See this paper for more details. LeCun, Yann A., et al. “Efficient backprop.” Neural networks: Tricks of the trade. Springer, Berlin, Heidelberg, 2012. 9-48\n\nlinear=nn.Linear(input_size,output_size)\nlinear.weight.data.uniform_(0, 1)\n\nxavier method: Xavier Initialization is another popular method and is used in conjunction with the tanh activation. It takes into consideration the number of input neurons “Lin” as well as the number of neuron in the next layer “L out”. This paper for more details: Glorot, Xavier and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks” 2010.\n\nlinear=nn.Linear(input_size,output_size)\ntorch.nn.init.xavier_uniform_(linear.weight)\n\nHe method: For relu we use the He initialize method, after we create a linear object, We use the following method to initialize the weights, for more info check out the following paper. He, Kaiming, et al. “Delving deep into rectifiers: surpassing human-level performance in imagenet classification”\n\nlinear = nn.Linear(input_size, output_size)\ntorch.nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')\n\nUngraded lab - initialization\n8.3.1.initializationsame.ipynb\n\n\nUngraded lab - Xavier initialization\n8.3.2Xaviermist1layer_v2.ipynb\n\n\nUngraded lab - He initialization\n8.3.3.He_Initialization_v2.ipynb\n\n\n\nGradient Descent with Momentum\n\nPyTorch implementation\nIn PyTorch, this is just defined at optim level\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum = 0.4)\n\n\nUngraded lab - momentum with different polynomial\n8.4.1_MomentumwithPolynomialFunctions_v2.ipynb\n\n\nUngraded lab - Neural Network momentum\n8.4.2_NeuralNetworkswithMomentum_v2.ipynb\n\n\n\nBatch Normalization\n\n\n\nimage.png\n\n\n𝛾, 𝛽 parameters are are actually scale and shift parameters, which we’re going to learn via training.\n\nusing nn.Module\nclass Net_BatchNorm(nn.Module):\n    def __init__(self, in_size, n_hidden1, n_hidden2, out_size):\n        super(Net_BatchNorm, self).__init__()\n\n        self.linear1 = nn.Linear(in_size, n_hidden1)\n        self.linear2 = nn.Linear(n_hidden1, n_hidden2)\n        self.linear3 = nn.Linear(n_hidden2, out_size)\n        \n        self.bn1 = nn.BatchNorm1d(n_hidden1)\n        self.bn2 = nn.BatchNorm1d(n_hidden2)\n        \n    def forward(self, x):\n        x=torch.sigmoid(self.bn1(self.linear1(x)))\n        x=torch.sigmoid(self.bn2(self.linear2(x)))\n        x=self.linear3(x)\n        return x\n\n\nUngraded lab - Batch normalization\n8.5.1BachNorm_v2.ipynb\ncomparing training loss for each iteration and accuracy on validation data for both Batch / No Batch normalization.\n\n\n\nimg\n\n\n\n\n\nimg\n\n\n\n\n\n\nWeek 6 - Convolutional neural networks\n\nLearning Objectives\n\nConvolution\nActivation Functions\nMax Pooling\nConvolution: Multiple Channels\nConvolutional Neural Network\nTORCH-VISION MODELS\n\n\n\nnotebook\nnotebook\n\n\nConvolution\nconvolution explanation from stanford course CS231\n\nconvolution\nconv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\nimage = torch.zeros(1,1,5,5)\nimage[0,0,:,2] = 1\nimage\n>> tensor([[[[0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.],\n          [0., 0., 1., 0., 0.]]]])\nz=conv(image)\n>> tensor([[[[ 0.6065,  0.0728, -0.7915],\n          [ 0.6065,  0.0728, -0.7915],\n          [ 0.6065,  0.0728, -0.7915]]]], grad_fn=<ThnnConv2DBackward>)\nconv.state_dict()\n>> OrderedDict([('weight',\n              tensor([[[[ 0.1132, -0.0418,  0.3140],\n                        [-0.2261, -0.1528, -0.3270],\n                        [-0.2140, -0.1900,  0.2127]]]])),\n             ('bias', tensor([0.0423]))])\n\n\nstride\nconv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride = 2)\n\n\nzeros padding\nconv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride = 2, padding = 1)\n\n\nsize of activation map\n\n\nFeature size = ((Image size + 2 * Padding size − Kernel size) / Stride)+1\n\n\nUngraded lab - What’s convolution\n9.1What_is_Convolution.ipynb\n\n\n\nActivation Functions and Max Polling\n\nActivation function using nn.Module\nimport torch\nimage = torch.zeros(1,1,5,5)\nimage[0,0,:,2] = 1\nconv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\nz=conv(image)\nA = torch.relu(z)\n\n\nActivation function using nn.Sequential\nrelu = nn.ReLU()\nA = relu(z)\n\n\nMax pooling\nmax = nn.MaxPool2d(2, stride=1)\nmax(image)\ntorch.max_pool2d(image, stride=1, kernel_size=2)\n\n\nUngraded lab - Activation Functions and Max Polling\n9.2Activation_max_pooling.ipynb\n\n\n\nMultiple Input and Output Channels\n\nUngraded lab - Activation Functions and Max Polling\n9.3Multiple Channel Convolution.ipynb\n\n\n\nConvolutional Neural Network\n\nusing nn.Module\nclass CNN(nn.Module):\n    def __init__(self,out_1=2,out_2=1):\n        \n        super(CNN,self).__init__()\n        #first Convolutional layers \n        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n\n        #second Convolutional layers\n        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n        #max pooling \n\n        #fully connected layer \n        self.fc1=nn.Linear(out_2*7*7,2)\n        \n    def forward(self,x):\n        #first Convolutional layers\n        x=self.cnn1(x)\n        #activation function \n        x=torch.relu(x)\n        #max pooling \n        x=self.maxpool1(x)\n        #first Convolutional layers\n        x=self.cnn2(x)\n        #activation function\n        x=torch.relu(x)\n        #max pooling\n        x=self.maxpool2(x)\n        #flatten output \n        x=x.view(x.size(0),-1)\n        #fully connected layer\n        x=self.fc1(x)\n        return x\n\n\ntraining\nn_epochs=10\ncost_list=[]\naccuracy_list=[]\nN_test=len(validation_dataset)\ncost=0\n#n_epochs\nfor epoch in range(n_epochs):\n    cost=0    \n    for x, y in train_loader:\n        #clear gradient \n        optimizer.zero_grad()\n        #make a prediction \n        z=model(x)\n        # calculate loss \n        loss=criterion(z,y)\n        # calculate gradients of parameters \n        loss.backward()\n        # update parameters \n        optimizer.step()\n        cost+=loss.item()\n    cost_list.append(cost)\n    correct=0\n    #perform a prediction on the validation  data  \n    for x_test, y_test in validation_loader:\n        z=model(x_test)\n        _,yhat=torch.max(z.data,1)\n        correct+=(yhat==y_test).sum().item()\n    accuracy=correct/N_test\n    accuracy_list.append(accuracy)\n\n\nUngraded lab - Convolutional Neural Network Simple example\n9.4.1ConvolutionalNeralNetworkSimple example.ipynb\ndef conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n    #by Duane Nielsen\n    from math import floor\n    if type(kernel_size) is not tuple:\n        kernel_size = (kernel_size, kernel_size)\n    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n    return h, w\nout=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out)\nout1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out1)\nout2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out2)\n\nout3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\nprint(out3)\n>> (10, 10)\n(9, 9)\n(8, 8)\n(7, 7)\n\n\nUngraded lab - Convolutional Neural Network MNIST\n9.4.2CNN_Small_Image.ipynbb\n\n\nUngraded lab - Convolutional Neural Networks with Batch Norm\n9.4.3CNN_Small_Image_batch.ipynb\n\n\nGPU in PyTorch\ntorch.cuda.is_available()\n>> True\n\ndevice = torch.device('cuda:0')\n\ntorch.tensor([1,2,32,4]).to(device)\n>> tensor([ 1,  2, 32,  4], device='cuda:0')\n\nmodel = CNN()\nmodel.to(device)\n\n\nTraining on GPU\nfor epoch in range(num_epochs):\n    for features, labels in train_loader:\n        features, labels = features.to(device), labels.to(device)\n        optimizer.zero_grad()\n        predictions = model(features)\n        loss = criterion(predictions, labels)\n        loss.backward()\n        optimizer.step()\n\n\n\nTORCH-VISION MODELS\nload resnet18 with pretrained parameters\nimport torch\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nmodel = models.resnet18(pretrained=True)\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ncomposed = transforms.Compose([transforms.Resize(224),\n                              transforms.ToTensor(),\n                              transforms.Normalize(mean, std)])\n\ntrain_dataset = Dataset(transform=composed, train = True)\nvalidation_dataset = Dataset(transform=composed)\nfreeze parameters and add a final layer to be trained\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.fc = nn.Linear(512, 7)\ntrain_loader = DataLoader(dataset=train_loader, batch_size=15)\nvalidation_loader = DataLoader(dataset=validation_loader, batch_size=10)\nprovides only parameters to be trained to optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam([parameters for parameters in model.parameters() if parameters.requires_grad], lr = 0.003)\n\nN_EPOCHS = 20\nloss_list = []\naccuracy_list = []\ncorrect = 0\nn_test = len(validation_dataset)\ntrain the model, switching to model.train and model.eval\nfor epoch in range(N_EPOCHS):\n    loss_sublist = []\n    for x, y in train_loader:\n        model.train()\n        optimizer.zero_grad()\n        z = model(x)\n        loss = criterion(z, y)\n        loss_sublist.append(loss.data.item())\n        loss.backward()\n        optimizer.step()\n    loss_list.append(np.mean(loss_sublist))\n    correct = 0\n    for x_test, y_test in validation_loader:\n        model.eval()\n        z = model(x_test)\n        _, yhat = torch.max(z.data, 1)\n        correct += (yhat == y_test).sum().item()\n    accuracy = correct / n_test\n    accuracy_list.append(accuracy)\n\n\n\nWeek 7 - Fashion MNIST\n\nLearning Objectives\n\nApply all you have learned to train a Convolutional Neural Network\n\n\n\nnotebook\nnotebook"
  },
  {
    "objectID": "posts/2021-07-09-git-ignore-large-files.html",
    "href": "posts/2021-07-09-git-ignore-large-files.html",
    "title": "git ignore large files",
    "section": "",
    "text": "$ ll .gitignore* update_git_ignore.sh\n .gitignore\n .gitignore_bigfiles\n .gitignore_static\n update_git_ignore.sh\n\n\nHere is my standard entries for .gitignore\n$ cat .gitignore_static\n*.history\n*/.ipynb_checkpoints/*\n.ipynb_checkpoints/*\nmlflow/*\nmlruns/*\n\n\n\nThose are filed created by update_git_ignore.sh\n\n\nadd all files > 100MB in .gitignore_bigfiles\nmerge .gitignore_static and .gitignore_bigfiles as .gitignore\ndisplay .gitignore\n$ cat update_git_ignore.sh\n#!/bin/bash\n\n#update gitignore_bigfiles\nfind . -size +100M -not -path \"./.git*\"| sed 's|^\\./||g' | cat > .gitignore_bigfiles\n\n# create gitignore as concat of gitingore_static and gitignore_bigfiles\ncat .gitignore_static .gitignore_bigfiles > .gitignore\n\n# print content of .gitignore_bigfiles\ncat .gitignore_bigfiles"
  },
  {
    "objectID": "posts/2021-07-09-git-ignore-large-files.html#usage",
    "href": "posts/2021-07-09-git-ignore-large-files.html#usage",
    "title": "git ignore large files",
    "section": "Usage",
    "text": "Usage\nLaunch ./update_git_ignore.shbefore adding files to git\n$ ./update_git_ignore.sh\nmlflow/1/5699a81e1a6a44ef8afecd98fff987fc/artifacts/Data Profile.html\n$ git add .\n$ git commit -m 'example without large files'"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html",
    "href": "posts/2021-07-29-git-clean-large-files.html",
    "title": "git clean repo with bfg",
    "section": "",
    "text": "bfg website"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html#bfg-installation-from-scratch",
    "href": "posts/2021-07-29-git-clean-large-files.html#bfg-installation-from-scratch",
    "title": "git clean repo with bfg",
    "section": "bfg installation from scratch",
    "text": "bfg installation from scratch\ninstall java8\nsudo apt install openjdk-8-jre-headless\njava -version\n>> openjdk version \"1.8.0_312\"\n>> OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~20.04-b07)\n>> OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\ndownload bfg.jar\ncd ~\nmkdir -p Applications/bfg\ncd Applications/bfg\n# link from https://rtyley.github.io/bfg-repo-cleaner/\nwget https://repo1.maven.org/maven2/com/madgag/bfg/1.14.0/bfg-1.14.0.jar\nand add an alias to .bashrc\n$ grep -n -s bfg .*\n.bashrc:95:alias bfg='java -jar ~/Applications/bfg/bfg-1.14.0.jar'\n$ source .bashrc"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html#general-usage",
    "href": "posts/2021-07-29-git-clean-large-files.html#general-usage",
    "title": "git clean repo with bfg",
    "section": "General usage",
    "text": "General usage\nWe will fix ~/git/d059-vld-ic"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html#remove-big-files",
    "href": "posts/2021-07-29-git-clean-large-files.html#remove-big-files",
    "title": "git clean repo with bfg",
    "section": "Remove big files",
    "text": "Remove big files\nno need to create a clone, we can directly work on our repo\nbfg --strip-blobs-bigger-than 100M ~/git/d059-vld-ic\ncd ~/git/d059-vld-ic\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive\nNote: if you get a message Warning : no large blobs matching criteria found in packfiles - does the repo need to be packed?, you have to launch git gc"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html#remove-big-files-from-protected-commits",
    "href": "posts/2021-07-29-git-clean-large-files.html#remove-big-files-from-protected-commits",
    "title": "git clean repo with bfg",
    "section": "Remove big files from protected commits",
    "text": "Remove big files from protected commits\nProtected commits\n-----------------\n\nThese are your protected commits, and so their contents will NOT be altered:\n\n * commit d914f24e (protected by 'HEAD')\nIn that case it is even easier, no need of bfg:\ngit rm --cached <my large file>\ngit commit --amend -C HEAD"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html#remove-forbidden-files-such-as-.mp3-.tar.gz",
    "href": "posts/2021-07-29-git-clean-large-files.html#remove-forbidden-files-such-as-.mp3-.tar.gz",
    "title": "git clean repo with bfg",
    "section": "Remove forbidden files such as .mp3, .tar.gz",
    "text": "Remove forbidden files such as .mp3, .tar.gz\nneed to create a clone, we can directly work on our repo\n$ cd ~/Applications/bfg\njava -jar bfg-1.13.0.jar --delete-files '*.mp3' --no-blob-protection ~/git/data-scientist-skills\njava -jar bfg-1.13.0.jar --delete-files '*.tar.gz' --no-blob-protection ~/git/data-scientist-skills\n\ngit reflog expire --expire=now --all && git gc --prune=now --aggressive"
  },
  {
    "objectID": "posts/2021-07-29-git-clean-large-files.html#improve-.gitignore",
    "href": "posts/2021-07-29-git-clean-large-files.html#improve-.gitignore",
    "title": "git clean repo with bfg",
    "section": "Improve .gitignore",
    "text": "Improve .gitignore\nsee git ignore large files"
  },
  {
    "objectID": "posts/2021-09-01-logbook-September.html",
    "href": "posts/2021-09-01-logbook-September.html",
    "title": "Logbook for September 21",
    "section": "",
    "text": "Thursday 9/2\nPaper reviewed on arxiv about Continuous Control With Deep Reinforcement Learning. (Lillicrap et. al - 2015) arXiv:1509.02971. This is about DDPG. Initial paper comes from David Silver: Deterministic policy gradient algorithms in ICML 2014, but is not easy to read. Here is a review from towardsdatascience, in which the Deep Deterministic Policy Gradients (DDPG) is presented, and is written for people who wish to understand the DDPG algorithm."
  },
  {
    "objectID": "posts/2021-09-01-logbook-September.html#week-36---september-21",
    "href": "posts/2021-09-01-logbook-September.html#week-36---september-21",
    "title": "Logbook for September 21",
    "section": "Week 36 - September 21",
    "text": "Week 36 - September 21\nMonday 9/6\nInstall of barrier to share keyboard/mouse between linux and windows. Nice combinaison with KVM usb switch.\nMove wsl to another drive with move-wsl\nWednesday 9/8\nCreation of custom gym environment and optimization using DQN, then DDPG with stable baselines 3. Takes around 50,000 steps to optimize a ultra simple grid problem… No success with DDPG, something missing?\nThursday 9/9\nStill playing with gym and stable baselines 3. A2C, PPO and SAC are working but DDPG and TD3 are not (and I don’t know why)"
  },
  {
    "objectID": "posts/2021-09-01-logbook-September.html#week-38---september-21",
    "href": "posts/2021-09-01-logbook-September.html#week-38---september-21",
    "title": "Logbook for September 21",
    "section": "Week 38 - September 21",
    "text": "Week 38 - September 21\nMonday 9/20\nBack to Aniti RL virtual school. Looking for material to be used to explain RL to my colleagues, and how to properly describe the experience I am running with gym.\nCertainly will start lectures from deepming: 2021 DeepMind x UCL RL Lecture Series\nThursday 9/23\nStart plotly course from datacamp using my datacamp learning process. I need basic interactivity and 3d plots to illustrate reward functions."
  },
  {
    "objectID": "posts/2021-09-16-Fix-non-unique-cell-issue-in-Jupyter-Notebook.html",
    "href": "posts/2021-09-16-Fix-non-unique-cell-issue-in-Jupyter-Notebook.html",
    "title": "Fix non-unique cell issue in Jupyter Notebook",
    "section": "",
    "text": "import nbformat as nbf\nfrom glob import glob\n\nimport uuid\ndef get_cell_id(id_length=8):\n    return uuid.uuid4().hex[:id_length]\n\n# your notebook name/keyword\nnb_name = '04 - data analysis from dataprophet - W34.ipynb'\nnotebooks = list(filter(lambda x: nb_name in x, glob(\"./*.ipynb\", recursive=True)))\n\n# iterate over notebooks\nfor ipath in sorted(notebooks):\n    # load notebook\n    ntbk = nbf.read(ipath, nbf.NO_CONVERT)\n    \n    cell_ids = []\n    for cell in ntbk.cells:\n        cell_ids.append(cell['id'])\n\n    # reset cell ids if there are duplicates\n    if not len(cell_ids) == len(set(cell_ids)): \n        for cell in ntbk.cells:\n            cell['id'] = get_cell_id()\n\n    nbf.write(ntbk, ipath)"
  },
  {
    "objectID": "posts/2021-09-28-Fix-pythoncom37.dll-Jupyter-Notebook.html",
    "href": "posts/2021-09-28-Fix-pythoncom37.dll-Jupyter-Notebook.html",
    "title": "Fix pythoncom37.dll popup when launching Jupyter Notebook",
    "section": "",
    "text": "https://i.stack.imgur.com/2tQoH.png\n\n\nin my case it refers to stablebaselines3 conda environment.\nI just rename pythoncom37.dllto pythoncom37.dll.old in"
  },
  {
    "objectID": "posts/2021-09-29-nbdev-notebook2script.html",
    "href": "posts/2021-09-29-nbdev-notebook2script.html",
    "title": "Generate python modules from jupyter notebooks",
    "section": "",
    "text": "I have been using this for more than a year and I have just realized I don’t have any blog entry about it?"
  },
  {
    "objectID": "posts/2021-09-29-nbdev-notebook2script.html#mark-cells-to-be-exported-in-your-notebook",
    "href": "posts/2021-09-29-nbdev-notebook2script.html#mark-cells-to-be-exported-in-your-notebook",
    "title": "Generate python modules from jupyter notebooks",
    "section": "Mark cells to be exported in your notebook",
    "text": "Mark cells to be exported in your notebook\nnotebook2scriptexpects a keyword at the top of each cell to be exported. This keyword is #export.\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ ExecuteTime=‘{“end_time”:“2021-09-29T06:44:34.555874Z”,“start_time”:“2021-09-29T06:44:34.552776Z”}’ execution_count=1}\nvariable = 'This will be exported in a module'\n:::\n::: {.cell 0=‘e’ 1=‘x’ 2=‘p’ 3=‘o’ 4=‘r’ 5=‘t’ ExecuteTime=‘{“end_time”:“2021-09-29T06:44:54.039338Z”,“start_time”:“2021-09-29T06:44:54.036767Z”}’ execution_count=2}\nvariable2 = 'This one as well'\n:::\n\nvariable3 = 'Not this one'\n\nYou got the idea"
  },
  {
    "objectID": "posts/2021-09-29-nbdev-notebook2script.html#export-your-module-my_great_module",
    "href": "posts/2021-09-29-nbdev-notebook2script.html#export-your-module-my_great_module",
    "title": "Generate python modules from jupyter notebooks",
    "section": "Export your module my_great_module",
    "text": "Export your module my_great_module\n\n#generate py from ipynb\n#code from Jeremy Howard (fastai v2)\n#!python notebook2script.py \"00D059_init_and_import.ipynb\"\n!python notebook2script.py --fnameout=\"my_great_module.py\"  \"2021-09-29-nbdev-notebook2script.ipynb\"\n\nConverted 2021-09-29-nbdev-notebook2script.ipynb to exp/my_great_module.py"
  },
  {
    "objectID": "posts/2021-09-29-nbdev-notebook2script.html#exported-module",
    "href": "posts/2021-09-29-nbdev-notebook2script.html#exported-module",
    "title": "Generate python modules from jupyter notebooks",
    "section": "Exported module",
    "text": "Exported module\nIf subfolder exp doesn’t exist, it will be automatically created.\nAnd my_great_module.py is being created as well.\n\n\n\nexported_module.jpg\n\n\nHere is the content generated.\n\n!cat exp/my_great_module.py\n\n\n#################################################\n### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###\n#################################################\n# file to edit: 2021-09-29-nbdev-notebook2script.ipynb\n\n\nvariable = 'This will be exported in a module'\n\n\nvariable2 = 'This one as well'"
  },
  {
    "objectID": "posts/2021-10-01-logbook-October.html",
    "href": "posts/2021-10-01-logbook-October.html",
    "title": "Logbook for October 21",
    "section": "",
    "text": "Monday 10/4\nJupyter Lab is now packaged as a desktop app. Gave a try 2 minutes but issue with my running environment\nTo do: read and handson Understanding Variational Autoencoders (VAEs)\nTo do: reand and handson Patsy: Build Powerful Features with Arbitrary Python Code\nThursday 10/7\n\nDeep learning seems unstoppable! I’m particularly impressed by the recent progress of deep learning on tabular data. This new survey paper provides an overview of the SOTA deep learning methods on tabular data. A great read for students and practitioners.\nDeep Neural Networks and Tabular Data: A Survey - arxiv 2110.01889"
  },
  {
    "objectID": "posts/2021-10-01-logbook-October.html#week-42---october-21",
    "href": "posts/2021-10-01-logbook-October.html#week-42---october-21",
    "title": "Logbook for October 21",
    "section": "Week 42 - October 21",
    "text": "Week 42 - October 21\nWednesday 10/20\nUsing fastai forums to get inspirational content for VAE with tabular data. This one sounds good: Adversarial Autoencoders (with Pytorch). And this talk demystifying bayesian stuff.\n\n“Most of human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake [base], supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don’t know how to make the cake.”\nYann LeCunn\n\nAnd Intuitively Understanding Variational Autoencoders mentioned by Jeremy Howard"
  },
  {
    "objectID": "posts/2021-10-21-Just-some-usefull-keyboard-shortcuts.html",
    "href": "posts/2021-10-21-Just-some-usefull-keyboard-shortcuts.html",
    "title": "Just some usefull keyboard shortcuts",
    "section": "",
    "text": "I cannot believe I have been using linux for more than 20 years as my main system and have never configured keyboard shortcuts to launch explorer files."
  },
  {
    "objectID": "posts/2021-11-01-logbook-November.html",
    "href": "posts/2021-11-01-logbook-November.html",
    "title": "Logbook for November 21",
    "section": "",
    "text": "Monday 11/22\nback after a nice vacation break + some catchup to do for work.\nto use iphone as a webcam on linux or windows: https://www.iriun.com/ (but not detected as a webcam in linux)\nWednesday 11/24\ngetting this message when pushing to gitlab: remote: GitLab: File  is larger than the allowed size of 100 MB\nif from the most recent commit:\ngit rm --cached <my large file>\ngit commit --amend -C HEAD\notherwise follow Tutorial: Removing Large Files from Git on medium (or git clean repo with BFG on this blog)\nThursday 11/25\nwhen exporting notebooks with plotly graphs, it can help to use\nimport plotly\nplotly.offline.init_notebook_mode()\nexporting to html will integrate these plotly graphs (but not exporting to pdf)"
  },
  {
    "objectID": "posts/2021-12-01-logbook-December.html",
    "href": "posts/2021-12-01-logbook-December.html",
    "title": "Logbook for December 21",
    "section": "",
    "text": "Thursday 12/9\nA deep learning course using PyTorch including Transformers, Generative models and self-supervised learning: https://deeplearning.neuromatch.io/tutorials/intro.html - W1D1 - Gradient Descent and AutoGrad done\nNeurIPS workshop on RL next Monday. https://sites.google.com/view/deep-rl-workshop-neurips2021. List of posters is just impressive.\nAnd best fastai sources compiled by Tanishq Mathew Abraham.\nFrom @paperswithcode, Deep learning models for tabular data continue to improve. What are the latest methods and recent progress? https://twitter.com/paperswithcode/status/1458433653269205002\nFriday 12/10\nRegistration for neurips 2021 taken. Specially interested by RL workshop on Monday."
  },
  {
    "objectID": "posts/2021-12-01-logbook-December.html#week-50---december-21",
    "href": "posts/2021-12-01-logbook-December.html#week-50---december-21",
    "title": "Logbook for December 21",
    "section": "Week 50 - December 21",
    "text": "Week 50 - December 21\nMonday 12/13\nNeurIPS tutorial: Real-Time Optimization for Fast and Complex Control Systems"
  },
  {
    "objectID": "posts/2022-01-01-logbook-January-22.html",
    "href": "posts/2022-01-01-logbook-January-22.html",
    "title": "Logbook for January 22",
    "section": "",
    "text": "Monday 1/3\nWill try to use Zotero for managing research papers. Can sync between PC. Seems helpful. My lib\nTuesday 1/4\nGit revert a file to a previous commit\ngit log 00\\ -\\ my_lib.ipynb\ngit checkout f97406b026bfdf529d2dc4de96224bdfbaa576a8 00\\ -\\ my_lib.ipynb"
  },
  {
    "objectID": "posts/2022-01-01-logbook-January-22.html#week-2---january-22",
    "href": "posts/2022-01-01-logbook-January-22.html#week-2---january-22",
    "title": "Logbook for January 22",
    "section": "Week 2 - January 22",
    "text": "Week 2 - January 22\nMonday 1/17\nTo update fastai from an existing envt under windows\nconda update -n base -c defaults conda (from base)\nconda update fastai -c fastai -c pytorch -c conda-forge -c nvidia (from fastai)\nTo install mamba under WSL2\nconda install mamba -n base -c conda-forge (from base)\n\nthen\n\nmamba init\nTo use system CA certificate in WSL2\nexport REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\nTo install fastai in WSL2 using mamba\nmamba install -c fastchan fastai\nThursday 1/20\nStephane Mallat - collège de France - Information et complexité but unfortunately video is not yet available.\nRe-read of arXiv:2110.01889 Deep Neural Networks and Tabular Data: A Survey (here on zotero)"
  },
  {
    "objectID": "posts/2022-01-01-logbook-January-22.html#week-3---january-22",
    "href": "posts/2022-01-01-logbook-January-22.html#week-3---january-22",
    "title": "Logbook for January 22",
    "section": "Week 3 - January 22",
    "text": "Week 3 - January 22\nTuesday 1/26\nVideo of 1st lecture of Stephane Mallat 2022 is now available."
  },
  {
    "objectID": "posts/2022-01-18-wsl2 conda mamba cuda.html",
    "href": "posts/2022-01-18-wsl2 conda mamba cuda.html",
    "title": "setup wsl2 conda mamba and cuda",
    "section": "",
    "text": "most of it is explained in an internal blog entry.\nTo display windows version: winver.exe\nI use version 20H2 build 19042.1415\n\n\nIt is now as easy as to run wsl --install in powershell as admin.\nFull detail at MS WSL doc\nOther commands:\n# list all wsl distributions installed and their WSL version\nwsl --list --verbose\n\n# list all distributions available\nwsl --list --online\nVoici la liste des distributions valides qui peuvent être installées.\nInstaller à l’aide de « wsl --install -d <Distribution> ».\n\nNAME            FRIENDLY NAME\nUbuntu          Ubuntu\nDebian          Debian GNU/Linux\nkali-linux      Kali Linux Rolling\nopenSUSE-42     openSUSE Leap 42\nSLES-12         SUSE Linux Enterprise Server v12\nUbuntu-16.04    Ubuntu 16.04 LTS\nUbuntu-18.04    Ubuntu 18.04 LTS\nUbuntu-20.04    Ubuntu 20.04 LTS\n\n# install Linux distributions\nwsl --install -d <Distribution Name>\n\n# shutdown wsl: shutdown all \nwsl --shutdown \n\n# define default wsl distribution to use with wsl\nwsl -s <DistributionName>\n\n\n\nand here is a more advanced config to install in a non system drive (D: instead of C:)\n#from powershell\nNew-Item D:\\WSL\\Ubuntu-20.04 -ItemType Directory\nSet-Location D:\\WSL\\Ubuntu-20.04\n\n#list+link of distributions in https://docs.microsoft.com/en-us/windows/wsl/install-manual#downloading-distributions\nInvoke-WebRequest -Uri https://aka.ms/wslubuntu2004 -OutFile Ubuntu-20.04.appx -UseBasicParsing\n\nRename-Item .\\Ubuntu-20.04.appx Ubuntu-20.04.zip\nExpand-Archive .\\Ubuntu-20.04.zip -Verbose\n# and then run Ubuntu_2004.2021.825.0_x64.appx\nI don’t know yet where the WSL disk is located (is it a .vhdx file?)\nDisks are located at %USERPROFILE%\\AppData\\Local\\Packages\\[distro name]\n2 ditros used:\n\nwsl1 ubuntu 18.04: CanonicalGroupLimited.Ubuntu18.04onWindows_79rhkp1fndgsc\nwsl2 ubuntu 20.04: CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\n\nSo disks are still in C:\n\nGuess I have to use move-wsl.\nSet-Location 'D:\\Program Files (x86)\\move-wsl\\'\n.\\move-wsl.ps1\n\nPS D:\\Program Files (x86)\\move-wsl> .\\move-wsl.ps1\nGetting distros...\nSelect distro to move:\n1: Ubuntu-18.04\n2: Ubuntu\n2\nEnter WSL target directory:\nD:\\wsl\\Ubuntu-20.04\nMove Ubuntu to \"D:\\wsl\\Ubuntu-20.04\"? (Y|n): Y\nExporting VHDX to \"D:\\wsl\\Ubuntu-20.04\\Ubuntu.tar\" ...\nAnd after that, have to create file/etc/wsl.conf\nguillaume@LL11LPC0PQARQ:~$ cat /etc/wsl.conf\n[user]\ndefault=guillaume\n\n\n\n\n\nIt is nicely explained in the Michelin blog entry. DNS resolution is kind of broken (I think due to internal protections we use on our corporate PC)\nvpnkit provides a secured solution to make it work. And sakai135 has packaged it for wsl: wsl-vpnkit\nThe steps to install wsl-vpn kit are:\n\nCreate a working directory on your windows workspace and download this packaging of wsl-vpnkit inside.\nNow, open a powershell and go to the location of wsl-vpnkit.tar.gz, downloaded during the previous step\nOn your powershell terminal, launch:\n\n  #/!\\ in powserhsell\n  wsl --import wsl-vpnkit $env:USERPROFILE\\wsl-vpnkit wsl-vpnkit.tar.gz\n  wsl -d wsl-vpnkit\n\nYou can now exit your powershell\nFor the last step, to ensure all wsl reboot good communication, we will write in .profile file of your ubuntu user wsl-vpnkit initialization command:\n\n  echo 'wsl.exe -d wsl-vpnkit service wsl-vpnkit start' >> ~/.profile\n\nRelaunch your WSL terminal\n\n\n\n\ncreate a SSH key pair under your distribution\nssh-keygen -t rsa -b 4096 -C \"WSL2\"\nIntegrate into gitlab using gitlab doc. (copy id_rsa.pub into gitlab > preferences > SSH Keys)cat .s\n\n\n\nMichelin SI is behind an ssl proxy with his proper PKI for certificates delivering. That is why, your subsystem must add this pki in her recognized authorities.\nTo do this, we will clone a repository with the certificates in the subsystem and copy them to ca-certificates:\ngit clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment\nsudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/\nsudo update-ca-certificates\nIf everything is ok, terminal notify you that certificates has been added.\nYou can now clean the temp working folder:\nrm -rf /tmp/devops_environment\nwe can have similar approach to update CA certifcates for Python. 1st step is to locate cacert.pem of your active python environment.\nimport certifi\ncertifi.where() \n>> '/home/guillaume/miniconda3/envs/fastai/lib/python3.9/site-packages/certifi/cacert.pem'\nTO BE FIXED\nAfter having run update-ca-certifcates, there is an updated ca file at /etc/ssl/certs/ca-certificates.crt. Let’s concatenate it to our cacert.pem.\ncp /etc/ssl/certs/ca-certificates.crt /tmp/ca-certificates.crt\nopenssl x509 -in /tmp/ca-certificates.crt -out /tmp/ca-certificates.pem -outform PEM\ncat /tmp/ca-certificates.pem | tee -a /home/guillaume/miniconda3/envs/fastai/lib/python3.9/site-packages/certifi/cacert.pem\n\n\n\nThe last step, to have a subsystem ready to use, is to have an apt with Michelin trusted sources configured. Ubuntu based package repositories can’t be used behind Michelin proxy.\nMichelin offers its own apt server with artifactory. To configure apt to use artifactory, launch these commands:\necho 'Acquire { http::User-Agent \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7\"; };' | sudo tee /etc/apt/apt.conf.d/90globalprotectconf\nsudo sed -i 's@^\\(deb \\)http://archive.ubuntu.com/ubuntu/\\( focal\\(-updates\\)\\?.*\\)$@\\1https://artifactory.michelin.com/artifactory/ubuntu-archive-remote\\2\\n# &@' /etc/apt/sources.list\nsudo sed -i 's@^\\(deb \\)http://security.ubuntu.com/ubuntu/\\( focal\\(-updates\\)\\?.*\\)$@\\1https://artifactory.michelin.com/artifactory/ubuntu-security-remote\\2\\n# &@' /etc/apt/sources.list\n\n\n\nTo verify if everything is OK on your distribution:\n\nThis command must return google ip:\n\n  host google.fr\n\nThis command must return artifactory ip:\n\n  host artifactory.michelin.com\n\nYou are able to update your distribution without error:\n\n  sudo apt update\n  sudo apt upgrade -y"
  },
  {
    "objectID": "posts/2022-01-18-wsl2 conda mamba cuda.html#conda-mamba---installation-and-configuration",
    "href": "posts/2022-01-18-wsl2 conda mamba cuda.html#conda-mamba---installation-and-configuration",
    "title": "setup wsl2 conda mamba and cuda",
    "section": "Conda Mamba - installation and configuration",
    "text": "Conda Mamba - installation and configuration\n\nconda installation\ntmpdir=$(mktemp -d)\ncd $tmpdir\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n# answer yes to question Do you wish the installer to initialize Miniconda3 by running conda init?\nbash Miniconda3-latest-Linux-x86_64.sh -p $HOME/miniconda3\n\n>> ==> For changes to take effect, close and re-open your current shell. <==\nWith this configuration, conda will be activate at startup. If you’d prefer that conda’s base environment not be activated on startup, set the auto_activate_base parameter to false: conda config --set auto_activate_base false\n\n\nconda configuration\nAs we have in-house CA certificates, and conda uses its own CA certificates (in ~/miniconda3/ssl)\nWe have to change this behaviour and ask conda to use system CA certifcates.\nAt the end of .bash_rc, add export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt\n\n\nmamba installation\nconda install mamba -n base -c conda-forge\nmamba init\n\n\ninstallation jupyter notebook, nb_conda_kernels, jupyter lab\nmamba install nb_conda_kernels\nmamba install -c conda-forge jupyterlab jupyterlab-git\n\n\ncreate conda envt - fastai (v2.5.3) (optional)\nmamba create --name fastai\nmamba activate fastai\nmamba install -c fastai -c pytorch fastai\nmamba install ipykernel"
  },
  {
    "objectID": "posts/2022-01-18-wsl2 conda mamba cuda.html#jupyter",
    "href": "posts/2022-01-18-wsl2 conda mamba cuda.html#jupyter",
    "title": "setup wsl2 conda mamba and cuda",
    "section": "Jupyter",
    "text": "Jupyter\nStart jupyter (lab or notebook) from base environment, and switch to desired python environment.\n\nmodify jupyter config\n#create jupyter config file in ~.jupyter\njupyter notebook --generate-config\nAnd activate jupyter config file to change #c.NotebookApp.use_redirect_file = True to c.NotebookApp.use_redirect_file = False"
  },
  {
    "objectID": "posts/2022-02-01-logbook-February-22.html",
    "href": "posts/2022-02-01-logbook-February-22.html",
    "title": "Logbook for February 22",
    "section": "",
    "text": "Thursday 2/3\nStephane Mallat - collège de France - Information et complexité video n°2: Estimation par maximum de vraisemblance"
  },
  {
    "objectID": "posts/2022-02-01-logbook-February-22.html#week-8---february-22",
    "href": "posts/2022-02-01-logbook-February-22.html#week-8---february-22",
    "title": "Logbook for February 22",
    "section": "Week 8 - February 22",
    "text": "Week 8 - February 22\nMonday 2/21\nStephane Mallat - collège de France - Information et complexité video n°3: Optimisation et modèles exponentiels\nTuesday 2/22\nAntonin Raffin (Stable Baselines 3 author) explains how to better evaluate RL agents using Rliable. Would like to test that."
  },
  {
    "objectID": "posts/2022-04-07-keep dotfiles in git.html",
    "href": "posts/2022-04-07-keep dotfiles in git.html",
    "title": "keep dotfiles in git",
    "section": "",
    "text": "as pointed by Jeremy Howard."
  },
  {
    "objectID": "posts/2022-04-07-keep dotfiles in git.html#how-to-setup-it",
    "href": "posts/2022-04-07-keep dotfiles in git.html#how-to-setup-it",
    "title": "keep dotfiles in git",
    "section": "How to setup it",
    "text": "How to setup it\n\nprerequisites\nI consider I already have a git repo with my dotfiles from other machines.\nRepo: git@<your_gitlab_address>:<your_id>/dotfiles.git\nI keep one separate branch per machine. Current branches: master (empty), and WSL2.\nI am going to add a machine called iolab.\n\n\nfrom iolab\ngit init --bare $HOME/.cfg\nalias config='/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME'\nconfig config --local status.showUntrackedFiles no\necho \"alias config='/usr/bin/git --git-dir=$HOME/.cfg/ --work-tree=$HOME'\" >> $HOME/.bash_aliases\nAnd we can now run config status\n(base) [ 09:53:56 ][ id: ~ ]$ config status\n# On branch master\n#\n# Initial commit\n#\nnothing to commit (create/copy files and use \"git add\" to track)\nbut now we would like to create a new branch, and push all this to our central repo.\nFirst we have to set this central repo.\nconfig remote add origin git@<your_gitlab_address>:<your_id>/dotfiles.git\nconfig fetch\nBefore creating our branch, we have to commit something (to really create our local branch master)\nconfig add .bashrc\nconfig commit -m 'init with .bashrc'\nAnd then only we can create our branch iolab\nconfig branch iolab\nconfig checkout iolab\nconfig push --set-upstream origin iolab\nwe are now ready to use it"
  },
  {
    "objectID": "posts/2022-04-07-keep dotfiles in git.html#how-to-use-it",
    "href": "posts/2022-04-07-keep dotfiles in git.html#how-to-use-it",
    "title": "keep dotfiles in git",
    "section": "How to use it",
    "text": "How to use it\nconfig add .bash_aliases\nconfig commit -m'bash aliases'\nconfig push"
  },
  {
    "objectID": "posts/2022-04-07-keep dotfiles in git.html#how-to-setup-2-remote-repo",
    "href": "posts/2022-04-07-keep dotfiles in git.html#how-to-setup-2-remote-repo",
    "title": "keep dotfiles in git",
    "section": "How to setup 2 remote repo",
    "text": "How to setup 2 remote repo\nThere is a nice explanation abut how to work with multiple repos in https://jigarius.com/blog/multiple-git-remote-repositories.\nTo follow that, I will configure my dotfile repo from WSL2 to push to 2 remotes, one on gitlab (internal) and one on github.\nFor the moment it is only connected to gitlab.\n$ config remote -v\norigin  git@gitlab.michelin.com:janus/dotfiles.git (fetch)\norigin  git@gitlab.michelin.com:janus/dotfiles.git (push)\nMy github repo is at: https://github.com/castorfou/dotfiles.git (I use https, because of my local firewall)\n$ config remote set-url --add --push origin git@gitlab.michelin.com:janus/dotfiles.git\n$ config remote set-url --add --push origin https://github.com/castorfou/dotfiles.git\n$ config push origin GR_WSL2\nI have to get a token from github to access in https\nTo generate a token:\n\nLog into GitHub\nClick on your name / Avatar in the upper right corner and select Settings\nOn the left, click Developer settings\nSelect Personal access tokens and click Generate new token\nGive the token a description/name and select the scope of the token\n\nI selected repo only to facilitate pull, push, clone, and commit actions\nClick the link Read more about OAuth scopes for details about the permission sets\n\nClick Generate token\nCopy the token – this is your new password!\n\nLastly, to ensure the local computer remembers the token, we can enable caching of the credentials. This configures the computer to remember the complex token so that we dont have too.\ngit config --global credential.helper cache"
  },
  {
    "objectID": "posts/2022-04-25-install ubuntu 22.04 on WSL.html",
    "href": "posts/2022-04-25-install ubuntu 22.04 on WSL.html",
    "title": "install ubuntu 22.04 on WSL",
    "section": "",
    "text": "How to install Ubuntu 21.10 on WSL for Windows 10 and 11"
  },
  {
    "objectID": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#installation",
    "href": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#installation",
    "title": "install ubuntu 22.04 on WSL",
    "section": "Installation",
    "text": "Installation\n\nuninstall image (if needed)\n# wsl --unregister <distroName>\nwsl --unregister ubuntu-22.04\n\n\ndownload images\nFrom cloud images ubuntu (cloud-images > jammy > current), now there are wsl images:\n\n\n\nubuntu cloud images\n\n\nI just have to download the last jammy (22.04) image jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz\n\n\ninstall and setup from powershell\nI have downloaded this ubuntu image to D:\\wsl\\ubuntu-22.04\\download\n(base) guillaume@LL11LPC0PQARQ:/mnt/d/wsl$ tree\n.\n├── Ubuntu-20.04\n│   └── ext4.vhdx\n├── Ubuntu-22.04\n│   ├── download\n│   │   └── jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz\n│   └── instance\nand my ubuntu-22.04 instance will stand in D:\\wsl\\ubuntu-22.04\\instance\nInstall with this command from powershell\n# wsl --import <distroname> <location of instance> <location of download>\nwsl --import ubuntu-22.04 D:\\wsl\\ubuntu-22.04\\instance D:\\wsl\\ubuntu-22.04\\download\\jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz\nIt takes 3-4 minutes to install. and should be visible in your wsl instances.\n wsl --list --all -v\n  NAME            STATE           VERSION\n  ubuntu-22.04    Stopped         2\nthen to run it\n# wsl -d <distroname>\nwsl -d ubuntu-22.04\nor\n\nuse Windows Terminal as a launcher\nWindows Terminal is a smart way to group all terminals (powershell, and all your wsl instances)\n\n\n\nwindows terminal\n\n\nIt can be installed even with limited windows store access by clicking install in Installer le Terminal Windows et commencer à le configurer\nAutomatically all wsl instances appear in Settings."
  },
  {
    "objectID": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#automatic-setup",
    "href": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#automatic-setup",
    "title": "install ubuntu 22.04 on WSL",
    "section": "Automatic setup",
    "text": "Automatic setup\ncopy these 2 scripts in /root/ (given they are in D:\\wsl\\ubuntu-22.04\\download)\ncp /mnt/d/wsl/Ubuntu-22.04/download/setup_wsl_* .\nsetup_wsl_root.sh download\n#!/bin/bash\n\necho \"0. get username: \"\nread user_name\n\n. /etc/lsb-release\n\necho Configuration for user [$user_name]\necho of distribution $DISTRIB_CODENAME\necho\n\necho \"1. create user and add in sudo\"\n#adduser --disabled-password --gecos \"\" $user_name\nadduser --gecos \"\" $user_name\nusermod -aG sudo $user_name\necho\n\necho \"2. create wsl.conf file\"\nrm -rf /etc/wsl.conf\ntee /etc/wsl.conf << EOF\n# Set the user when launching a distribution with WSL.\n[user]\ndefault=$user_name\nEOF\necho\n\necho \"3. prepare setup by user\"\ncp setup_wsl_user.sh /home/$user_name\nchown $user_name:users /home/$user_name/setup_wsl_user.sh\nchmod 750  /home/$user_name/setup_wsl_user.sh\ntee -a /home/$user_name/.bashrc << EOF\nif [ ! -e \".wsl_configured\" ]; then\n        ./setup_wsl_user.sh\n        touch .wsl_configured\nfi\nEOF\necho\n\necho \"end of configuration for root\"\necho \"stop wsl instance by running 'wsl -t <distro-name>' from powershell\"\necho \"and start from Windows Terminal\"\nsetup_wsl_user.sh download\n#!/bin/bash\n\necho \"1. setup wsl-vpnkit\"\nif grep -Fxq \"wsl-vpnkit\" ~/.profile\nthen\n    # code if found\n    echo \"   wsl-vpnkit already setup\"\nelse\n    # code if not found\n    echo 'wsl.exe -d wsl-vpnkit service wsl-vpnkit start' >> ~/.profile\nfi\nwsl.exe -d wsl-vpnkit service wsl-vpnkit start\nsource ./.bashrc\necho\n\necho \"2. create ssh key to copy to gitlab\"\n. /etc/lsb-release\nif [ ! -e \".ssh/id_rsa.pub\" ]; then\n        ssh-keygen -t rsa -b 4096 -C \"WSL2 ubuntu $DISTRIB_RELEASE\"\n        cat .ssh/id_rsa.pub\n        echo \"copy this content to gitlab > preferences > SSH Keys\"\n        read -p \"Press any key to resume ...\"\nfi\necho\n\necho \"3. update certificates\"\ngit clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment\nsudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/\nsudo update-ca-certificates\nrm -rf /tmp/devops_environment\nif [ $DISTRIB_RELEASE == \"22.04\" ]\nthen\necho 'bug SSL with ubuntu 22.04 - https://bugs.launchpad.net/ubuntu/+source/openssl/+bug/1963834/comments/7'\nsudo tee -a /etc/ssl/openssl.cnf << EOF\n[openssl_init]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nOptions = UnsafeLegacyRenegotiation\nEOF\nfi\necho\n\necho \"4. update apt sources with artifactory\"\necho 'Acquire { http::User-Agent \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7\"; };' | sudo tee /etc/apt/apt.conf.d/90globalprotectconf\nsudo sed -i 's,http://archive.ubuntu.com/ubuntu,https://artifactory.michelin.com/artifactory/ubuntu-archive-remote,g' /etc/apt/sources.list\nsudo sed -i 's,http://security.ubuntu.com/ubuntu,https://artifactory.michelin.com/artifactory/ubuntu-archive-remote,g' /etc/apt/sources.list\nsudo apt update\nsudo apt upgrade -y\necho\nThen\nchmod +x setup_wsl_root.sh\n./setup_wsl_root.sh\nAs explained stop wsl instance by running wsl --shutdown ubuntu-22.04 from powershell and start from Windows Terminal\nIt restarts from your user and it will install:\n\nsetup wsl-vpnkit\ncreate ssh key to copy to gitlab\nupdate certificates\nupdate apt sources with artifactory"
  },
  {
    "objectID": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#and-now-we-can-install-other-parts",
    "href": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#and-now-we-can-install-other-parts",
    "title": "install ubuntu 22.04 on WSL",
    "section": "And now we can install other parts",
    "text": "And now we can install other parts\nall the scripts are in https://github.com/castorfou/guillaume_blog/tree/master/files\n\n00 - keep config files in git\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_00_config_files_in_git.sh | bash\nsource .bashrc\n\n\n01 - automount secured vbox\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_01_automount_secured_vbox.sh | bash\n\n\n02 - python with conda and configure base environment (jupyterlab, mamba)\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_02_install_python_conda_part1.sh | bash\ncd \nsource .bashrc\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_02_install_python_conda_part2.sh | bash\nsource .bashrc\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_02_install_python_conda_part3.sh | bash\nsource .bashrc\n\n\n03 - bat cat\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_03_install_batcat.sh | bash\nsource .bashrc\n\n\n04 - git access\nln -s /mnt/d/git/ ~/\n\n\n05 - X access with GWSL\nGWSL Homepage\nif you have access to Windows Store, it is available.\nOr alternate download are possible.\n\n\n\n06 - git credential manager\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_06_git_credential_manager.sh | bash\n\n\n07 - install wslu\nwslu\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_07_wslu.sh | bash\nSome examples:\n$ wslfetch\n\n               .-/+oossssoo+/-.               Windows Subsystem for Linux (WSL2)\n           `:+ssssssssssssssssss+:`           guillaume@LL11LPC0PQARQ\n         -+ssssssssssssssssssyyssss+-         Build: 19044\n       .ossssssssssssssssssdMMMNysssso.       Branch: vb_release\n      /ssssssssssshdmmNNmmyNMMMMhssssss/      Release: Ubuntu 22.04 LTS\n     +ssssssssshmydMMMMMMMNddddyssssssss+     Kernel: Linux 5.10.102.1-microsoft-standard-WSL2\n    /sssssssshNMMMyhhyyyyhmNMMMNhssssssss/    Uptime: 0d 3h 44m\n   .ssssssssdMMMNhsssssssssshNMMMdssssssss.\n   +sssshhhyNMMNyssssssssssssyNMMMysssssss+\n   ossyNMMMNyMMhsssssssssssssshmmmhssssssso\n   ossyNMMMNyMMhsssssssssssssshmmmhssssssso\n   +sssshhhyNMMNyssssssssssssyNMMMysssssss+\n   .ssssssssdMMMNhsssssssssshNMMMdssssssss.\n    /sssssssshNMMMyhhyyyyhdNMMMNhssssssss/\n     +sssssssssdmydMMMMMMMMddddyssssssss+\n      /ssssssssssshdmNNNNmyNMMMMhssssss/\n       .ossssssssssssssssssdMMMNysssso.\n         -+sssssssssssssssssyyyssss+-\n           `:+ssssssssssssssssss+:`\n               .-/+oossssoo+/-.\n               \n$ wslpath -u \"C:\\Program Files\\Typora\\Typora.exe\"\n/mnt/c/Program Files/Typora/Typora.exe\n\n\n08 - configure pip\nwget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_08_pip.sh | bash\n\n\n09 - install vscode\ncode .\n(given Visual Studio Code is installed on the Windows side (not in WSL))\n(and if needed install Remote development)"
  },
  {
    "objectID": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#manual-setup-skip-if-to-follow-automatic-setup",
    "href": "posts/2022-04-25-install ubuntu 22.04 on WSL.html#manual-setup-skip-if-to-follow-automatic-setup",
    "title": "install ubuntu 22.04 on WSL",
    "section": "Manual setup (skip if to follow automatic setup)",
    "text": "Manual setup (skip if to follow automatic setup)\n\nbasic setup\nWith this way to install, you don’t have any user, you don’t have any launcher within Windows.\nCreate a user and add it to sudo:\n# adduser <yourusername>\n# usermod -aG sudo <yourusername>\nadduser guillaume\nusermod -aG sudo guillaume\nand I can switch to this user simply with\n# su <yourusername>\nsu guillaume\n\n\nlaunch distro with yourusername - update wsl.conf\nManually you can now start your distro with your username from powershell\n# wsl -d <distroname> -u <yourusername>\nwsl -d ubuntu-22.04 -u guillaume\nOr from another wsl (huge avantage to run in linux terminal instead of powershell)\nwsl.exe -d ubuntu-22.04 -u guillaume\nbut you can better keep this username setting by updating wsl.conf\n# /etc/wsl.conf\n# Set the user when launching a distribution with WSL.\n[user]\ndefault=YourUserName\nIt is now setup. You can now shutdown this instance from powershell.\n# wsl --shutdown <distroname>\nwsl --shutdown ubuntu-22.04\nand when starting wsl -d ubuntu-22.04, you reach your username.\n\n\nwsl-vpnkit\nAs wsl-vpnkit is already installed, I just have to\necho 'wsl.exe -d wsl-vpnkit service wsl-vpnkit start' >> ~/.profile\nsource .bashrc\n\n\ngitlab\nssh-keygen -t rsa -b 4096 -C \"WSL2 ubuntu 22.04\"\nand copy id_rsa.pub into gitlab > preferences > SSH Keys\n\n\ncorporate CA certificates\ngit clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment\nsudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/\nsudo update-ca-certificates\nrm -rf /tmp/devops_environment\n\n\napt sources\nhad to replace focal (20.04) to jammy (22.04)\necho 'Acquire { http::User-Agent \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7\"; };' | sudo tee /etc/apt/apt.conf.d/90globalprotectconf\nsudo sed -i 's@^\\(deb \\)http://archive.ubuntu.com/ubuntu/\\( jammy\\(-updates\\)\\?.*\\)$@\\1https://artifactory.michelin.com/artifactory/ubuntu-archive-remote\\2\\n# &@' /etc/apt/sources.list\nsudo sed -i 's@^\\(deb \\)http://security.ubuntu.com/ubuntu/\\( jammy\\(-updates\\)\\?.*\\)$@\\1https://artifactory.michelin.com/artifactory/ubuntu-security-remote\\2\\n# &@' /etc/apt/sources.list\n\n\ncheck everything is ok\n\nThis command must return google ip:\n\n  host google.fr\n\nThis command must return artifactory ip:\n\n  host artifactory.michelin.com\n\nYou are able to update your distribution without error:\n\n  sudo apt update\n  sudo apt upgrade -y\n\n\nSetup config dotfiles with whole filesystem (/)\nas detailed in keep dotfiles in git\nbut to manage the whole filesystem.\n\ninit local repo\nsudo mkdir -p /.cfg\nsudo chown $USER:users /.cfg\ngit init --bare /.cfg\nalias config='/usr/bin/git --git-dir=/.cfg/ --work-tree=/'\nconfig config --local status.showUntrackedFiles no\necho \"alias config='/usr/bin/git --git-dir=/.cfg/ --work-tree=/'\" >> $HOME/.bash_aliases\ncd\nsource .bashrc\n\n\ngit default identity (if needed)\nconfig config --global user.email \"guillaume.ramelet@michelin.com\"\nconfig config --global user.name \"guillaume\"\n\n\nsetup branch and push to central repo\nconfig remote add origin git@gitlab.michelin.com:janus/dotfiles.git\nconfig fetch\ncd\nconfig add .bashrc\nconfig commit -m 'init with .bashrc'\n\nconfig branch GR_WSL2_ubuntu22.04\nconfig checkout GR_WSL2_ubuntu22.04\nconfig push --set-upstream origin GR_WSL2_ubuntu22.04"
  },
  {
    "objectID": "posts/2022-05-02-upgrade-fastpages.html",
    "href": "posts/2022-05-02-upgrade-fastpages.html",
    "title": "upgrade to last version of fastpages",
    "section": "",
    "text": "as detailed in https://github.com/fastai/fastpages/issues/634\nHamel asks to restart from a new repo. But how to keep the same blog url?\nEasy way is to rename former repo (from guillaume_blog to guillaume_blog_old) and initiate new repo as former one (guillaume_blog).\nHere are the steps."
  },
  {
    "objectID": "posts/2022-05-02-upgrade-fastpages.html#installation-and-setup",
    "href": "posts/2022-05-02-upgrade-fastpages.html#installation-and-setup",
    "title": "upgrade to last version of fastpages",
    "section": "Installation and setup",
    "text": "Installation and setup\n\nInstallation\n\nGenerate a copy of fastpages repo. Just have to follow instructions by clicking at https://github.com/fastai/fastpages/generate. Name repo as guillaume_blog\nClick on the PR Initial Setup in your new repo. There are instructions to create a SSH_DEPLOY_KEY.\nMerge this PR\nClone this repo locally\nBecause I use https, I have to create a token at Settings > Developer Settings > Personal Access Tokens\nand to keep this token locally, I enter git config --global credential.helper manager before pushing\n\n\n\nCopy content\n\nDirectories: _notebooks, _posts, _files, _images\nClean content from directories (examples) in _notebooks, _posts, _words\nPages: _pages/about.md, index.html, README.md\nand utils: refresh_blog_content.sh, publish.sh"
  },
  {
    "objectID": "posts/2022-05-03-save git https credentials under wsl.html",
    "href": "posts/2022-05-03-save git https credentials under wsl.html",
    "title": "save git https credentials under wsl",
    "section": "",
    "text": "Microsoft has released a tool to securely keep https credentials:\ngit-credential-manager\nUsefull when one has to use https instead of git(ssl) to connect to git repos. My case when I am behing my corporate firewall and has to link to github repos (such as this blog)"
  },
  {
    "objectID": "posts/2022-05-03-save git https credentials under wsl.html#how-to-setup-it",
    "href": "posts/2022-05-03-save git https credentials under wsl.html#how-to-setup-it",
    "title": "save git https credentials under wsl",
    "section": "How to setup it",
    "text": "How to setup it\n\ncreate token in github\nI have to create a token at Settings > Developer Settings > Personal Access Tokens\n\n\ninstallation of git-credential-manager inside WSL\nDownload the latest (v2.0.696 at May/3rd 2022) .deb package, and run the following:\nsudo dpkg -i <path-to-package>\ngit-credential-manager-core configure\ngit config --global credential.credentialStore gpg\nexport GPG_TTY=$(tty)\ngpg --full-generate-key\nsudo apt install -y pass\nkey_id=`gpg --list-keys | awk -F: '/^ / { print $0 }' | cut -d\" \" -f7`\npass init $key_id\nor see the step 06 in install ubuntu 22.04 on WSL"
  },
  {
    "objectID": "posts/2022-06-01-logbook-June-22.html",
    "href": "posts/2022-06-01-logbook-June-22.html",
    "title": "Logbook for June 22",
    "section": "",
    "text": "Wednesday 6/15\n\ncontinue deep rl class with unit 2, interesting notebook to learn about using optuna to tune hyperparameters\n\nFriday 6/17\n\nSylvain pointed me to a paper from X: Robust Reinforcement Learning with Distributional Risk-averse formulation now in zotero to use distribution information to strengthen RL epxloration. To be read and tested"
  },
  {
    "objectID": "posts/2022-06-15-deep-rl-class-with-huggingface.html",
    "href": "posts/2022-06-15-deep-rl-class-with-huggingface.html",
    "title": "Deep RL class - huggingface",
    "section": "",
    "text": "Thomas is now part of HuggingFace.\n1st step is to fork the repo, and for mine it is here.\nAnd clone it locally: git clone git@github.com:castorfou/deep-rl-class.git ou git clone https://github.com/castorfou/deep-rl-class.git\nI followed the 1st unit in May/11.\nthere is a community on discord at https://discord.gg/aYka4Yhff9, with a lounge about RL.\n\nUnit 1 - Introduction to Deep Reinforcement Learning\n\n📖 It starts with some general introduction to deep RL and then a quizz.\n\n\n👩‍💻 1st practice uses this lunar lander environment, and you train a PPO agent to get the highest score,\n\nand this runs on colab : https://github.com/huggingface/deep-rl-class/blob/main/unit1/unit1.ipynb (just by clicking on )\nthere is a leaderboard running under huggingface (one can publish models to huggingface) https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard . Just need an huggingface account for that (used my Michelin account)\n\nA guide has been recently added explaining how to tune hyperparameters using optuna. 👉 https://github.com/huggingface/deep-rl-class/blob/main/unit1/unit1_optuna_guide.ipynb. Should do it!\nTo start unit2. Introduction to Q-Learning\n\nfirst update from fork just by clicking\nand update your local repo (git fetch git pull)\n\n\n\n\nUnit 2 - Introduction to Q-Learning\n\n📖 part 1 - we learned about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning. Then a quizz (easy one)\n\n\n📖 part 2 - and then Q-learning which is an off-policy value-based method that uses a TD approach to train its action-value function. Then a quizz (less easier)\n\n\n👩‍💻 hands-on. 1st algo (FrozenLake) is published in Guillaume63/q-FrozenLake-v1-4x4-noSlippery. 2nd algo (Taxi) is published in Guillaume63/q-Taxi-v3. Leaderboard is here\n\n\n\nUnit 3 - Deep Q-Learning with Atari Games\n\n📖 The Deep Q-Learning chapter 👾 👉 https://huggingface.co/blog/deep-rl-dqn\n\n\n👩‍💻 Start the hands-on here 👉 https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit3/unit3.ipynb\nfrom discord, a video (30’) by Antonin Raffin about Automatic Hyperparameter Optimization @ ICRA 22 - Tools for Robotic RL 6/8. Never thought about it that way, it can help to speed training phase.\nfrom discord as well a video to build a doom ai model (3 hours!)\nand from discord a lecture from Pieter Abbeel explaining Q-value to DQN and why we have this double network at L2 Deep Q-Learning (Foundations of Deep RL Series. This is part of a larger lecture available at Foundations of Deep RL – 6-lecture series by Pieter Abbeel\nAnd then a video explaining Deep RL at the Edge of the Statistical Precipice. This was from a paper at Neurips.\n\n\n\nUnit 4 - An Introduction to Unity ML-Agents with Hugging Face 🤗\n\n📖 tutorial 👉 https://link.medium.com/KOpvPdyz4qb\nThomas starts with evolutions on RL domain, citing Decision Transformers as one of the last hot topic. And then introduces Unity and how it can now be used with RL agents.\n\n\n\nunity ML-Agents toolkit\n\n\nInteresting idea to introduce curiosity and to make it real as an intrinsic reward.\n\nNote: It guided me to gentle introductions to cross-entropy for machine learning and information entropy.\n\nLow Probability Event (surprising): More information. High entropy.\nHigher Probability Event (unsurprising): Less information. Low entropy.\nSkewed Probability Distribution (unsurprising): Low entropy.\nBalanced Probability Distribution (surprising): High entropy.\n\n$$ Information:\n\\h(x)=-(P(x)) $$\n\\[\nEntropy:\n\\\\H(X) = – \\sum_{x \\in X} P(x)  \\log(P(x))\n\\]\n\\[\nCross-Entropy:\\\\H(P, Q) = – \\sum_{x \\in X} P(x)  \\log(Q(x))\n\\]\nCross-Entropy and KL divergence are similar but not exactly the same. Specifically, the KL divergence measures a very similar quantity to cross-entropy. It measures the average number of extra bits required to represent a message with Q instead of P, not the total number of bits.\n\\[\nKL\\ Divergence\\ (relative\\ entropy):\n\\\\KL(P||Q)=– \\sum_{x \\in X} P(x)  \\frac{\\log(Q(x))}{\\log(P(x))}\n\\\\H(P, Q) = H(P) + KL(P || Q)\n\\]\n\n\n\n👩‍💻 Here are the steps for the training:\n\nclone repo and install environment\n\n# from ~/git/guillaume\ngit clone https://github.com/huggingface/ml-agents/\n# bug with python 3.9 - https://github.com/Unity-Technologies/ml-agents/issues/5689\nconda create  --name ml-agents python=3.8\nconda activate ml-agents\n# Go inside the repository and install the package \ncd ml-agents \npip install -e ./ml-agents-envs \npip install -e ./ml-agents\n\ndownload the Environment Executable (pyramids from google drive)\n\nUnzip it and place it inside the MLAgents cloned repo in a new folder called trained-envs-executables/linux\n\nmodify nbr of steps to 1000000 in config/ppo/PyramidsRND.yaml\ntrain\n\nmlagents-learn config/ppo/PyramidsRND.yaml --env=training-envs-executables/linux/Pyramids/Pyramids --run-id=\"First Training\" --no-graphics\n\nmonitor training\n\ntensorboard --logdir results --port 6006\n(auto reload is off by default this day, click settings and check Reload data) (because I have installed v2.3.0 and not 2.4.0, there is no autofit domain to data and it is annoying)\n\npush to 🤗 Hub\n\nCreate a new token (https://huggingface.co/settings/tokens) with write role\nCopy the token, Run this and past the token huggingface-cli login\nPush to Hub\nmlagents-push-to-hf --run-id='First Training' --local-dir='results/First Training' --repo-id='Guillaume63/MLAgents-Pyramids' --commit-message='Trained pyramids agent upload'\nand now I can play it from https://huggingface.co/Guillaume63/MLAgents-Pyramids and watch your Agent play…\n\n\n\nUnit 5 - Policy Gradient with PyTorch\n\n1️⃣ 📖 Read Policy Gradient with PyTorch Chapter.\nAdvantage and disadvantage of policy gradient vs DQN.\nReinforce algorithm (Monte Carlo policy gradient): it uses an estimated return from an entire episode to update the policy parameters.\nThe output of it is a probability distribution of actions. And we try to maximize J(θ) which is this estimated return. (details of Policy Gradient theorem in this video from Pieter Abbeel)\nWe will update weights using this gradient: \\[\n\\theta \\gets  \\theta + \\alpha\\nabla_\\theta J(\\theta)\n\\] \n\n\\(\\nabla_\\theta\\log\\pi_\\theta(a_t \\| s_t)\\) is the direction of steepest increase of the (log) probability of selecting action at from state \\(s_t\\). => This tells use how we should change the weights of policy if we want to increase/decrease the log probability of selecting action at state \\(s_t\\).\n\\(R(\\tau)\\) is the scoring function:\n\nIf the return is high, it will push up the probabilities of the (state, action) combinations.\nElse, if the return is low, it will push down the probabilities of the (state, action) combinations.\n\n\n\n\n2️⃣ 👩‍💻 Then dive on the hands-on where you’ll code your first Deep Reinforcement Learning algorithm from scratch: Reinforce.\n👉 https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit5/unit5.ipynb\n1st model is Cartpole. After training on 10’000 episodes, perfect score of 500 +- 0. Thomas pointed me to a video (3h) from Aniti where Antonin Raffin gives some tips and tricks. And points to many papers such as Deep Reinforcement Learning that Matters (in zotero)\n2nd model is Pixelcopter. High level of variance in perf. Recommended by Thomas to tune hyper parameters (optuna?).\n3rd model is Pong."
  },
  {
    "objectID": "posts/2022-07-01-logbook-July-22.html",
    "href": "posts/2022-07-01-logbook-July-22.html",
    "title": "Logbook for July 22",
    "section": "",
    "text": "Friday 7/1\n\ncontinue deep rl class with unit 4, unity ml agents within huggingface. And there is a link to a page explaining decision transformers. Seems quite powerful and could be useful for me."
  },
  {
    "objectID": "posts/2022-07-01-logbook-July-22.html#week-27---july-22",
    "href": "posts/2022-07-01-logbook-July-22.html#week-27---july-22",
    "title": "Logbook for July 22",
    "section": "Week 27 - July 22",
    "text": "Week 27 - July 22\nWednesday 7/6\n\ncontinue deep rl class with unit 5, Policy Gradient with PyTorch"
  },
  {
    "objectID": "posts/2022-07-01-logbook-July-22.html#week-29---july-22",
    "href": "posts/2022-07-01-logbook-July-22.html#week-29---july-22",
    "title": "Logbook for July 22",
    "section": "Week 29 - July 22",
    "text": "Week 29 - July 22\nThursday 7/21\n\nI would like to host kaggle-like competitions. I have found EvalAI which could be an option. I could push a competition for my data manufacturing colleagues and for other areas in my company. There is a comparison with other kind of platforms (both closed and open sourced)\ninstalling rancher desktop to test EvalAI I don’t have administrator rights anymore, so I have moved to installing docker in WSL.\n\nFriday 7/22\n\nbased on my docker in WSL installation, I tried to follow EvalAI instructions. It fails at docker-compose build phase. I have opened a ticket."
  },
  {
    "objectID": "posts/2022-07-01-logbook-July-22.html#week-30---july-22",
    "href": "posts/2022-07-01-logbook-July-22.html#week-30---july-22",
    "title": "Logbook for July 22",
    "section": "Week 30 - July 22",
    "text": "Week 30 - July 22\nMonday 7/25\nAs a matter of test, installation of EvalAI on my linux machine (no issue with corporate FW) using docker.\nWhen starting, this error: ERROR: for db Cannot start service db: [...] listen tcp 0.0.0.0:5432: bind: address already in use. Just kill the running postgres process as explained in Evalai - Common Errors during installation\nTuesday 7/26\nAs a matter of test, installation of EvalAI on my wsl machine using virtualenv (no docker) to try a gitlab connectivity instead of github. Tried on ubuntu-22.04. And tried on ubuntu-18.04 without success.\nWednesday 7/27\nNot giving up 😓. Will try this: build docker image from linux, save it. Moved it to my wsl image. Restore it. Pray. How to copy a Docker image from one server to another without pushing it to a repository first?\n\nfrom linux: sudo docker save -o /tmp/evalai_nodejs.tar evalai_nodejs\nfrom wsl:\n\nsudo mkdir /mnt/e\nsudo mount -t drvfs E: /mnt/e\n# pv to copy with a progress bar\npv /mnt/e/janus/evalai_nodejs.tar > ~/tmp/evalai_nodejs.tar\nsudo docker load -i evalai_nodejs.tar\ncd ~/evalai\ndocker-compose up\nbut I don’t know how to go further as explained in this evalai issue\nOk this works with the following images:\nevalai_django.tar  \nevalai_nodejs.tar  \nevalai_nodejs_v2.tar  \nevalai_worker.tar\nFriday7/29\nNow that I can start Evalai on my corporate machine, how to setup it with gitlab."
  },
  {
    "objectID": "posts/2022-07-21-install docker on linux.html",
    "href": "posts/2022-07-21-install docker on linux.html",
    "title": "install docker within linux",
    "section": "",
    "text": "Install Docker Engine on Ubuntu"
  },
  {
    "objectID": "posts/2022-07-21-install docker on linux.html#installation-and-test",
    "href": "posts/2022-07-21-install docker on linux.html#installation-and-test",
    "title": "install docker within linux",
    "section": "Installation and test",
    "text": "Installation and test\n\nSet up the repository\nsudo apt-get update\nsudo apt-get install \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n\n# Add Docker’s official GPG key\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n\n# set up the Docker repository\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\n\nInstall Docker Engine\n# Install Docker Engine\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin\n\n\nTest\nsudo docker run hello-world\n> Hello from Docker!\n> [...]"
  },
  {
    "objectID": "posts/2022-07-21-install docker on WSL.html",
    "href": "posts/2022-07-21-install docker on WSL.html",
    "title": "install docker within WSL",
    "section": "",
    "text": "WSL 2 Docker inside WSL 2\ninstall ubuntu 22.04 on WSL"
  },
  {
    "objectID": "posts/2022-07-21-install docker on WSL.html#pre-requisite-installation---ubuntu-docker",
    "href": "posts/2022-07-21-install docker on WSL.html#pre-requisite-installation---ubuntu-docker",
    "title": "install docker within WSL",
    "section": "Pre-requisite Installation - ubuntu-docker",
    "text": "Pre-requisite Installation - ubuntu-docker\n\nuninstall image (if needed)\n# wsl --unregister <distroName>\nwsl --unregister ubuntu-docker\n\n\nsetup installation directory\ncreate this structure by copying the existing one from ubuntu-22.04\nguillaume@LL11LPC0PQARQ:/mnt/d/wsl/Ubuntu-docker$ tree\n.\n└── download\n    ├── GWSL.Traditional.140.release.x64.exe\n    ├── ZscalerRootCA.crt\n    ├── jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz\n    ├── setup_wsl_root.sh\n    └── setup_wsl_user.sh\n\n\ncreate ubuntu-docker image\nfrom Powershell\nwsl --import ubuntu-docker D:\\wsl\\ubuntu-docker\\instance D:\\wsl\\ubuntu-docker\\download\\jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz\n\n#should appear in \nwsl --list --all -v\n\n\ndeclare ubuntu-docker in Windows Terminal\nWindows Terminal > Settings > Add a profile > Duplicate a profile (from Ubuntu-22.04)\nName: ubuntu-docker\nCommand line: C:\\WINDOWS\\system32\\wsl.exe -d ubuntu-docker\nTab title: ubuntu docker\n\n\nsetup root configuration\nStart ubuntu-docker from Windows Terminal\ncp /mnt/d/wsl/Ubuntu-docker/download/setup_wsl_* .\nchmod +x setup_wsl_root.sh\n./setup_wsl_root.sh\nenter username and password\nFrom powershell, stop ubuntu-docker\nwsl -t ubuntu-docker\n\n\nsetup user configuration\nStart ubuntu-docker from Windows Terminal\nFollow instructions, don’t skip the integration of ssh key in gitlab\nguillaume@LL11LPC0PQARQ:~$ host google.fr\ngoogle.fr has address 142.250.75.227\ngoogle.fr has IPv6 address 2a00:1450:4007:811::2003\nHost google.fr not found: 3(NXDOMAIN)\nThat’s it for WSL setup, docker can now be installed"
  },
  {
    "objectID": "posts/2022-07-21-jupyter-export-lab-as-py.html",
    "href": "posts/2022-07-21-jupyter-export-lab-as-py.html",
    "title": "Auto export python code from jupyter lab",
    "section": "",
    "text": "And this is an updated version for Jupyter lab when Auto export python code from jupyter notebooks was specifically for Jupyter notebook.\n\nInstall NBAUTOEXPORT\nYou should install it in the same virtual environment that you are running Jupyter Notebook or JupyterLab from.\n# in base (from in _conda_env/base.txt)\npip install nbautoexport\n\n!cat ~/_conda_env/base.txt\n\nconda install -y mamba -n base -c conda-forge\nmamba init\nmamba install -y nb_conda_kernels\nmamba install -y -c conda-forge jupyterlab jupyterlab-git\nmamba install -y ipywidgets\n\nmamba install -y -c conda-forge jupyterlab_execute_time\n#and change in jupyter lab : Settings- > Advanced Settings Editor -> Notebook: {\"recordTiming\": true} \n\npip install --upgrade nvitop\n\njupyter labextension install jupyterlab-spreadsheet\npip install jupyterlab-tabular-data-editor\npip install nbautoexport\n\n\nThere are two main steps to setting up nbautoexport:\n\nInstall nbautoexport as a post-save hook with Jupyter (once per machine you use it on)\nConfigure a notebooks directory (once per project you are working on)\n\n\n\nInstall nbautoexport as a post-save hook\n# from base environment\nnbautoexport install\n\n> nbautoexport post-save hook successfully installed with Jupyter.\n> If a Jupyter server is already running, you will need to restart it for nbautoexport to work.\n\n\nConfigure a notebooks directory\n# from your notebooks directory\nnbautoexport configure .\nIt creates this file .nbautoexport\n───────┬───────────────────────────────────────────────────────────\n       │ File: .nbautoexport\n───────┼───────────────────────────────────────────────────────────\n   1   │ {\n   2   │   \"export_formats\": [\n   3   │     \"script\"\n   4   │   ],\n   5   │   \"organize_by\": \"extension\",\n   6   │   \"clean\": {\n   7   │     \"exclude\": []\n   8   │   }\n   9   │ }\n───────┴───────────────────────────────────────────────────────────\n\n\nTest\nAfter running it in my blog _notebooks directory, and saving a notebook; a script directory and .py file with notebook content are created\n(base) guillaume@LL11LPC0PQARQ:~/git/guillaume_blog/_notebooks$ tree\n.\n├── .nbautoexport\n├── 2022-07-21-jupyter-export-lab-as-py.ipynb\n├── README.md\n└── script\n    └── 2022-07-21-jupyter-export-lab-as-py.py\n2022-07-21-jupyter-export-lab-as-py.py has been created as expected\nI can now delete .nbautoexport and it won’t happen in that directory"
  },
  {
    "objectID": "posts/2022-07-26-install ubuntu 18.04 on WSL.html",
    "href": "posts/2022-07-26-install ubuntu 18.04 on WSL.html",
    "title": "install ubuntu 18.04 on WSL and then evalai",
    "section": "",
    "text": "install ubuntu 22.04 on WSL\ninstall evalai in ubuntu"
  },
  {
    "objectID": "posts/2022-07-26-install ubuntu 18.04 on WSL.html#installation-ubuntu-18.04",
    "href": "posts/2022-07-26-install ubuntu 18.04 on WSL.html#installation-ubuntu-18.04",
    "title": "install ubuntu 18.04 on WSL and then evalai",
    "section": "Installation ubuntu-18.04",
    "text": "Installation ubuntu-18.04\n\nuninstall image (if needed)\n# wsl --unregister <distroName>\nwsl --unregister ubuntu-18.04\n\n\ndownload images\nFrom cloud images ubuntu (cloud-images > bionic> current), now there are wsl images:\n\n\n\nubuntu cloud images\n\n\nI just have to download the last bionic (18.04) image bionic-server-cloudimg-amd64-wsl.rootfs.tar.gz\n\n\ninstall and setup from powershell\nI have downloaded this ubuntu image to D:\\wsl\\ubuntu-18.04\\download\n(base) guillaume@LL11LPC0PQARQ:/mnt/d/wsl$ tree\n.\n├── Ubuntu-20.04\n│   └── ext4.vhdx\n├── Ubuntu-22.04\n│   ├── download\n│   │   └── jammy-server-cloudimg-amd64-wsl.rootfs.tar.gz\n│   └── instance\nand my ubuntu-18.04 instance will stand in D:\\wsl\\ubuntu-18.04\\instance\nInstall with this command from powershell\n# wsl --import <distroname> <location of instance> <location of download>\nwsl --import ubuntu-18.04 D:\\wsl\\ubuntu-18.04\\instance D:\\wsl\\ubuntu-18.04\\download\\bionic-server-cloudimg-amd64-wsl.rootfs.tar.gz\nIt takes 3-4 minutes to install. and should be visible in your wsl instances.\n wsl --list --all -v\n  NAME            STATE           VERSION\n  ubuntu-22.04    Stopped         2\nthen to run it\n# wsl -d <distroname>\nwsl -d ubuntu-18.04\nor\n\nuse Windows Terminal as a launcher\nWindows Terminal is a smart way to group all terminals (powershell, and all your wsl instances)\n\n\n\nwindows terminal\n\n\nIt can be installed even with limited windows store access by clicking install in Installer le Terminal Windows et commencer à le configurer\nAutomatically all wsl instances appear in Settings."
  },
  {
    "objectID": "posts/2022-07-26-install ubuntu 18.04 on WSL.html#automatic-setup",
    "href": "posts/2022-07-26-install ubuntu 18.04 on WSL.html#automatic-setup",
    "title": "install ubuntu 18.04 on WSL and then evalai",
    "section": "Automatic setup",
    "text": "Automatic setup\ncopy these 2 scripts in /root/ (given they are in D:\\wsl\\ubuntu-18.04\\download)\ncp /mnt/d/wsl/Ubuntu-18.04/download/setup_wsl_* .\nsetup_wsl_root.sh download\n#!/bin/bash\n\necho \"0. get username: \"\nread user_name\n\n. /etc/lsb-release\n\necho Configuration for user [$user_name]\necho of distribution $DISTRIB_CODENAME\necho\n\necho \"1. create user and add in sudo\"\n#adduser --disabled-password --gecos \"\" $user_name\nadduser --gecos \"\" $user_name\nusermod -aG sudo $user_name\necho\n\necho \"2. create wsl.conf file\"\nrm -rf /etc/wsl.conf\ntee /etc/wsl.conf << EOF\n# Set the user when launching a distribution with WSL.\n[user]\ndefault=$user_name\nEOF\necho\n\necho \"3. prepare setup by user\"\ncp setup_wsl_user.sh /home/$user_name\nchown $user_name:users /home/$user_name/setup_wsl_user.sh\nchmod 750  /home/$user_name/setup_wsl_user.sh\ntee -a /home/$user_name/.bashrc << EOF\nif [ ! -e \".wsl_configured\" ]; then\n        ./setup_wsl_user.sh\n        touch .wsl_configured\nfi\nEOF\necho\n\necho \"end of configuration for root\"\necho \"stop wsl instance by running 'wsl -t <distro-name>' from powershell\"\necho \"and start from Windows Terminal\"\nsetup_wsl_user.sh download\n#!/bin/bash\n\necho \"1. setup wsl-vpnkit\"\nif grep -Fxq \"wsl-vpnkit\" ~/.profile\nthen\n    # code if found\n    echo \"   wsl-vpnkit already setup\"\nelse\n    # code if not found\n    echo 'wsl.exe -d wsl-vpnkit service wsl-vpnkit start' >> ~/.profile\nfi\nwsl.exe -d wsl-vpnkit service wsl-vpnkit start\nsource ./.bashrc\necho\n\necho \"2. create ssh key to copy to gitlab\"\n. /etc/lsb-release\nif [ ! -e \".ssh/id_rsa.pub\" ]; then\n        ssh-keygen -t rsa -b 4096 -C \"WSL2 ubuntu $DISTRIB_RELEASE\"\n        cat .ssh/id_rsa.pub\n        echo \"copy this content to gitlab > preferences > SSH Keys\"\n        read -p \"Press any key to resume ...\"\nfi\necho\n\necho \"3. update certificates\"\ngit clone git@gitlab.michelin.com:devops-foundation/devops_environment.git /tmp/devops_environment\nsudo cp /tmp/devops_environment/certs/* /usr/local/share/ca-certificates/\nsudo update-ca-certificates\nrm -rf /tmp/devops_environment\nif [ $DISTRIB_RELEASE == \"22.04\" ]\nthen\necho 'bug SSL with ubuntu 22.04 - https://bugs.launchpad.net/ubuntu/+source/openssl/+bug/1963834/comments/7'\nsudo tee -a /etc/ssl/openssl.cnf << EOF\n[openssl_init]\nssl_conf = ssl_sect\n\n[ssl_sect]\nsystem_default = system_default_sect\n\n[system_default_sect]\nOptions = UnsafeLegacyRenegotiation\nEOF\nfi\necho\n\necho \"4. update apt sources with artifactory\"\necho 'Acquire { http::User-Agent \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:13.37) Gecko/20100101 Firefox/31.33.7\"; };' | sudo tee /etc/apt/apt.conf.d/90globalprotectconf\nsudo sed -i 's,http://archive.ubuntu.com/ubuntu,https://artifactory.michelin.com/artifactory/ubuntu-archive-remote,g' /etc/apt/sources.list\nsudo sed -i 's,http://security.ubuntu.com/ubuntu,https://artifactory.michelin.com/artifactory/ubuntu-archive-remote,g' /etc/apt/sources.list\nsudo apt update\nsudo apt upgrade -y\necho\nThen\nchmod +x setup_wsl_root.sh\n./setup_wsl_root.sh\nAs explained stop wsl instance by running wsl --shutdown ubuntu-22.04 from powershell and start from Windows Terminal\nIt restarts from your user and it will install:\n\nsetup wsl-vpnkit\ncreate ssh key to copy to gitlab\nupdate certificates\nupdate apt sources with artifactory"
  },
  {
    "objectID": "posts/2022-07-26-install ubuntu 18.04 on WSL.html#installation-evalai",
    "href": "posts/2022-07-26-install ubuntu 18.04 on WSL.html#installation-evalai",
    "title": "install ubuntu 18.04 on WSL and then evalai",
    "section": "Installation EvalAI",
    "text": "Installation EvalAI\n\nStep 1: Install prerequisites\n\nInstall git - postgres\n\nsudo apt-get install git postgresql libpq-dev\n\ninstall rabbit-mq\n\nsudo apt-get -y install socat logrotate init-system-helpers adduser erlang-base \n# download the package\nwget https://github.com/rabbitmq/rabbitmq-server/releases/download/v3.10.6/rabbitmq-server_3.10.6-1_all.deb\n\n# install the package with dpkg\nsudo dpkg -i rabbitmq-server_3.10.6-1_all.deb\nrm rabbitmq-server_3.10.6-1_all.deb\n\ninstall python 3.7\n\nsudo apt install python3.7\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python3.7 1\nsudo update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2\nupdate-alternatives --list python\nsudo update-alternatives --config python\n\ninstall virtualenv\n\n# only if pip is not installed\nsudo apt-get install python3-pip build-essential\n# upgrade pip\npip3 install --upgrade pip\n# upgrade virtualenv\npip --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org install --upgrade virtualenv\nsource .profile\n\n\nStep 2: Get EvalAI code\ngit clone https://github.com/Cloud-CV/EvalAI.git evalai\n\n\nStep 3: Setup codebase\n\nCreate a python virtual environment and install python dependencies.\n\n#pour curl-config\nsudo apt install libcurl4-openssl-dev libssl-dev\n\ncd evalai\nvirtualenv -p python3.7 venv\nsource venv/bin/activate\n\npip --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org install -r requirements/dev.txt\n# issue on django-autofixture \n# https://github.com/gregmuellegger/django-autofixture/issues/117\n\ncannot go further due to this error. at some time in setuptools, dist.py has been introduced long_description ends-with, and it is not managed by ‘UltraMagicString’ in django-autofixture\n\n\nRename settings/dev.sample.py as dev.py\n\ncp settings/dev.sample.py settings/dev.py\n\nCreate an empty postgres database and run database migration.\n\ncreatedb evalai -U postgres\n# update postgres user password\npsql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"\n# run migrations\npython manage.py migrate\n\nFor setting up frontend, please make sure that node(>=7.x.x), npm(>=5.x.x) and bower(>=1.8.x) are installed globally on your machine. Install npm and bower dependencies by running\n\nnpm install\nbower install"
  },
  {
    "objectID": "posts/2022-07-29-evalai and gitlab.html",
    "href": "posts/2022-07-29-evalai and gitlab.html",
    "title": "EvalAI and gitlab",
    "section": "",
    "text": "Deploy evalai docker images by copying from another PC\nHost challenge using github"
  },
  {
    "objectID": "posts/2022-07-29-evalai and gitlab.html#run-docker-from-wsl",
    "href": "posts/2022-07-29-evalai and gitlab.html#run-docker-from-wsl",
    "title": "EvalAI and gitlab",
    "section": "Run docker from wsl",
    "text": "Run docker from wsl\ncd ~/evalai\ndocker-compose up\nIn case of errors:\n\nERROR: for nodejs UnixHTTPConnectionPool(host='localhost', port=None): Read timed out. Restart docker service and rerun evalai: sudo service docker restart"
  },
  {
    "objectID": "posts/2022-09-01-logbook-September-22.html",
    "href": "posts/2022-09-01-logbook-September-22.html",
    "title": "Logbook for September 22",
    "section": "",
    "text": "Monday 9/5\n\nStart with fastai courses version 2022. Video of Lesson 1\n\nTuesday 9/6\nKeep backups on an external drive to upgrade PC. Using freefilesync.\n\nbackup files\nsbox\ngit (check with clustergit)\nwsl\n\n\nfrom WSL2 Backup and Restore Images using Import and Export\n# from PowerShell\nwsl --list\n> Ubuntu (par défaut)\n> ubuntu-docker\n> ubuntu-18.04\n> ubuntu-22.04\n> wsl-vpnkit\n\nwsl --shutdown\n\nwsl --export <Image Name> <Export location file name.tar>\n\n\ninstalled apps\n\nwsl2, prtscr, vscode, jdiskreport (centre logiciel), accessdatabasenginex64, driver nvidia, freefilesync (centre logiciel), notepad++ (centre logiciel), keepass (centre logiciel), zotero (centre logiciel), Windows Terminal, barrier, GWSL, typora,\n\nversion windows - 21H2 19044.1889\ntree structure\n\nD:\\> tree > e:\\tree_structure.txt\nD:\\> xcopy d: e:\\t /t /s\nWednesday 9/7\nFrom time to time when running screen from WSL I have the following error:\n$ screen -dR\nCannot make directory '/run/screen': Permission denied\nthen one can run\nsudo /etc/init.d/screen-cleanup start\nThurdsay 9/8\nRestore backups on this new PC and without administrator rights.\nInstall of wsl2 following corporate instructions"
  },
  {
    "objectID": "posts/2022-09-01-logbook-September-22.html#week-37---september-22",
    "href": "posts/2022-09-01-logbook-September-22.html#week-37---september-22",
    "title": "Logbook for September 22",
    "section": "Week 37 - September 22",
    "text": "Week 37 - September 22\nThursday 9/15\nPlayed a lot with nbdev2. Most thinks work now with gitlab. That’s great\nFriday 9/16 Hamel just announced that fastpages will be discontinued as nbdev+quarto is now a valid option to provide a blogging platform. He has written a migration guide for that. Will have to follow that."
  },
  {
    "objectID": "posts/2022-09-01-logbook-September-22.html#week-38---september-22",
    "href": "posts/2022-09-01-logbook-September-22.html#week-38---september-22",
    "title": "Logbook for September 22",
    "section": "Week 38 - September 22",
    "text": "Week 38 - September 22\nThursday 9/22 I have completed migration of my blog to quarto as explaine in Blog - migrate to quarto\nFriday 9/23 Activate Auto export python code from jupyter lab Remove previous hack (jupyter_notebook_config.py)"
  },
  {
    "objectID": "posts/2022-09-01-logbook-September-22.html#week-39---september-22",
    "href": "posts/2022-09-01-logbook-September-22.html#week-39---september-22",
    "title": "Logbook for September 22",
    "section": "Week 39 - September 22",
    "text": "Week 39 - September 22\nMonday 9/26 Just tested way to install last (untagged) version of a lib from gitlab with\npip uninstall janus-tools\npip install git+https://gitlab.michelin.com/janus/janus_tools.git"
  },
  {
    "objectID": "posts/2022-09-06-ssl-certificate_verify_failed.html",
    "href": "posts/2022-09-06-ssl-certificate_verify_failed.html",
    "title": "SSL: CERTIFICATE_VERIFY_FAILED",
    "section": "",
    "text": "Context\nMy company uses some ssl interceptor and it has to be considered as a cert autority.\n\n\nSolution\nfrom https://stackoverflow.com/questions/51390968/python-ssl-certificate-verify-error\nwhere certifcates are kept\n\nimport certifi\npem_path = certifi.where() \npem_path\n\n'/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/certifi/cacert.pem'\n\n\nget company certificates\n\ntmpdir = !mktemp -d\ntmpdir\n\n['/tmp/tmp.0V7L0xu2a5']\n\n\n\n!git clone git@gitlab.michelin.com:DEV/bib-certificates.git {tmpdir[0]}\n\nCloning into '/tmp/tmp.0V7L0xu2a5'...\nremote: Enumerating objects: 87, done.\nremote: Total 87 (delta 0), reused 0 (delta 0), pack-reused 87\nReceiving objects: 100% (87/87), 78.90 KiB | 1.55 MiB/s, done.\nResolving deltas: 100% (26/26), done.\n\n\n\n!ls -l {tmpdir[0]}/*trust-ca.pem\n\n-rw-r--r-- 1 guillaume guillaume 1606 Sep  6 19:11 /tmp/tmp.0V7L0xu2a5/cert_M_X5C_aze-cn-sslfwd-trust-ca.pem\n-rw-r--r-- 1 guillaume guillaume 1606 Sep  6 19:11 /tmp/tmp.0V7L0xu2a5/cert_M_X5C_rnh-ac-sslfwd-trust-ca.pem\n-rw-r--r-- 1 guillaume guillaume 1606 Sep  6 19:11 /tmp/tmp.0V7L0xu2a5/cert_M_X5C_rnh-eu-sslfwd-trust-ca.pem\n-rw-r--r-- 1 guillaume guillaume 1606 Sep  6 19:11 /tmp/tmp.0V7L0xu2a5/cert_M_X5C_rnh-na-sslfwd-trust-ca.pem\n-rw-r--r-- 1 guillaume guillaume 1602 Sep  6 19:11 /tmp/tmp.0V7L0xu2a5/cert_M_X5C_sase-mob-sslfwd-trust-ca.pem\n-rw-r--r-- 1 guillaume guillaume 1602 Sep  6 19:11 /tmp/tmp.0V7L0xu2a5/cert_M_X5C_sase-net-sslfwd-trust-ca.pem\n\n\n\nimport os\n\nfor filename in os.listdir(tmpdir[0]):\n    if filename.endswith(\"trust-ca.pem\"): \n         # print(os.path.join(directory, filename))\n        !cat {os.path.join(tmpdir[0], filename)} >> {pem_path}\n        continue\n    else:\n        continue\n\n\n\nValidate it works\n\nimport urllib.request\nwith urllib.request.urlopen('http://python.org/', cafile=certifi.where()) as response:\n   html = response.read()\n\n/tmp/ipykernel_2003/2808005746.py:2: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n  with urllib.request.urlopen('http://python.org/', cafile=certifi.where()) as response:\n\n\n\nhtml[:100]\n\nb'<!doctype html>\\n<!--[if lt IE 7]>   <html class=\"no-js ie6 lt-ie7 lt-ie8 lt-ie9\">   <![endif]-->\\n<!-'\n\n\nand from a command-line\nexport SSL_CERT_FILE='/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/certifi/cacert.pem'\nnbdev_new\n\n\nIntegration in WSL2\nI will modify SSL cert of my (base) environment.\nand add export SSL_CERT_FILE in .bashrc\nI have made the modification at install ubuntu 22.04 on WSL\n\n!cat ../files/setup_wsl_02_install_python_conda_part3.sh\n\necho \"configure SSL cert v1\"\n\nexport SSL_CERT_FILE=`python -c 'import certifi;print(certifi.where())'`\n\ntee -a ~/.bashrc << EOF\nexport SSL_CERT_FILE=$SSL_CERT_FILE\nEOF\n\nexport TMPDIR=`mktemp -d`\ngit clone git@gitlab.michelin.com:DEV/bib-certificates.git $TMPDIR\ncd $TMPDIR\ncat *trust-ca.pem >> $SSL_CERT_FILE\n\nif [ -e \"/.cfg\" ]; then\n        config='/usr/bin/git --git-dir=/.cfg/ --work-tree=/'\n        $config add ~/.bashrc\n        $config commit -m'export certificates for commandline'\n        $config push        \nfi"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html",
    "href": "posts/2022-09-12-nbdev2.html",
    "title": "nbdev2 - first steps",
    "section": "",
    "text": "fastai has just released nbdev2.\nThis is a complete rewrite with quarto. I like how they displayed features in that card"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#create-github-project",
    "href": "posts/2022-09-12-nbdev2.html#create-github-project",
    "title": "nbdev2 - first steps",
    "section": "create github project",
    "text": "create github project\n\ncreate a new project with github: dataset_tools. Give a description it will be reused by nbdev"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#integrate-nbdev-in-your-python-environment",
    "href": "posts/2022-09-12-nbdev2.html#integrate-nbdev-in-your-python-environment",
    "title": "nbdev2 - first steps",
    "section": "integrate nbdev in your python environment",
    "text": "integrate nbdev in your python environment\n\ncreate a local conda env dataset_tools with what is required to develop this library\n\n\n!cat /home/guillaume/_conda_env/dataset_tools.txt\n\nconda remove --name dataset_tools --all\nconda create --name dataset_tools python=3.9\nconda activate dataset_tools\nconda install ipykernel\npython -m ipykernel install --user --name=dataset_tools\npip install nbdev -U\npip install pandas\n\n\n\nimport sys\n!{sys.prefix}/bin/pip list|grep nbdev\n\nnbdev              2.2.10"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#clone-repo-and-turned-it-into-a-nbdev-repo",
    "href": "posts/2022-09-12-nbdev2.html#clone-repo-and-turned-it-into-a-nbdev-repo",
    "title": "nbdev2 - first steps",
    "section": "clone repo and turned it into a nbdev repo",
    "text": "clone repo and turned it into a nbdev repo\n\nclone repo dataset_tools and turn it into a nbdev repo\n\ngit clone git@github.com:castorfou/dataset_tools.git\nconda activate dataset_tools\ncd dataset_tools"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#nbdev_-commands-are-ready-to-be-used",
    "href": "posts/2022-09-12-nbdev2.html#nbdev_-commands-are-ready-to-be-used",
    "title": "nbdev2 - first steps",
    "section": "nbdev_ commands are ready to be used",
    "text": "nbdev_ commands are ready to be used\n\nnbdev can be used from here. For example nbdev_help to display all nbdev_ commands and what it does. And more detail can be got with -h: nbdev_new -h\n\n\n!{sys.prefix}/bin/nbdev_help\n\nnbdev_bump_version              Increment version in settings.ini by one\nnbdev_changelog                 Create a CHANGELOG.md file from closed and labeled GitHub issues\nnbdev_clean                     Clean all notebooks in `fname` to avoid merge conflicts\nnbdev_conda                     Create a `meta.yaml` file ready to be built into a package, and optionally build and upload it\nnbdev_create_config             Create a config file.\nnbdev_deploy                    Deploy docs to GitHub Pages\nnbdev_docs                      Create Quarto docs and README.md\nnbdev_export                    Export notebooks in `path` to Python modules\nnbdev_filter                    A notebook filter for Quarto\nnbdev_fix                       Create working notebook from conflicted notebook `nbname`\nnbdev_help                      Show help for all console scripts\nnbdev_install                   Install Quarto and the current library\nnbdev_install_hooks             Install Jupyter and git hooks to automatically clean, trust, and fix merge conflicts in notebooks\nnbdev_install_quarto            Install latest Quarto on macOS or Linux, prints instructions for Windows\nnbdev_merge                     Git merge driver for notebooks\nnbdev_migrate                   Convert all directives and callouts in `fname` from v1 to v2\nnbdev_new                       Create an nbdev project.\nnbdev_prepare                   Export, test, and clean notebooks, and render README if needed\nnbdev_preview                   Preview docs locally\nnbdev_pypi                      Create and upload Python package to PyPI\nnbdev_quarto                    Create Quarto docs and README.md\nnbdev_readme                    Render README.md from index.ipynb\nnbdev_release_both              Release both conda and PyPI packages\nnbdev_release_gh                Calls `nbdev_changelog`, lets you edit the result, then pushes to git and calls `nbdev_release_git`\nnbdev_release_git               Tag and create a release in GitHub for the current version\nnbdev_sidebar                   Create sidebar.yml\nnbdev_test                      Test in parallel notebooks matching `path`, passing along `flags`\nnbdev_trust                     Trust notebooks matching `fname`\nnbdev_update                    Propagate change in modules matching `fname` to notebooks that created them\n\n\n\nnbdev_new. It is creating the structure and files such as settings.ini.\nfrom base environment we can start jupyter notebook. It is advised to install nb_extensions (pip install jupyter_contrib_nbextensions), and activate TOC2. Open 00_core.ipynb with dataset_tools kernel. Rename 00_core.ipynb –> 00_container.ipynb"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#and-prefix-in-notebooks-as-well",
    "href": "posts/2022-09-12-nbdev2.html#and-prefix-in-notebooks-as-well",
    "title": "nbdev2 - first steps",
    "section": "and #| prefix in notebooks as well",
    "text": "and #| prefix in notebooks as well\nJeremy explains then what are #| used by quarto and nbdev.\nAnd for example #| hide will allow to be executed but hide in your documentation.\nActually from a single notebook, you have 3 usages: * the notebook by itself - all cells are executed, whatever are the prefix #| that you display on cells * the python file - only the cells with #| export will be published in a python file referenced as #| default_exp <name of python file>. A new file is genreated when nbdev_export is called. * the documentation - all cells are used, except the one started with #| hide. Seems to be dynamically generated (when nbdev_preview is running?). #| export are handled specifically: if you have import, nothing is displayed. If you have code, definitions and docstrings are exported, and arguments as well.\nThere is an easy way to describe arguments of a function.\nJust make some indentation with comments such as in\n    def __init__(self, \n                 cle : str, # la clé du container\n                 dataset : pd.DataFrame = None, # le dataset\n                 colonnes_a_masquer : list = [], # les colonnes à masquer\n                 colonnes_a_conserver : list = [] # les colonnes qui ne seront pas transformées\n                ):\n\nshow_doc\nand we can directly see the effect of it by calling show_doc (show_doc(Container)). You can even call show_doc on code not written with nbdev, or not even written by you.\n\nfrom nbdev.showdoc import *\nimport pandas as pd\nshow_doc(pd.DataFrame)\n\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section See Also\n  else: warn(msg)\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section Examples\n  else: warn(msg)\n\n\n\n\nDataFrame\n\n DataFrame (data=None, index:Axes|None=None, columns:Axes|None=None,\n            dtype:Dtype|None=None, copy:bool|None=None)\n\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\nData structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nNoneType\nNone\nDict can contain Series, arrays, constants, dataclass or list-like objects. Ifdata is a dict, column order follows insertion-order. If a dict contains Serieswhich have an index defined, it is aligned by its index... versionchanged:: 0.25.0 If data is a list of dicts, column order follows insertion-order.\n\n\nindex\nAxes | None\nNone\nIndex to use for resulting frame. Will default to RangeIndex ifno indexing information part of input data and no index provided.\n\n\ncolumns\nAxes | None\nNone\nColumn labels to use for resulting frame when data does not have them,defaulting to RangeIndex(0, 1, 2, …, n). If data contains column labels,will perform column selection instead.\n\n\ndtype\nDtype | None\nNone\nData type to force. Only a single dtype is allowed. If None, infer.\n\n\ncopy\nbool | None\nNone\nCopy data from inputs.For dict data, the default of None behaves like copy=True. For DataFrameor 2d ndarray input, the default of None behaves like copy=False... versionchanged:: 1.3.0\n\n\n\n\n\n\n\n\nunit testing\nThere are some basic testing functionalty available by importing fastcore. from fastcore.test import *\nWith test_eq very closed to assert and test_ne closed to assert not\n\nfrom fastcore.test import *\nshow_doc(test_eq)\nshow_doc(test_ne)\n\n\nsource\n\ntest_ne\n\n test_ne (a, b)\n\ntest that a!=b\n\n\n\nThis is convenient to integrate all the unit tests that way. When you will export by running Restart & Run All, if an error is met, export won’t be done.\nAnd one can run nbdev_test from the command line.\n\n\n@patch - define method out of its class\nJust by adding this import\nfrom fastcore.utils import *\none can use\n@patch\ndef new_method(self:myclass):\n    pass\n\n\nnbdev_export\nfrom command line, one can run nbdev_export\nor directly from jupyter, for example will be executing Restart & Run All\n#| hide\nimport nbdev; nbdev.nbdev_export()\nAnd we can install it to be used directly by running pip install -e .\nIt means that you can now import your project with\nfrom dataset_tools.container import *\nWhen it will be published (pypi or conda), it will be installable by calling\npip install dataset-tools\nor\nconda install -c fastai dataset-tools\nNB: see how _ has been turned into -, and for that to happen we have to update lib_name and lib_path in settings.ini by replacing _with -\nNB2: it is still confusing for me. It looks like modifying lib_path is not a good optiom.\n\n\nindex.ipynb\nHere it is a good idea to give overview about how to use it.\nBy importing your library and start using it.\nAnd it will be exported as the homepage of your lib.\nJust have to decide what should land in index and what should land in module page.\n\n\nnbdev_preview\nJust run it from command line\nnbdev_preview\nand it is accessible from http://localhost:3000.\nThis is a quarto webserver. The 1st time you launch it it will install quarto for you. On ubuntu this is a standard package so it will be updated regularly.\n\nfrom getpass import getpass\n!echo {getpass()} | sudo -S apt-cache show quarto\n\n········\n[sudo] password for guillaume: Package: quarto\nStatus: install ok installed\nPriority: optional\nSection: user/text\nInstalled-Size: 242759\nMaintainer: RStudio, PBC <quarto@rstudio.org>\nArchitecture: amd64\nVersion: 1.1.189\nDescription: Quarto is an academic, scientific, and technical publishing system built on Pandoc.\nDescription-md5: 516c872f9c3577457bbd01eac38f8130\nHomepage: https://github.com/quarto-dev/quarto-cli\n\n\n\n\n\nnbdev_test\nAs mentionned earlier, one can run nbdev_test to execute all tests in all notebooks.\nIf it fails, Hamel has shared his dev workflow. He runs Restart kernel & run All, and use %debug magic command to enter debug mode.\nYou then have access to all ipdb commands such as h for help, p var to print content of var, w for stacktraces\n\n%debug\n\n> /tmp/ipykernel_2453/349085080.py(1)<cell line: 1>()\n----> 1 show_doc(test_eq)\n\nipdb> h\n\nDocumented commands (type help <topic>):\n========================================\nEOF    commands   enable    ll        pp       s                until \na      condition  exit      longlist  psource  skip_hidden      up    \nalias  cont       h         n         q        skip_predicates  w     \nargs   context    help      next      quit     source           whatis\nb      continue   ignore    p         r        step             where \nbreak  d          interact  pdef      restart  tbreak         \nbt     debug      j         pdoc      return   u              \nc      disable    jump      pfile     retval   unalias        \ncl     display    l         pinfo     run      undisplay      \nclear  down       list      pinfo2    rv       unt            \n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\nipdb> q\n\n\n\n\nGolden rule: don’t mix imports and code\nFor a reason it is asked not to mix cells with imports and code.\nI am not sure what is the core reason for that. Something due to show_doc or doc generation?\nDuring my tests, I have seen something complaining about it after running nbdev_export or nbdev_test but cannot reproduce that. Hmmm\n\n\nnbdev_clean\nJust to remove unnecessary metadata in ipynb files.\nWill open an issue, because it fails to run here\n(dataset_tools) guillaume@LK06LPF2LTSSL:~/git/dataset_tools$ nbdev_clean\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/nbdev/clean.py:110: UserWarning: Failed to clean notebook\n  warn(f'{warn_msg}')\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/nbdev/clean.py:111: UserWarning: clean_ids\n  warn(e)\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/nbdev/clean.py:111: UserWarning: clean_ids\n  warn(e)\n\n\npush to github\nNot a bad thing to run all these stuff\nnbdev_clean\ngit diff\ngit status\ngit add -A\nnbdev_export\nnbdev_test\nnbdev_docs\ngit commit -am'init version'\ngit push\nNote that for a reason nbdev_clean is failing\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/nbdev/clean.py:110: UserWarning: Failed to clean notebook\n  warn(f'{warn_msg}')\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/nbdev/clean.py:111: UserWarning: clean_ids\n  warn(e)\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/nbdev/clean.py:111: UserWarning: clean_ids\n  warn(e)\nAnd Hamel suggests to add clean_ids = True in settings.ini\nnbdev_docsis pushing the content of index.ipynb to README.md\n\n\ndefine dep\nJust modify settings.inito add dependancies (here pandas)\n# end of settings.ini\n[..]\n### Optional ###\nrequirements = fastcore pandas\n# dev_requirements =\n# console_scripts =\nclean_ids = True\nEt voila!, doc is available at https://castorfou.github.io/dataset_tools/ and you can push that address to your repo settings\n\n\npublish to Pypi, conda, …\nThis is done by calling nbdev_pypior nbdev_conda. And it is modifying settings.ini to increment version number. (very much as nbdev_bump_version does)\nThere are other commands such as nbdev_release_xxx the seems to do quite the same for git."
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#my-tests-with-our-internal-gitlab",
    "href": "posts/2022-09-12-nbdev2.html#my-tests-with-our-internal-gitlab",
    "title": "nbdev2 - first steps",
    "section": "my tests with our internal gitlab",
    "text": "my tests with our internal gitlab\n\ncreate project in gitlab\nProject name : nbdev_gitlab\nProject URL : https://gitlab.michelin.com janus nbdev_gitlab\nProject description : This is the smallest project to make nbdev working with gitlab\n\nCreate project\n\n\n\nclone it\nconda activate dataset_tools\ncd ~/git\ngit clone git@gitlab.michelin.com:janus/nbdev_gitlab.git\n\n\nnbdev_new\nexport SSL_CERT_FILE='/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/certifi/cacert.pem'\nnbdev_new\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/ghapi/core.py:99: UserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\nCould not access repo: janus/nbdev_gitlab to find your default branch - `main` assumed.\nEdit `settings.ini` if this is incorrect.\nIn the future, you can allow nbdev to see private repos by setting the environment variable GITHUB_TOKEN as described here:\nhttps://nbdev.fast.ai/cli.html#Using-nbdev_new-with-private-repos\nrepo = nbdev_gitlab # Automatically inferred from git\nuser = janus # Automatically inferred from git\nauthor = guillaume # Automatically inferred from git\nauthor_email = guillaume.ramelet@michelin.com # Automatically inferred from git\n# Please enter a value for description\ndescription = This is the smallest project to make nbdev working with gitlab\nsettings.ini created.\n/home/guillaume/miniconda/envs/dataset_tools/lib/python3.9/site-packages/ghapi/core.py:99: UserWarning: Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated\n  else: warn('Neither GITHUB_TOKEN nor GITHUB_JWT_TOKEN found: running as unauthenticated')\npandoc -o README.md\n  to: gfm+footnotes+tex_math_dollars-yaml_metadata_block\n  standalone: true\n  default-image-extension: png\n\nmetadata\n  description: This is the smallest project to make nbdev working with gitlab\n  title: nbdev_gitlab\n\nOutput created: _docs/README.md\n\n!ls -l ~/git/nbdev_gitlab\n\ntotal 36\n-rwxrwxrwx 1 guillaume guillaume   978 Sep  5 18:31 00_core.ipynb\n-rwxrwxrwx 1 guillaume guillaume 11337 Sep  5 18:31 LICENSE\n-rwxrwxrwx 1 guillaume guillaume   111 Sep  5 18:31 MANIFEST.in\n-rwxrwxrwx 1 guillaume guillaume   309 Sep 13 14:02 README.md\ndrwxrwxrwx 1 guillaume guillaume  4096 Sep 13 14:02 _docs\n-rwxrwxrwx 1 guillaume guillaume   728 Sep 13 14:02 _quarto.yml\n-rwxrwxrwx 1 guillaume guillaume  1561 Sep 13 14:02 index.ipynb\ndrwxrwxrwx 1 guillaume guillaume  4096 Sep 13 14:02 nbdev_gitlab\n-rwxrwxrwx 1 guillaume guillaume   945 Sep 13 14:02 settings.ini\n-rwxrwxrwx 1 guillaume guillaume  2541 Sep  5 18:31 setup.py\n-rwxrwxrwx 1 guillaume guillaume   600 Sep  5 18:31 styles.css\n\n\n\n\nchange in settings.ini\n\nset company_name = michelin\nset doc_path = public\nset branch = main instead of master\ndoc_host = https://%(user)s.pages.gitlab.%(company_name)s.com/\ngit_url = https://gitlab.%(company_name)s.com/%(user)s/%(repo)s\ndoc_baseurl = /%(repo)s\n\n\n\nchange in _quarto.yml\nnothing to be done with nbdev > v2.3.3\n\n\ncreate .gitlab-ci.yml –> build/publish documentation, push to artifactory\nWith gitlab you have a nice editor to edit pipelines (CI lint)\nOne way to debug is to insert sleep xx and then click debug.\nYou then have access to your docker image.\ndefault:\n  image: 'docker.artifactory.michelin.com/michelin/hub/ubuntu20.04:bib-1.1'\n  tags:\n    - k8s\n  interruptible: true\n  retry:\n    max: 2\n    when:\n      - runner_system_failure\n      - stuck_or_timeout_failure\n\n# Functions that should be executed before the build script is run\nbefore_script:\n  - apt -y install wget\n  - wget \"https://github.com/quarto-dev/quarto-cli/releases/download/v1.1.189/quarto-1.1.189-linux-amd64.deb\"\n  - dpkg -i quarto-1.1.189-linux-amd64.deb\n  - apt -y install python3-pip\n  - wget --no-check-certificate --content-disposition -O - https://raw.githubusercontent.com/castorfou/guillaume_blog/master/files/setup_wsl_08_pip.sh | bash\n  - pip3 install nbdev\n  - nbdev_install\n\nstages:\n  - test\n  - build_doc\n  - build\n  - deploy_artifactory\n\ntests:\n  stage: test\n  script:\n    - nbdev_test\n\npages:\n  stage: build_doc\n  script:\n    - nbdev_docs\n  artifacts:\n    paths:\n      # The folder that contains the files to be exposed at the Page URL\n      - public\n  rules:\n    # This ensures that only pushes to the default branch will trigger\n    # a pages deploy\n    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH\n\nwheel:\n  stage: build\n  script:\n    - mkdir -p public\n    - echo \"Build wheel with python version `python3 --version`:\"\n    - pip install -U setuptools wheel pydnx_packaging\n    - pip install -e .\n    - python3 setup.py bdist_wheel\n    - mkdir -p packages && mv dist/* packages/\n  artifacts:\n    when: always\n    paths:\n      - packages/\n\npublish:\n  stage: deploy_artifactory\n  dependencies:\n    - wheel\n  only:\n    - tags\n  script:\n    # create credential config file\n    - >\n      if [ -f '.pypirc' ]; then\n        echo \"Information: .pypirc file is not mandatory anymore.\" && cp .pypirc ~/\n      else\n        echo \"[distutils]\n        index-servers = local\n        [local]\n        repository: https://artifactory.michelin.com/api/pypi/pypi\n        username: fm00884\n        password: <don't even think about it>\" > ~/.pypirc\n      fi\n    - pip install -U twine\n    - pip index versions nbdev_gitlab || true\n    - echo 'If the \"twine upload\" command below failed with a 403 status code, please check that the version is not already uploaded on artifactory (see versions of nbdev_git above).'\n    - twine upload --verbose -r local packages/*\n\n\ncommit to gitlab\nnbdev_prepare\nrm -rf index_files\nnbdev_docs #optionnal if nbdev_preview was used\nnbdev_proc_nbs\ngit diff\ngit status\ngit add -A\ngit commit -am'<proper comment>'\ngit push\n\n\nsetup online-documentation badge\n\nFrom Settings > General > Badges\ncreate a new entry doc\nLink: https://janus.si-pages.michelin.com/nbdev_gitlab/\nBadge image URL: https://img.shields.io/badge/-online_documentation-grey.svg\n\n\n[manual way] create a new version, tag, publication to artifactory\nTo summarize, here is the publishing process\nnbdev_bump_version\ngit add -A\ngit commit -am'<my tag changelog>'\ngit tag -a 0.0.3 -m \"<my tag changelog>\"\ngit push origin 0.0.3 \nAnd it will be published at: artifactory janus-tools package\nFor later: would be nice to automatically get tag name from settings.ini\nnbdev_bump_version increases version number (in settings.ini and ini.py)\n$ nbdev_bump_version\nOld version: 0.0.2\nNew version: 0.0.3\nwhich is modifying 2 files:\n$ git diff\ndiff --git a/nbdev_gitlab/__init__.py b/nbdev_gitlab/__init__.py\n-__version__ = \"0.0.2\"\n+__version__ = \"0.0.3\"\ndiff --git a/settings.ini b/settings.ini\n-version = 0.0.2\n+version = 0.0.3\n\n\n[script way] create a new version, tag, publication to artifactory\njust call push_tag.sh, it will increase version (calling nbdev_bump_version), and create/push tag with this version and last git commit\n\n!cat ../files/push_tag.sh\n\n#!/bin/bash\nnbdev_bump_version\nLAST_GIT_COMMENT=`git log -1 --pretty=%B`\nVERSION_TO_TAG=`grep \"version \" settings.ini | cut -d '=' -f 2`\n\necho \"Tag: $VERSION_TO_TAG - Comment: $LAST_GIT_COMMENT\"\n\ngit add -A\ngit commit -am\"$LAST_GIT_COMMENT\"\ngit tag -a $VERSION_TO_TAG -m \"$LAST_GIT_COMMENT\"\ngit push origin $VERSION_TO_TAG"
  },
  {
    "objectID": "posts/2022-09-12-nbdev2.html#fixed-nbdev_clean-fails",
    "href": "posts/2022-09-12-nbdev2.html#fixed-nbdev_clean-fails",
    "title": "nbdev2 - first steps",
    "section": "[fixed] nbdev_clean fails",
    "text": "[fixed] nbdev_clean fails\nhttps://forums.fast.ai/t/nbdev-clean-fails-cryptically/98784/13\nSolution: update fastcore to version > 1.2.5"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html",
    "title": "Blog - migrate to quarto",
    "section": "",
    "text": "Hamel just announced that fastpages will be discontinued as nbdev+quarto is now a valid option to provide a blogging platform. He has written a migration guide for that.\nThis is my walkthrough."
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#install-quarto",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#install-quarto",
    "title": "Blog - migrate to quarto",
    "section": "install quarto",
    "text": "install quarto\n$ sudo apt install quarto\n[sudo] password for guillaume:\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nquarto is already the newest version (1.1.189).\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\nwill have to see what will happen on platforms where I don’t have admin rights.\nHere Quarto is already present because I use it for nbdev2."
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#create-repo-blog",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#create-repo-blog",
    "title": "Blog - migrate to quarto",
    "section": "create repo blog",
    "text": "create repo blog\nI create blog repo on github.\nAnd I can now get it locally: git clone https://github.com/castorfou/blog.git (I am from office, only https is accepted)"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#create-a-quarto-blog",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#create-a-quarto-blog",
    "title": "Blog - migrate to quarto",
    "section": "create a quarto blog",
    "text": "create a quarto blog\ncd ~/git/blog\nquarto create-project --type website:blog .\nquarto install extension quarto-ext/video\nIssue here with quarto install when running from a corporate pc\n$ quarto install extension quarto-ext/video\nSending fatal alert BadCertificate\nERROR: TypeError: error sending request for url (https://github.com/quarto-ext/video/archive/refs/heads/main.tar.gz): error trying to connect: invalid peer certificate contents: invalid peer certificate: UnknownIssuer\nI have opened an issue at https://github.com/quarto-ext/video/issues/27"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#copy-former-blog-content---notebooks-and-markdown-files",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#copy-former-blog-content---notebooks-and-markdown-files",
    "title": "Blog - migrate to quarto",
    "section": "copy former blog content - notebooks and markdown files",
    "text": "copy former blog content - notebooks and markdown files\nYour new repo will have a posts/ directory.\nThis is where you will copy all of your notebook and markdown posts from fastpages.\ncp -r ../guillaume_blog/_notebooks/* posts/\ncp -r ../guillaume_blog/_posts/* posts/\nI have to fix some stuff here,\nsome markdown posts have empty description which is not allowed by the migration process.\nTo fix that I will run sed -i -- 's/^description:[[:space:]*]$/description:\\ \\\"\\\"/' ~/git/blog/posts/*.md\nGlobally to identify culprit, I executed this:\nchemin=`pwd`\nfor FILE in ../guillaume_blog/_posts/*; \\\ndo echo $FILE; \\\ncp \"$FILE\" posts/; \\\nsed -i -- 's/^description:[[:space:]*]$/description:\\ \\\"\\\"/' $chemin/posts/*.md; \\\nnbdev_migrate --path posts; \\\nrm -f posts/* 2> /dev/null; \\\ndone;\nbut now that I know the migration issues, I can just execute:\n# handle empty description in markdown files\nchemin=`pwd`\nsed -i -- 's/^description:[[:space:]*]$/description:\\ \\\"\\\"/' $chemin/posts/*.md\n# code should not be here\nrm posts/notebook2script.py \nrm -rf posts/exp\nWhat was wrong with 2021-02-10-college-de-france-representations-parcimonieuses.md was accents in title. Removing é with e fixed it."
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#copy-former-blog-content---images",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#copy-former-blog-content---images",
    "title": "Blog - migrate to quarto",
    "section": "copy former blog content - images",
    "text": "copy former blog content - images\nmkdir images\ncp -r ../guillaume_blog/images/* images\ncp -r ../guillaume_blog/images/copied_from_nb/* images"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#migrate-posts-to-quarto",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#migrate-posts-to-quarto",
    "title": "Blog - migrate to quarto",
    "section": "migrate posts to quarto",
    "text": "migrate posts to quarto\nconda activate dataset_tools #this is an env with nbdev installed\n#install last version of nbdev\npip install -U nbdev\nnbdev_migrate --path posts"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#update-some-files",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#update-some-files",
    "title": "Blog - migrate to quarto",
    "section": "update some files",
    "text": "update some files\n\n.gitignore: we suggest adding _site/ as well as dot files .*\nabout.qmd: I reuse my former _pages/about.md\nprofile.jpg: and use my profile picture\n\n\n!cat ~/git/blog/.gitignore\n\n/.quarto/\n_site/\n.*\n!.gitignore"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#preview",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#preview",
    "title": "Blog - migrate to quarto",
    "section": "preview",
    "text": "preview\nquarto preview\nHere we can fix many thinks, and auto update rendered pages is just excellent!\n\nmove images from posts to posts/images (have to restart quarto preview after that)\ndelete the 2 examples (welcome and post-with-code)\n\nWill have to browse through all the site to see if all is properly rendered. > fix for broken links or Jekyll shortcodes (things with {% … %}) that need to be converted to Quarto. Search the the Quarto documentation if you need help locating specific Quarto features."
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#keep-git-repo-in-sync",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#keep-git-repo-in-sync",
    "title": "Blog - migrate to quarto",
    "section": "keep git repo in sync",
    "text": "keep git repo in sync\nNOW=`date '+%F_%H:%M'`;\ngit add .\ngit commit -m \"$NOW\"\ngit push"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#publication",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#publication",
    "title": "Blog - migrate to quarto",
    "section": "publication",
    "text": "publication\nThere are 2 ways to publish. A straightforword one by calling quarto publish. And a more advanced one with github actions.\nUsing my corporate PC, quarto publish fails so I will give github actions a try.\n\nquarto publish\nquarto publish gh-pages\n:heavy_check_mark: This is ok when publishing with home PC.\n:x: But fails when publishing with corporate PC.see below quarto publishing issue behind firewall\nshared it with community at discord: https://discord.com/channels/689892369998676007/1020178609605984267/1020631703653462038\n\n\ngithub actions\nas explained in https://quarto.org/docs/publishing/quarto-pub.html#github-action\n\n_publish.yml\nFirst step is to set _publish.yml\n- source: project\n  quarto-pub:\n    - id: \"5f3abafe-68f9-4c1d-835b-9d668b892001\"\n      url: \"https://castorfou.github.io/blog/\"\nbut what is this id? https://github.com/quarto-dev/quarto-web/issues/404"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#quarto-publishing-issue-behind-firewall",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#quarto-publishing-issue-behind-firewall",
    "title": "Blog - migrate to quarto",
    "section": "quarto publishing issue behind firewall",
    "text": "quarto publishing issue behind firewall\nIdentical to error happening when installing quarto extension\nhttps://github.com/quarto-ext/video/issues/27\n$ quarto publish\n? Publish update to: › https://castorfou.github.io/blog/ (GitHub Pages)\nFrom https://github.com/castorfou/blog\n * branch            gh-pages   -> FETCH_HEAD\n \norigin  https://github.com/castorfou/blog.git (fetch)\norigin  https://github.com/castorfou/blog.git (push)\nTo https://github.com/castorfou/blog.git\n + 0a3710d...1aeaf23 HEAD -> gh-pages (forced update)\nfatal: 'fadc274b' is not a working tree\n\nNOTE: GitHub Pages sites use caching so you might need to click the refresh\nbutton within your web browser to see changes after deployment.\n\n(\\) Deploying gh-pages branch to website (this may take a few minutes)Sending fatal alert BadCertificate\n[✓] Deploying gh-pages branch to website (this may take a few minutes)\nERROR: TypeError: error sending request for url (https://castorfou.github.io/blog/.nojekyll): error trying to connect: invalid peer certificate contents: invalid peer certificate: UnknownIssuer\nFor context, I use quarto as a replacement of fastai/fastpages and followed a migration guide from Hamel Hussain asking for this installation.\nAnd I’m in a corporate environment with transparent proxies and self signed certificates. My system has updated CERT in /usr/local/share/ca-certificates/, and SSL_CERT_FILE environment variable pointing to updated corporate pem.\n$ quarto install extension quarto-ext/lightbox\nSending fatal alert BadCertificate\nERROR: TypeError: error sending request for url (https://github.com/quarto-ext/lightbox/archive/refs/heads/main.tar.gz): error trying to connect: invalid peer certificate contents: invalid peer certificate: UnknownIssuer\nIt fails with the same message. When I run quarto publish I have the same issue.\nOne option is to use github actions. Need to find what is this id used in _publish.yml"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#workaround-inline-images-are-not-properly-rendered",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#workaround-inline-images-are-not-properly-rendered",
    "title": "Blog - migrate to quarto",
    "section": "[workaround] inline images are not properly rendered",
    "text": "[workaround] inline images are not properly rendered\nanalysis made at inline images from jupyter with quarto\nshort answer: use jupyter lab to blog"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#albert-rapp-as-an-example",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#albert-rapp-as-an-example",
    "title": "Blog - migrate to quarto",
    "section": "Albert Rapp as an example",
    "text": "Albert Rapp as an example\nhttps://albert-rapp.de/posts/13_quarto_blog_writing_guide/13_quarto_blog_writing_guide.html"
  },
  {
    "objectID": "posts/2022-09-16-migrate blog from nbdev to quarto.html#links-are-not-rendered-as-links",
    "href": "posts/2022-09-16-migrate blog from nbdev to quarto.html#links-are-not-rendered-as-links",
    "title": "Blog - migrate to quarto",
    "section": "links are not rendered as links",
    "text": "links are not rendered as links\nin jupyter, just typing url turns it into a link. Not with quarto.\nanyway to do it ?\nhttps://github.com/quarto-dev/quarto-cli/discussions/2609"
  },
  {
    "objectID": "posts/2022-09-23-inline-images-with-quarto.html",
    "href": "posts/2022-09-23-inline-images-with-quarto.html",
    "title": "inline images from jupyter with quarto",
    "section": "",
    "text": "using jupyter notebook\n1st image\n\n\n\nimage.png\n\n\n2nd image\n\n\n\nimage.png\n\n\nif we see a the same image, we have a problem\n\n\nanalysis\nwhen using jupyter lab, cell code for images are like that\n![image.png](attachment:71a4fc95-5776-4180-ae3a-b815bbf1b7a0.png)\nand with jupyter notebook or vscode, it is like that\n![image.png](attachment:image.png)\nThere is an enhancement request at vscode to support it\nhttps://github.com/microsoft/vscode/issues/151408\nand quarto when creating _site, is saving these images in this structure:\n\n!tree ../_site/posts/2022-09-23-inline-images-with-quarto_files/\n\n../_site/posts/2022-09-23-inline-images-with-quarto_files/\n└── figure-html\n    ├── 71a4fc95-5776-4180-ae3a-b815bbf1b7a0.png\n    ├── 74e0d8d8-ad0f-4eca-b52b-167ac518d0e9.png\n    └── image.png\n\n1 directory, 3 files\n\n\nonly one image.png for 2 files, that cannot work :(\n\n\nworkaround\nuse jupyter lab to blog"
  }
]